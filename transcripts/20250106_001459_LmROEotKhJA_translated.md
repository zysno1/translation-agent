【00：00-00：08】感谢您对我进行的亲切介绍和邀请我成为新年第一次聚会的第一位演讲者。
【00：08-00：19】我的名字是Jules Damji，我是Enniscale的首席开发倡导者，在我加入Enniscale之前，我在Databricks与Karen和Dorothy密切合作了大约六年。
【00：19-00：30】所以这对我来说就像回家一样，我很高兴也很荣幸能回到数据加AI社区，并分享一些正在发生的事情的一些方面。
【00：30-00：38】今天的演讲将是一个关于扩展和分布式Python和机器学习的框架。
【00：38-00：49】我们将从一个非常高的层次开始概述，因为这是关于Ray的第一个演讲，你们中的一些人可能听说过Ray，或者可能没有听说过Ray。
【00：49-01：03】我想说的是，从两万英尺的高度开始，Ray背后的动机是什么，Ray是什么，为什么是Ray，以及什么生态系统允许人们编写大规模的分布式应用程序，
【01：03-01：15】然后我们会降到一万英尺以下的海平面，到海平面上方的Ray架构和组件，你知道，当你揭开幕布时，内部是什么，Ray的核心是什么，它是如何工作的？
【01：15-01：26】它看起来像什么，幕后发生了什么，然后我们会一路降到C级，谈论记录库、本地库、API和设计模式，这些允许
【01：26-01：36】人们编写和扩展这些应用程序，然后我们将看看建立在Ray之上的相关库。
【01：36-01：44】特别是，我会给你一个关于RayTune的快速概述，这是一个由数据科学家和ML工程师广泛使用的库，用于调整。
【01：44-01：54】超参数计算，这是人们实际执行的一个非常常见的工作负载，而且如果时间允许，我们还会尝试谈谈exiboot ray，但我将做一个演示来谈论exiboot ray。
【01：54-02：10】这样我们就不用看幻灯片了，然后演示将讨论你实际上扩展的常见工作负载，你做训练，你做推理，你做超参数训练，这些都是开发周期的方式，然后我会试着回答你的一些问题。
【02：10-02：19】那么，为什么是Ray呢？在我们谈论为什么是Ray之前，我认为谈论一些正在发生的行业趋势是很合适的。
【02：19-02：30】当Ray的原始创建者在rice实验室时，那里是Spark及其前身Rice实验室的创新熔炉，MLAB是在那里开发的，Mesos也是在那里开发的，Titehorn是其中一个分布式系统。
【02：30-02：40】当新的rice实验室成立时，那里的研究人员和博士生，以及前老年病学执行官和创始人Jan Stoiker，
【02：40-02：50】讨论了今天在行业中实际发生的事情，我认为有三个基本因素，他们实际上考虑到了一个是机器学习在几乎每个领域都非常普遍。
【03：01-03：08】 我认为这是合适的，我会说，我认为我们已经进入了我所说的ml的zeisgeist，对吧，我们在一个地方。
【03：08-03：18】 我们有更多应用程序受到影响，并在垂直领域的所有领域编写和部署，你从健康开始。
【03：18-03：26】 到金融行业，到汽车，到疤痕行业，到健康，ML无处不在。
【03：26-03：38】 因此，它非常普遍，因为这些在这些行业中部署的复杂应用程序，它们需要相当多的计算，它们需要相当多的计算，唯一的方法是你可以真正做到这一点，
【03：38-03：45】 是要大规模地分布，所以现在那是那很有趣的第一部分。
【03：45-03：56】 第二部分是由于这种性质，现在分布式计算将成为一种必要性，它将成为一种规范，它不会成为例外，今天人们将只是做写分布式应用程序。
【03：56-04：07】 就像他们在笔记本电脑上写Python函数和Python类一样，在Jupiter笔记本上，突然间它现在在集群上无缝运行，这将成为一种规范和必要性。
【04：07-04：16】 对于我们来说，满足这一特定需求。其中一件事，只是为了让你了解为什么这实际上很重要，因为如果你看看如何。
【04：16-04：24】 Ml应用程序和深度学习图像，它们的训练在十年的时间里是如何演变的。
【04：24-04：33】 你会发现，实际进行这种计算所需的petal浮点数的需求每十八个月都在增加，而CPU负载。
【04：33-04：41】 实际上我们增加的速度只是没有跟上，因此在这两者之间存在巨大的差距。
【04：41-04：53】 你可能会说，那么加速器呢？比如我们有GPU和TPU，它们确实有区别，有一个增量的区别，但仍然存在巨大的差距，实际上存在于今天的，
【04：53-05：01】 非常复杂的，无论是深度学习还是ML应用程序都被部署了，它们实际上需要。
【05：01-05：11】 不仅仅是世界的fangs或世界的mangas在这样做，普通的初创公司实际上正在使用复杂的应用程序，他们需要计算能力来做这件事。
【05：11-05：21】 因此，我们认为解决这一需求的唯一方法，缩小差距的唯一方法是完全以非常分布式的方式走出去，这就是。
【05：21-05：34】 第二个需求正在到来，分布式计算是一种必要性，它将成为一种规范，我们最好为此做好准备，我们最好有框架，我们最好有编程语言，我们最好有...
【05：34-05：41】 计算层，数据科学家和机器学习工程师可以很容易地构建它，而不是。
【05：41-05：49】 有一个非常困难的时间，所以第三个问题是他们试图看的第三个趋势是。
【05：49-06：02】 那些在电话上的人，或者那些写过分布式应用程序的人，那些与其它分布式应用程序一起工作过的人，这并不容易，对吧，编程部分有点难，因为你必须真正理解，
【06：02-06：14】很多概念，而且众所周知很难，如果你看看现有的解决方案，你可以把它们归类为开发的便利性和通用性。
【06：14-06：23】如果你看看中间，两个之间的中位数，这些系统是由系统专家拼凑在一起的。
【06：23-06：34】由SRE，由真正理解分布式计算细微差别的人，你有MPI，Kafka，Spark，Flink，Spring，Horvod，TensorFlow，Cloudflare，他们把东西拼凑在一起。
【06：34-06：42】它们工作，你知道，现有的解决方案工作，但它们有取舍，对吧，取舍是，有时它们很难开发，有时它们很难部署和管理。
【06：42-06：52】原因在于这些都是定制的解决方案，它们有不同的语言语义，它们有不同的通信协议，所以要让它们非常和谐地工作有时会很麻烦。
【06：52-07：03】但是它们确实工作并且存在，所以这是人们今天实际做的中庸之道，我们今天都在使用这些系统，当它们工作时，它们工作起来毫不费力，当它们崩溃时，它们确实臭名昭著。
【07：04-07：15】如果你走到最右边，我认为有一种观念是这里没有发明，所以我们从一张白纸开始，自己动手做，所以你实际上可以有一只狗或容器，你可以把它放在Kubernetes上，
【07：15-07：23】你可以让你的应用程序docker很好，你可以使用最新的grpc运行你的通信，然后你去做它。
【07：23-07：33】这没什么错，它非常通用，它可以扩展到巨大的规模，但它开发起来很昂贵，因为你需要真正的系统专家，他们真正理解并能够将事情粘合在一起。
【07：33-07：44】然后，如果你走到最左边，这是最容易的部分，像你我和其他人这样写无服务器应用程序的人，就像Lambda函数或者它们只是执行。
【07：44-07：55】一个大查询或者他们只是使用Databricks SQL来发送一个查询，让Delta Lake House来处理它，并且它会自动扩展，它会处理我实际需要多少计算。
【07：55-08：04】那里的问题是它非常特定于云，所以如果某些东西在云上不存在，你就会受到那些限制。
【08：04-08：15】它们只有无状态的，所以如果你实际上有一个Lambda函数或者你有一个BigQuery，如果它不维护特定的状态，服务器就会消失，除非你有一个第三方的rest服务，你实际上会持久化。
【08：15-08：30】然后你必须为它编写更多的代码，可能不会有你需要的所有GPU和TPU以及硬件加速器，所以它有一些局限性，所以你实际上可以认为现有的解决方案是在这个特定的权衡之间。
【08：30-08：39】那么Ray是如何适应以解决这些特定需求的呢，我想我想Ray背后的信条和原则。
【08：39-08：51】Ray的愿景是，我们想让分布式计算变得简单，我们想让它对每个开发者都可访问，换句话说，他们真的不需要成为系统专家。
【08：51-09：02】他们实际上可以只编写Python函数和Python类，就像他们在编写那样，编写Python代码，并使用我即将谈到的一些设计模式，
【09：03-09：10】将他们的代码更改为现有的或分布式系统，关键在于让我们实际上采取这种做法并使其变得更容易。
【09：10-09：23】那么Ray是如何做到这一点的呢？正如我所说，我要从非常高的层次开始，如果你看一下Ray，你可以把它看作是一个分层蛋糕，一个功能和能力的分层蛋糕。
【09：23-09：34】在最底层，你有所有可以在其上运行的云，对吧，你实际上可以在AWS、任何云提供商，包括你的笔记本电脑上运行它，这实际上是非常棒的，因为现在你实际上可以在你的笔记本电脑上开发东西。
【09：34-09：41】或者我们可以在预置环境或任何硬件上运行，如果你向上堆栈，你会看到。
【09：41-09：48】基本的通用框架，这是我们所说的通用分布式计算的核心。
【09：48-10：03】背后的哲学原则，前提，公理是，Ray是通用计算，它不处理任何特定的工作流程，它没有你在上面构建API的特定抽象。
【10：03-10：14】相反，它给你这些低级原语，如任务、演员，我会谈到，你可以实际编写来构建任何你想要处理的特定目的工作负载。
【10：14-10：25】这就是Ray的本质，我们将负责所有的弹性、所有的容错性、所有的计算基底，你可以在上面构建东西。
【10：25-10：37】我们会给你做这些事情的原语，把它想象成像Unix操作系统，贝尔实验室的那些人，你知道，二三十年前，实际上只是给了你构建东西的基本API。
【10：37-10：51】你构建了一个写设备驱动程序的API，你构建了一个做窗口管理器的API，你写了一个做分布式计算的API，你fork和exit所有这些，然后你在上面构建东西，非常相似的概念，你实际上被给予某些API，然后你在上面构建东西。
【10：51-10：58】当你再上一层时，就是本地库出现的地方，本地库正在使用这些API。
【10：58-11：07】来实际处理特定的工作负载。所以如果你想进行分布式训练，你使用ray train。如果你想进行强化学习。
【11：07-11：18】如果你想使用RACE扩展模型，所以如果你想调整你的参数，你实际上可以使用超参数调整。
【11：18-11：28】这些都是建立在那之上的本地库，沿着右边中间，你会听到所有这些第三方集成，它们实际上有不同的集成级别，
【11：28-11：37】与play的集成有一级、二级和三级。每个都有各种方式与Ray集成。它们都在使用这些原语。
【11：37-11：50】插入Ray以扩展这些内容，最后如果你是一个分布式计算机程序员，实际上想要构建自己的分布式框架，这实际上与机器学习无关，与任何东西都无关，
【11：50-11：59】你只是想以分布式方式编写你的Python应用程序，你可以实际使用这个API并运行它，所以这是什么的某种层次。
【11：59-12：09】Wood ray实际上在那个从两万英尺的高度来看，生态系统是Ray的实际力量所在，它现在处理。
【12：09-12：18】特定的深度学习，ml工作负载，你实际上需要在大规模上进行这些机器学习模型，所以电池是包括在内的。
【12：18-12：27】如果你实际上正在做数据处理，你可以实际使用数据核心和数据集，你可以使用现代，如果你实际上想要以分布式方式做熊猫。
【12：27-12：39】你可以在Ray上使用Dask，很多人实际上这样做，如果你正在使用SPACI，如果你正在使用Tensor Library进行大量计算，你可以在Ray上使用Mars。
【12：39-12：47】这允许你以有纪律的方式做这些事情，如果你正在训练，对吧，深度学习训练，你可以使用PyTorch、TensorFlow或Herobot作为插件。
【12：47-12：55】连同这个特定的库一起做那个服务是另一个你可以实际使用的，一旦你的模型被训练和部署，你实际上可以使用Ray服务。
【12：55-13：05】来部署这些模型，RASO允许你构建这些管道，允许你拥有所有这些不同的模式，你实际上看到ML模型是如何在生产中部署的。
【13：05-13：17】最后，一个非常常见的工作负载是超参数调整，对吧，它与超优或optuna集成在一起，无论你想使用什么搜索算法，无论你想使用什么调度，
【13：17-13：27】你可以使用超参数调整来分发你的试验，最后如果你是一个游戏玩家或者你在做模拟，如果你正在使用RecSystem，你实际上可以使用RLLib来构建模拟模型。
【13：27-13：38】来做那个特定的工作负载，所以这是一个相当丰富的生态系统，然后越来越多的人实际上在这些原始基础上构建东西，以实际拥有他们自己的Ray集成。
【13：38-13：43】所以这是一个非常高的水平，谁在哪里使用，我的意思是，我必须有这个。
【13：43-13：56】与这张幻灯片平行，我们实际上有一些值得注意的公司，他们在大规模使用RAI，亚马逊实际上在RAI上使用Dask来进行大规模并行处理，以摄取他们的模型中的数据。
【13：56-14：08】Dendrite正在使用超参数，Judy McKinsey，阿里巴巴，所有这些人都在使用阿里巴巴，实际上，他们有超过20万个内核，实际上他们在大规模上运行稀有。
【14：08-14：14】所以这些都是实际上以不同方式使用所有这些的公司。
【14：14-14：20】所以刚才我要换档，我要稍微揭开面纱。
【14：22-14：30】你看幕后，你知道Ray是什么样子的，对吧，内部是什么样的，你打开引擎盖，你弹出引擎盖，然后你会看到一些东西。
【14：30-14：40】现在，如果你来自CS Spark，如果你来自DOSC，或者如果你来自几年前的Hadoop，或者HPC，这是一个常见的架构。
【14：41-14：48】这个想法并不太不同，你知道，你有一些实际运行的进程的工作节点。
【14：48-14：59】头节点和工作节点之间的唯一区别是，你有一个额外的称为驱动程序的工作进程，它可以运行在任何地方，它只是碰巧在这个节点上运行。
【14：59-15：10】然后你实际上拥有我们称之为全局控制存储的东西，它就像一个全局元目录，跟踪所有可用的资源，谁在做什么，这就是它的大脑。
【15：10-15：20】然后你实际上拥有的是这些railets，它们是你的分布式调度，所以railets就像点对点通信，它实际上告诉你。
【15：20-15：30】我在做什么，我的资源是什么，它们负责创建你的工作进程，所以每个工作进程都有一个正在运行的进程，它与一个核心绑定。
【15：31-15：39】然后你有一个分布式对象存储，它在共享内存上运行，对象在全局客户之间共享。
【15：39-15：49】过去当任务需要时，所以这是Ray的独特之处，没有人真正使用这个，因为分布式调度和对象巡游。
【15：49-15：59】我们可以非常容易地扩展它，我们可以让成百上千的任务异步运行，并使用这些工具，所以如果你实际上有需要某些东西的工作进程，
【15：59-16：08】在一个特定的节点上，还有其他我们已经创建了数据的过程，你可以使用共享内存来执行零复制，这实际上是非常独特的。
【16：08-16：21】所以让我再往下走一级，告诉你每个节点发生了什么，正如我所说，每个节点都有这些工作进程，工作进程是，
【16：21-16：33】如果你们有十二个核心，你们将有两个工作进程，而relay是那个实际进行通信的人，所以每当你要安排一个特定的工作时，relay就是那个说。
【16：33-16：41】或者我是否有四个GPU所需的资源，以及我需要多少内存，所以是的，我要接受那个，我要运行那个特定的任务。
【16：41-16：53】它将你所有的资源都传达给全局存储，然后全局元存储每十毫秒广播一次图片，集群的实际样子。
【16：53-17：02】每个节点有一个relay，它实际上负责协调所有这些，这就是它的分布式性质。
【17：02-17：12】Global Store是一个键值存储，目前我们使用Redis，但你可以使用任何你想要的可插拔组件，还有一个在上面运行的进程，它管理和与之通信。
【17：12-17：16】所以这就是幕后发生的事情，基本上就是这样。
【17：17-17：25】那么现在让我一路向下到一个稍微低级一点的层次，存在的分布式设计模式。
【17：25-17：35】现在，设计模式并不是我们意识中的新事物，你知道，大多数软件开发人员可能已经读过或听说过。
【17：35-17：44】书或他们发誓，通过那本书从gango for和那些是设计模式，当面向对象编程实际出现时，这些设计模式被引入。
【17：44-17：52】并允许我们重用对象，并引入诸如信号转换器和装饰器等设计模式的概念。
【17：52-18：03】迭代器、协程等等。因此这不是什么新鲜事，所以RAID所做的就是建立在这个特定的设计模式上，引入自己的设计模式，
【18：03-18：11】并且有三种与Ray相关的设计模式，显然有很多设计模式，但这是三个。
【18：11-18：23】引入了Ray的概念之一是，所有函数都可以转换为我们所谓的并行Ray任务，这些任务实际上在Ray上作为执行单元运行。
【18：23-18：33】这些可以在集群中的任何地方，在任何工作程序或任何驱动程序中分布，任何工作程序都可以是一个驱动程序，它实际上执行该任务，所以没有一个中央位置，驱动程序决定一切。
【18：33-18：42】任何工作程序都可以提交任务，第二个是对象存储或Ray对象是未来的，对吧？它们是对象引用。
【18：42-18：53】当特定任务完成时，会返回给你，所以如果Python创建了一个对象或Python返回了该对象，你不会立即得到那个对象，你得到的是一个句柄。
【18：53-19：01】作为一个未来，所以你们中处理Scala未来或Python未来的人，这是一个你可以实际询问的引用。
【19：01-19：05】这些是不可变的，存储在对象存储中，对吧，共享内存。
【19：06-19：13】第三个是演员，如果你不熟悉演员，这些是你可能在Arkao Airline中听说过的演员。
【19：13-19：29】但它们之间的区别在于，这些演员是非常有状态的，它们使用对象存储来维护特定的状态，这些反应器是你实际看到在所有本地库中使用的根本设计模式，我们稍后会看到我的意思。
【19：29-19：42】因此，这些是有状态的服务，实际上允许你维护特定机器学习算法的特定状态，你正在运行，所以特别是当你进行率调整或训练参数时，你正在运行试验，演员正在运行试验。
【19：42-19：51】保持该状态，所以如果出现问题，他们实际上可以从他们离开的地方开始，现在这里有关于并行编程模式的链接。
【19：51-20：00】RAID设计模式，这只是三个，但大约有六七个，对于那些想要编写大规模分布式应用程序的人来说，它们更高级。
【20：00-20：11】还有关于如何集成库的设计模式，我提到了第三方库，它们有不同的集成级别，稍微多一点一级、二级和三级，所以如果你有兴趣将你的库集成到Ray中。
【20：11-20：22】你遵循这些设计模式，你可以无缝地在三个不同的级别上与Ray集成，每个级别都有不同程度的难度或容易程度。
【20：23-20：35】那么我们来看一下设计模式函数，正如我所说的，可以转换为任务，它们可以在节点上的任何地方运行，一个类可以转换为有状态的actor，actor可以具有设计和方法，实际上可以在节点上运行。
【20：35-20：47】然后你有分布式对象straw，你的对象现在可以变成一个引用，它可以在节点中的任何地方被引用，并且有一个节点的主要所有权的概念。
【20：47-21：01】因此，在节点a上调度的任何函数，如果它创建了一个特定的值并返回一个值，那个特定的节点将拥有该元数据的所有权，但那实际上是主要副本，所以那个节点将拥有那个特定的数据。
【21：01-21：10】如果另一个任务在其他地方被调度，并且那个任务需要它，那么它将使用GsvRPC复制过来。
【21：10-21：14】所以这就是三种设计模式，让我们看一些代码，对吧？
【21：15-21：23】所以我有一个函数，我想把它转换成一个任务，你只需要用雷达远程注释那个特定的函数。
【21：23-21：35】现在，这将被转换成分布式任务，我有两个函数，一个读取numpy数组，然后只是把它们加在一起，然后，当你想要调用那些东西时，你所做的是。
【21：36-21：45】使用函数名，然后用点远程注释，然后传递参数，此时会发生什么，它将在某个节点上被调度。
【21：45-21：53】并且它会立即异步返回，这是它的强大之处，因为这样你可以实际扩展很多任务，它会返回给你一个id引用。
【21：53-22：01】然后你可以实际使用第二个函数来调用第二个文件，然后你可以只使用这些引用将其发送到第三个函数称为远程。
【22：01-22：10】然后你可以只做一个get，所以所有这些未知的阻塞调用，唯一会阻塞的代码是最后一个。
【22：10-22：18】在这里，节点数组一将有一个文件一，它实际上被发送或是一个路径，它将从GC存储或AWS中读取，或者如果是，
【22：18-22：30】如果它在一个本地文件系统中，你已经填充了它，那么它将读取它，并记住它将动态创建图形，第二个将在节点二上被调度，它将读取那个特定的文件。
【22：30-22：38】现在你要把它发送到add remote，它将创建一个图形，然后立即返回你任务，然后，当你实际做一个get。
【22：38-22：47】它们在后台运行，如果它们完成了，这不会很容易地阻塞，但如果它们是长时间运行的任务，它们将等待直到完成。
【22：47-22：55】它将返回回来，所以这基本上就是任务API背后发生的事情。
【22：55-23：04】这个概念是我们想动态和异步地运行事情，因为我们不想等待。
【23：04-23：17】按顺序同步和其他分布式系统所做的那样，我们实际上要等到一个动作被调用或一个突然的调用被执行，然后它才会执行这里的特定内容，事情会动态发生。
【23：17-23：27】 动态地来说，它之所以重要，是因为在某些机器学习算法中，你决定如何动态创建下一个任务时，必须确切知道前一个任务的结果。
【23：27-23：39】 这样它就可以急切地执行事情而不是懒惰地执行。我想谈的概念是分布式对象存储，正如我所说，这是它相当独特的东西，我们有一个共享内存，它运行在，
【23：39-23：54】 老的工作者之间，你可以把共享内存看作是一个Apache Plasma Store，它是Pi Arrow版本的Plasma Store。
【23：54-24：06】 它实际上是在所有节点之间共享的，所以现在如果你有特定节点上运行的工作进程，并且它们需要其他进程创建的一些数据，你只需要做一个零拷贝，就像这样，如果你有numpy数组或者如果你有张量，这些是你保存的，
【24：06-24：16】 进程A可以直接读取指针，这样它们就不做任何IPC了，所以性能非常高，如果以某种方式你的数据没有填满共享内存存储。
【24：16-24：29】 它总是可以溢出到数据，并且所有的跟踪实际上是由元数据完成的，说，好吧，这个存储在这里，但它已经溢出到磁盘上的这个特定分区，这个特定扇区，所以我们需要那个特定的数据。
【24：29-24：39】 再次加载回来，如果有旧的内存坐在那里进行垃圾收集，所以这一切都是在幕后完成的，你不必担心，但我认为我会和你分享这一点作为它的一个重要方面。
【24：39-24：48】 因此，这里是一个关于数据局部性如何工作的例子，我有一个分布式的不可变对象，我有一个实际正在执行任务的节点一。
【24：48-24：57】 它返回一个值X。我还有一个函数G，它实际上得到一个值。当我做F点远程时，它将返回Y。
【24：57-25：06】 它将在节点二上调度，假设，然后我发送g远程的对象引用，它会立即返回给我。
【25：06-25：17】 发生的事情是现在G将因为数据局部性而在节点二上调度，它将只是从共享内存中读取数据，然后返回G的值，
【25：17-25：27】 然后我可以对它进行get操作，现在实际上有了这个值，所以你可以看到对象存储是如何形成的，以及异步性。
【25：27-25：34】 并行执行的任务允许人们以大规模的方式运行事物。
【25：34-25：48】 所以，只是为了让你了解事情是如何运作的，例如，你知道，我有一个雷达远程，它将被安排在某个地方，现在它实际上是如何工作的，再次，在这里，我展示了一个驱动程序，但任何工作程序都可以实际执行它，只是碰巧我的代码没有在驱动程序上运行。
【25：48-25：59】 所以它会问这个本地中继，嘿，我需要调度这个任务，这个任务需要五个CPU或一个CPU，你实际上有资源吗？
【25：59-26：11】 它会给它一个列表并说，是的，我有，继续执行它，那个特定的驱动程序将序列化代码，现在工作程序实际上被使用。
【26：11-26：23】如果工人实际上有，如果代码在双工内调用另一个函数驱动程序现在不涉及其中，现在工人是那个将要不提交另一个节点的任务的。
【26：23-26：32】它会说，哦，我需要一个GPU，但我没有。我要问一下reler。你真的有资源去做吗？我没有。但是节点三有。
【26：32-26：40】它为它获取最少的，并且保持最少的执行非常相似的任务，这就是我们如何实际扩展的，这只是一个算法，它实际上是如何通过的。
【26：40-26：50】获得租约等等，所以我不会让你们烦恼如何安排租约，这样我们就不会做这个恒定的GRPC。
【26：50-27：03】所以一旦我有了列表，我可以有成千上万的任务，它们是类似的类型，需要类似的资源，可以在那台特定的机器上调度，如果他们有十六个核心，他们将在那里运行，如果他们有三十二个核心，他们将在那里运行。
【27：03-27：11】如果他们超出范围，他们将在第二个节点上调度，所以所有这些实际上都是动态发生的，所有这些调度策略都是动态完成的。
【27：11-27：20】你不必担心那个 所以我认为这是这个，这是我现在想谈的。
【27：20-27：27】在我真正进入之前，这是一个好主意，在我进入之前，看看这里是否有任何问题。
【27：27-27：37】好的，也许我们会坚持到最后，很棒 好吧，所以现在我想谈谈的是Ray生态系统和和我说的那样，我不能谈论所有的库，鉴于我们实际拥有的时间。
【27：37-27：50】所以我们打算做的是我会谈两个，我可能会跳过Xe boost Ray，因为当我做演示时，我会确切地谈论Xe boost day rose的主题，但我想谈谈Raytune，因为如果你是一个数据科学家，如果你是ML。
【27：50-28：00】调整是非常重要的机器学习工作负载方面，你通常会这样做，你知道你会创建你的基线，然后你会使用超参数调整来实际做到这一点。
【28：00-28：07】然后Tune是Ray中一个非常灵活、强大的库，允许你在集群中大规模地进行调整。
【28：07-28：15】所以让我们来看看那个，那么什么是非常调谐的呢？非常简单的库支持。
【28：15-28：27】来自最新文献的最先进的算法，允许你并行运行试验，因为它在RAE上运行，所有的编排，所有的分布式编排你的试验，
【28：27-28：36】但配置空间是并行完成的，所以这让你摆脱了以顺序方式完成它的负担。
【28：36-28：49】每个工作人员或每个ray都会得到配置空间的一片，你实际上会有你的试验，它将通过电机训练来进行超参数调整。
【28：49-28：57】它易于使用API，最重要的是，你只需使用tune，你有你的训练函数，你只需运行train model。
【28：57-29：07】你可以在单个进程中运行这些，对吧，如果你真的想要，它可以使用所有的代码，或者你实际上可以使用同一台机器上的多个GPU，或者如果你真的想要使用多节点。
【29：07-29：21】多集群，你也可以实际使用它，所以它非常简单，它与所有兼容的机器学习库非常容易集成，这些库实际上对于超参数调整非常重要，因此XC boost ray，scikit learn。
【29：21-29：31】Tensorflow pie toward carousel，你命名它，所有这些库都可以供您使用，关于这一点的好消息是这个概念在真实的库中。
【29：31-29：44】你可以让其他库作为子程序运行，所以我的训练模型可以是一个PyTorch函数，这实际上是另一个库，对吧，但我正在REC内运行它，这是一个强大的概念，你需要掌握它。
【29：44-29：54】然后它与其他所有库都非常互操作，所以如果我使用RAID二，我可以使用RAID train将我的转换为分布式训练。
【29：54-30：04】我可以使用ray数据集来在我的超参数调整需要并行执行任务的所有工作器上分片我的数据，所以它实际上相当强大。
【30：04-30：12】只是为了给您一个快速的最新搜索算法的概述，今天Ray。
【30：12-30：23】你知道，默认情况下，你会做随机网格搜索，你有贝叶斯带优化，所以这些都是我们实际支持的非常简单的内置算法。
【30：23-30：35】为了让你使用它们，当你实际创建一个ray tune时，对象，库，你只需创建一个搜索算法的实例，你实际上需要并且会使用那个搜索参数与这个超优。
【30：35-30：42】无论是Optuna，我们支持它还是它与Ray集成，然后你实际上会使用它来搜索你的超空间。
【30：42-30：50】你也可以使用调度程序，对吧，所以调度程序是根据你可能拥有的某些策略来安排试验的。
【30：50-31：00】你可以使用Asha进行超参数调度，减半算法只安排做得好的事情，其他事情被丢弃。
【31：00-31：08】你可以使用Hyperband，你可以使用所有这些基于人口的训练调度程序与RayTune，以便能够在大规模上调整事物。
【31：08-31：18】如果你是超参数的新手，如果你是HPO的新手，超参数到底是什么意思，调整到底是什么意思，对于那些新手来说？
【31：18-31：31】我相信对于那些熟悉的人来说，这只是一个小问题，所以你知道有两种类型的参数，对吧，有模型参数，这些是你在实际运行试验时实际学习的，
【31：31-31：43】这些是权重，这些是偏差，这些是线性回归模型中的所有东西，在网络中，这将是传递给每个层的权重的参数。
【31：43-31：54】超参数是在训练之前设置的，它们决定了学习参数将如何表现，这些可能是你知道的学习率可能与你的管道配置有关。
【31：54-32：02】你实际上想要每个和每个学习率在你的，比如说，心理学习中，如果你使用的是基于树的算法，树的数量，深度。
【32：02-32：14】你将多频繁地进行随机操作，这些是定义超参数的东西，这里还有另一个超参数调整的例子，如果你在调整。
【32：14-32：24】一个特定的网络，你知道什么类型的层，什么类型的过滤器你使用你知道你的最大池化会是什么，有多少密集层，你可以有这些参数，你可以问你的训练。
【32：24-32：33】算法来找出，给定你实际拥有的空间，它将根据你实际想要的最大或最小损失得出最佳最优算法。
【32：34-32：44】所以这就是超参数的大致内容，现在有一些挑战，你知道，当你在大规模上进行超参数调整时，这可能需要，你知道，
【32：44-32：55】从几分钟到几小时到今天，所以你必须意识到这是一个耗时的过程，因为你有一个大的超参数空间，并且你想拥有最好的模型。
【32：55-33：04】它实际上也可能很昂贵，而且有些挑战在这里，以确保它将以最优化的方式使用资源，它必须是弹性的，它必须是完全容错的。
【33：04-33：16】所以你可能在大约五个小时内进行HPO，突然间其中一个服务器宕机了，然后会发生什么？你必须重新开始工作，不，你不必，所以我认为这些都是你必须处理的超参数挑战。
【33：16-33：24】Ray试图解决这些问题，特别是Ray Tune试图通过三种方式解决这些问题，你可以实际使用穷举搜索。
【33：24-33：32】并继续前进，直到找到最好的一个，或者你可以使用更贝叶斯优化的方法，你只。
【33：32-33：40】按顺序从前一个中选择结果。所以现在你可以说，好吧，我要放弃这个，但我将使用这个，因为。
【33：40-33：50】前一次试验是一个更好的结果，我会这样做，或者你可以做高级调度，比如ash所做的那样，或者其他更先进的算法。
【33：50-33：58】具有某些策略，例如，早期停止或减半算法，只有在。
【33：58-34：03】有有希望的试验时才会继续进行，所以如果我们看看每一个，你知道。
【34：04-34：12】你可以很容易地进行穷举搜索，因为它很容易实现，你只需要一定的网格，你可以并行化那个对吧，你彻底地遍历那个。
【34：12-34：22】随机搜索也是如此，你均匀地遍历你的参数扫描，你这样做，但这是非常低效的，对吧，因为我们不知道，我们没有跟踪。
【34：22-34：35】前一个的网格参数应该实际使用那个组合，所以它是低效的，但是今天的文献实际上正在改变，他们正在使用更高效的算法来实际并行化那个。
【34：35-34：46】所以你可以使用，例如，前一个的结果来决定你接下来要做什么，贝叶斯优化是另一个，如果你看这张特定的图表。
【34：46-34：59】如果你使用贝叶斯优化非常调优，它会尝试找出我最好的最小损失是什么，然后我会在周围创建一个特定的空间，如果其他配置落入其中，那么我就要跟着它。
【34：59-35：11】其余的配置我会把它扔掉，所以这是一种很好的方法，它是固有的顺序，因为贝叶斯优化依赖于以前的试验结果，
【35：11-35：19】看看我是否想为我的下一个试验安排类似的配置参数，或者我只是想把它扔掉并把它放在红色区域。
【35：19-35：29】蓝色区域中的那些是落入其中并做到这些库RayTune与HyperOpt集成在一起的，如果你熟悉HyperOpt，你想使用它。
【35：29-35：38】就像我说的，你只需要创建超参数搜索并使用它，砰的一声，Ray将负责那个optoma Skype，Psyche优化了新的一个已经被。
【35：38-35：48】已经集成并且从未毕业，这是另一种方法，你可以实际减少成本来做到这一点，第三种是Ray实际上使用的，即早期停止。
【35：48-35：56】它所做的就是将所有这些初始探索扇形展开到你的配置空间，并且它们只会使用中间的。
【35：56-36：08】结果来自树或样本，它会动态修剪，因为它实际上会进入，所以我们从大量开始，而那些表现不佳的试验，他们使用halbin算法来实际。
【36：08-36：18】提前停止，这样可以降低成本，它不会等到彻底达到那个目标，一旦它看到经过几个阶段后损失没有变化，也没有什么变化。
【36：18-36：28】它就会放弃那个，他们会挑选下一个，正如你实际上可以看到的，随着时间的推移，那些表现更好的，你会开始继续沿着那些进行。
【36：28-36：39】所以这些都是Raytune实际使用的一些策略和政策，只是为了给你一个代码示例，让你知道使用Raytune有多容易。
【36：39-36：50】非常简单的步骤，这就像你的pyTorch函数，你实际上创建了一个训练函数，你定义了那个特定的函数，你创建了一个特定的模型，你将有一个你要经历的时代，
【36：50-36：58】那是你唯一的函数，你定义你的训练函数，你把它交给tune，对吧，所以tune run是那个将要。
【36：58-37：04】检查那个特定的函数，现在你要提供你实际想要的均匀配置空间。
【37：04-37：18】你想要的样本数量，你实际想要的试验数量，比如在这个例子中，每个时期的数量，然后你想要使用什么样的调度，对吧，你想使用ASHA来做早期停止，所以你实际上可以使用如何算法来为下一个试验做出更好的决策。
【37：18-37：29】你想使用什么样的搜索算法，你想使用opt in，你只需指定它，然后当你说运行tune时，在这一点上会发生什么？
【37：29-37：38】你的驱动程序进程，其中驱动程序正在运行，将创建这个名为read，tune，dot，run的函数，这是主要的演员或主要的跟踪器。
【37：38-37：46】它将在我们称之为工作进程的这些演员上启动，每个演员都会有一个你的集中函数的副本。
【37：46-37：59】它将拥有你配置的一部分，并且它将并行运行，它将做什么是在试验结束后报告指标，这样跟踪器就可以说好的下一个我想协调的是什么。
【37：59-38：07】或者我应该只是放弃那个还是早期停止，我做得不太好，或者我只是减半，诸如此类的事情，所以报告指标回去。
【38：07-38：17】如果有早期停止，它会停止那个特定的工作进程，然后我们会用一个新的配置启动一个新的试验，这种做法会继续下去，如果出了问题，你知道我们定期做检查点。
【38：17-38：27】试验检查点可以在本地目录或云中的中央位置完成，因此如果工作进程关闭，我们可以实际加载检查点并开始。
【38：27-38：34】从最后一个检查点恢复并开始训练，所以你不会你知道你不会训练所有的数据，你只是从最后一个检查点开始训练，你继续前进。
【38：34-38：41】所以本质上这就是ray train，XGBoost实际上所做的。
【38：43-38：49】让我在这里暂停一下，看看有没有问题。
【38：56-39：03】非常好，好的，我要做的是我现在要在这里提出，我要画草图，我要跳过。
【39：05-39：14】XU boost ray，因为当我真正要做演示时我会讲到这一点，一些设计特点，但只是在非常高的层次上。
【39：14-39：19】嗯。
【39：20-39：28】只是在非常高的层次上，你知道展览真的是一种分布式训练，对吧？那里有展览，你可以使用。
【39：28-39：35】在单台机器上的单个课程上进行分布式处理，但这实际上是Ray，它实际上允许你将你的训练扩展到。
【39：35-39：47】跨多个核心，就像数据并行一样，实际上，每个工作进程都会得到特定模型的一个副本，你用它来处理数据的一部分，它是非常容错的。
【39：47-39：55】因为它使用了那个，所以我把这些链接放在这里，我会得到那个我真的不想深入进去，因为我想要你知道我想向你们展示如何。
【39：55-40：04】你实际上在特定的集群上扩展那些东西，所以让我们来做这些，然后去我的演示。
【40：05-40：14】你们能看见吗？看起来不错，看起来不错，好的，所以我要在这里向你们展示的是。
【40：14-40：24】在你的日常旅程中，当你在笔记本电脑上构建一个特定的模型，或者最终你会使用你拥有的集群。
【40：24-40：36】你将使用三个非常常见的工作负载，你要扩展，一个是训练，你实际上想在大规模的数据上进行训练。
【40：36-40：44】第二个工作负载，在你的日常迭代过程中关于模型构建是非常常见的，是超参数调整，即你实际上是如何获得最佳模型的，对吧？
【40：44-40：54】创建一个基线，但然后你有一个配置空间，你想进行超参数调整，然后那就是第三个，推理，你知道一旦你实际上有了最好的模型。
【40：54-41：06】你实际上是如何在特定规模上进行推理的，所以这里的想法是，我们将在一台具有有限容量和有限核心数量的本地机器上尝试做一些事情。
【41：06-41：16】我们会看到它实际花费的时间是巨大的，然后我们可以将其扩展到一个射线集群，并看看它在时间上的表现如何。
【41：16-41：28】所以这三件事是我想要做的，所以我在这里所做的就是我只是一个普通的分类算法，我将要使用。
【41：28-41：41】展览训练那个分类算法，只是普通的导入，没有什么不同，我大约有1000万行，我要对其进行分类。
【41：41-41：54】大约四十个不同的特征和两个类，我想这样做，这段代码只是读取文件并使用射线数据集将其分片到我的集群中。
【41：54-42：02】我有三组文件，因为我想要使用三种不同的方法来测试它们，三个兆字节，三个千兆字节，和十一个千兆字节的文件。
【42：02-42：10】所以让我们看看每一个，首先我们要做的是我们将使用常规的XGBoost进行训练，对吧，所以当你使用你的常规XGBoost时。
【42：10-42：19】这是你的训练函数，你实际上会使用XGBoost，没什么不同，对吧，你要在这里做的就是定义XGBoost参数。
【42：19-42：27】你要给你的属性训练器，我的目标方法是近似，我正在使用目标逻辑。
【42：27-42：34】逻辑分类评估确保你不会丢失和错误。
【42：34-42：48】我导入了我的XGBoost，我将导入我的DMatrix训练，这个叫做train的函数非常通用的XGBoost训练函数，你实际上会使用所有这些都是XGBoost的一部分，你会有你的标签。
【42：48-42：59】你要读取加载parquet文件，把它们分成七十五二十五，然后把它们转换成D矩阵，这是XGBoost数组如何放入数据结构的高效版本。
【42：59-43：08】你要创建你的训练函数，为什么你要设置参数。
【43：08-43：19】你要设置你实际想要的评估，你实际想要的结果，你是否想要详细信息，然后你要训练十个回合，然后这些只是你实际拥有的回调。
【43：19-43：30】非常标准的东西，你通常会使用，所以让我们看看这个函数是否真的有效，所以我要做的是我要试着确保这个函数在一个小数据集上工作，大约三兆字节，对吧。
【43：30-43：38】所以我会继续训练我的特定边界，这在我的Jupiter上运行，在一个本地节点上，大约有十六个核心。
【43：38-43：48】你可以看到，好吧，这实际上有效，三兆字节，没问题，我要做的是看看我是否可以在最大的数据集上尝试，这是三个，三个千兆字节。
【43：48-43：59】现在我不打算运行它，因为你知道这将需要大约四分钟，所以我只是展示我刚刚在那之前运行了这个，你可以看到，当我用三个千兆字节的文件在本地机器上运行XGBoost时，只有三个千兆字节的文件，
【43：59-44：06】 大约需要三分钟，或者说是两分半钟左右，对吧？
【44：06-44：16】 那么，如果我使用展览Ray呢？展览Ray的工作方式是这样的，它有一个概念，记得我告诉过你。
【44：16-44：31】 演员们的工作方式，你有一个跟踪器，它实际上创建了你实际想要的演员数量，每个工作者都会得到一个关联的演员，在这个演员上运行，它将拥有运行所需的数据。
【44：31-44：40】 所有这些演员都使用基于树的减少算法RABIT相互通信，这就是他们如何同步梯度的。
【44：40-44：51】 然后它们返回到跟踪器，所以这就是Exegggoost在Ray上的工作方式，它是从使用普通的Ray中删除和替换，但现在你实际上正在运行。
【44：51-45：03】 在集群中以分布式的方式而不是在同一节点上，所以一切都是相同的，完全相同，我提供我的xgversed参数，相同的参数，
【45：03-45：13】 我在这里唯一改变的是我从Exibus ray导入Exibus ray，ray数据库，ray d矩阵版本的d矩阵，这是，
【45：13-45：24】 它实际上高效地存储数据的方式，我要导入训练算法，训练函数，以及ray参数，我会展示它是什么。
【45：24-45：36】 这实际上是相同的函数，这里唯一的区别是我现在使用相同的代码，相同的数据，但我使用rate指标，它现在将在我的工作者之间分布。
【45：36-45：44】 然后我提供相同的参数，除了这里的区别是我有一个额外的参数叫做reperums，我会在一会儿谈论这些reperums是什么。
【45：45-45：55】 所以我们要做的是定义我想使用的并行性，这与我上面执行的代码完全相同，只是现在我有。
【45：55-46：05】 我正在使用Xd boot参数，我发送的数据文件是相同的三个千兆字节的文件，现在我告诉array要使用。
【46：07-46：15】 八个演员，八个演员或八个工作程序在每台机器上，并使用十六个CPU，对吧，所以我只是继续训练它。
【46：15-46：27】 让我们看看这是否有效，所以现在，这实际上在我的任何规模上创建的Ray集群上运行，你可以看到它使用环来执行。
【46：27-46：39】 执行所有映射减少以与梯度通信，并且现在它实际上在所有八个不同的节点上运行，使用所有提供的GPU。
【46：39-46：48】 所以这实际上是在大约十三秒内完成的，所以这比在我的本地主机上运行快了大约三倍，在我的本地节点上。
【46：49-47：00】 所以这就是Ray训练，展览Ray，你可以在节点上使用小数据集来查看它是否有效，然后在更大的数据集上进行训练以实现这一点。
【47：00-47：10】 因此，你实际上可以看到在本地节点上运行和在Ray中跨集群运行之间的巨大差异，当你实际使用Ray并行化它们时，它大约需要十三秒，这比。
【47：10-47：19】 你知道，比本地节点上的那个快得多，人们通常会遇到的第二个工作负载。
【47：19-47：32】在构建模型时，重复做很多次的是超参数调整，而超参数调整再次非常内置，您实际上可以使用XGboost。
【47：32-47：44】您的训练函数在那里进行扩展，因此在这里我几乎有相同的展览配置，我使用二进制损失，然后我提供我的超参数调整，使用这是我的。
【47：44-47：54】ETA学习率从我提供的参数中均匀分布，样本深度从一到九。
【47：54-48：04】然后我建议我实际上如何并行化它，所以我想要我没有任何gpu，我只是要使用新的演员，我要使用每个演员两个gpu。
【48：04-48：14】我将拥有八个演员，他们将在我的节点上运行，然后我只把它交给调谐点运行，并且然后提供我的。
【48：14-48：25】Xe boost函数，我想要的文件，它们是三吉字节，我想要的后端参数，以及进度条，然后我开始并行运行这个特定的。
【48：25-48：36】这就是这里发生的事情，现在它将出去创建八个演员并开始使用，所以你实际上可以看到它实际上正在使用一百三十六个GPU。
【48：36-48：49】和十四g的所有一百四十四g CPU，我现在实际上正在跨我的超参数状态并行进行训练，所以我有八个演员，每个演员都获得了一个配置参数的副本。
【48：49-48：57】它只是继续进行训练，在某个时候它会终止，现在我正在并行运行八个。
【48：57-49：08】试验，这是我最后一次试验的最佳参数，目前我应该大约有，是的，一个已经终止，我有七个正在运行。
【49：08-49：19】你可以看到现在它实际上正在使用一百一十九个CPU，下降了八十五个，因为我有三个已经终止，终止并不意味着它停止终止，它已经完成了一次试验并且得到了结果。
【49：19-49：26】这是完成的，我还有五个正在运行，所以这里实际发生的是，正如我说的。
【49：26-49：40】这正是这里发生的事情，所以现在你有一个由演员并行运行的试验，他们向后报告指标以查看这些指标的实际表现，并根据该指标启动第二个试验。
【49：40-49：49】所以我们大约有五个，七个终止，一个正在运行，你可以看到它现在实际上只是使用Simt和CPU来完成最后一个。
【49：49-50：00】它在大约八十二秒内终止，如果我在我的笔记本电脑或我的单个节点上运行这个，这将需要你相当长的时间。
【50：00-50：12】因此，超参数调整是你经常使用的东西，为了扩展它，你实际上可以在ray集群上使用ray tune来为你提供最佳参数，这是我的最佳近似值。
【50：12-50：20】我的参数，我的ETAs是零点一phi，子样本是phi，下一个最大数字是三。
【50：21-50：33】所以第三次在这里，好的，所以第三次和最终的训练，大多数人实际上做的工作负载是推理，对吧，你实际上想在大规模上进行推理。
【50：33-50：42】所以现在你训练模型，你得到了最佳参数优化，你要训练这个特定的模型，现在你想做推理来看看它实际上是如何表现的。
【50：42-50：53】让我们先试着在一台机器上使用几乎相同的模型进行预测，但除了在这里，我们实际上并没有发送。
【50：53-50：59】我们没有使用d矩阵部分分发图像，让我们运行那个。
【50：59-51：12】所以这将需要大约90秒，我相信，来做这件事，所以这里发生了什么，它实际上正在获取三吉字节的文件，并且实际上正在对其中的每一个进行预测。
【51：12-51：19】它以一种顺序的方式在一个节点上进行，并且实际上正在通过这些特定的数据。
【51：24-51：29】这是在运行，让我检查一下。
【51：32-51：47】好的，所以我们有大约七吉字节，所以我们可能还有另外三个要走，所以你可以看到这实际上是在真正地运转，它没有照顾到，它没有真正利用这样一个高维集群的事实。
【51：47-51：55】我可以并行地进行推理，而不是按顺序进行，所以我们快到了，这可能需要大约。
【51：59-52：04】再过几秒钟，但我们应该完成了。
【52：11-52：19】所以我们已经通过了所有的十一吉字节，他们应该实际上给我们一个计时，它是如何实际花费的时间来。
【52：19-52：27】得到结果，所以他花了大约八十一秒，大约一分半钟左右。
【52：27-52：36】让我们看看我们的推理结果，这就是我们的分类算法和返回的结果，现在我们要做同样的事情，我们要在x等于Ray上做同样的事情。
【52：36-52：46】你看到这里唯一的区别是，现在我正在使用RD矩阵来获取我的推理数据参数，然后我唯一发送的东西与之前的不同，
【52：46-52：57】是我正在说使用九个演员来并行进行推理，所以这将在并行中运行，我现在创建了八个远程演员。
【52：57-53：02】我只是在并行中进行推理，那应该几乎立即返回。
【53：04-53：16】这花了大约九秒钟，大约十倍快，所以你实际上可以看到差异，你实际上可以如何本地扩展事物和远程扩展事物，关于Ray的一件很棒的事情是你真的。
【53：16-53：26】可以使用你的笔记本电脑进行波段分割，然后当你想要做一些事情时，当你想要在集群上运行一些东西时，你只需使用Rate连接到集群。
【53：26-53：35】它连接得非常好，所以这些是我的结果，你可以看到它们相当相同，所以我有一些你知道的笔记本电脑在这里，它们。
【53：35-53：47】更深入地研究这个问题，但这是一个人们实际上经常做的非常常见的工作负载，Ray Tune是允许你这样做的一种库。
【53：47-54：00】它与其他库配合得很好，我不必使用XGVous，如果我想使用PyTorch，我在这里的训练函数现在将是一个PyTorch函数，而我的模型在这里将是一个卷积网络，我实际上拥有。
【54：00-54：07】然后我的训练功能将被赋予给射线训练，继续进行，如果我有一个超参数空间，我提供的我想。
【54：07-54：19】我的密集层在这个特定的范围内，我的过滤器是那种类型的，我的连接网络是这个集合，它会找出最佳参数。
【54：19-54：28】或者特定的空间，所以它非常强大，在你实际获得的好处方面是巨大的，并且节省了时间和成本。
【54：28-54：39】所以我认为这就是我在演示中所拥有的，我要做什么，让我回到我的幻灯片这里。
【54：39-54：44】在结束之前快速地。
【55：16-55：25】哦，所以是的，我在这里举一个例子，你实际上看到我在使用简单的API示例来使用我的XGBoost数组而不使用数组。
【55：25-55：37】而我在这里唯一改变的是，我做了导入，导入了放射性测量，展览射线，我想要的并行度。
【55：37-55：45】以及从射线中训练函数，我只是改变了这个特定的思维，比如说，我只是用射线训练发送那些参数和。
【55：45-55：55】现在我的训练函数以分布式方式运行，这只需要更改三到四行代码，采用你现有的实际提升方式，然后将其转换为并行。
【55：55-56：05】并行代码，就是这样，收获是你知道分布式计算将成为必要，你知道我们必须现在接受它，今天几乎一切都是一个gnome。
【56：05-56：18】Ray的愿景是使事情变得简单得多，使API变得简单得多，这是其背后的信条，对吧，我记得当我早期加入Databricks六年前。
【56：18-56：28】早期，我们的口号是让大数据变得简单，在这里，我们的口号是让分布式计算变得简单，你可以像在Python文件上编写它们一样使用你的Python函数。
【56：28-56：38】然后使用远程装饰器来进行分布式计算，这真的解决了困难的工作，你不必成为系统专家，只需使用非常远程来做所有的事情。
【56：38-56：47】并使用真实的库来扩展你的工作负载，以及提供给你做特定目的的东西的生态系统。
【56：47-56：58】我们有一些人实际上对强化学习感兴趣，我们实际上有一个会议即将在大约一个月后举行，它是免费的和虚拟的。
【56：58-57：10】你实际上可以在这里加入，当我发送幻灯片时，你也会在这里拥有URL，然后，如果你实际上想参加这个关于上下文带宽的非常有趣的教程，如何使用离线。
【57：10-57：21】使用RealLib进行学习，由RealLib的创建者和维护者，主要维护者，为你提供折扣，只需使用他们的meetup代码，你就可以得到它。
【57：21-57：32】这真的很值得，因为Aria Live现在是一个新事物，人们正在大量使用它，如果你想开始运行，了解Ray，只需在本地机器上安装Ray。
【57：32-57：43】并开始使用它，我们有大量的文档，我放了很多代码示例，我也有关于它的教程，我们有一个meetup，我们已经开始了两年的复兴。
【57：43-57：52】 我们在1月举行了第一次聚会，我们将在3月2日举行另一次聚会，这将涉及RayTrain，这是新发布的深度学习库。
【57：52-58：01】 然后在3月30日，我将主持RACE服务聚会，如果你有兴趣使用RACE服务来生产化模型，请参加那个聚会。
【58：01-58：09】 我们有一个充满活力的社区，实际上有很多讨论正在进行中，加入我们并讨论Ray。
【58：09-58：19】 社区也有一个Slack，关注我们的媒体并在GitHub上访问我们，如果你喜欢的话，请给我们一个星。
【58：19-58：28】 所以我想非常感谢你，如果你想和我联系，Jules at endscale.com，在Twitter上关注我，我会关注你，我知道。
【58：28-58：37】 听起来可疑，但我一直用那句话，然后是的，通过LinkedIn与我联系，告诉我你参加了我的演讲，希望这样我能知道你是谁。
【58：37-58：46】 因此，本质上这里有很多东西是我所涵盖的，让我看看是否有任何问题。
【58：46-58：54】 大家，如果有人有问题，Jills会立即回答你的问题。
【58：56-59：01】 是的，不，我认为，是的。
【59：05-59：11】 哦，有一个问题。
【59：13-59：24】 如果我听对了，内部通信是每十毫秒，如果我想在更短的时间内做某事，我想我可能误称了它，我认为它比那短得多，有一个可配置的参数。
【59：24-59：32】 但我会和你核实一下Tom，我认为十亿秒是，可能是十亿秒，但我会核实，对，我会核实那一点，并确保它是正确的。
【59：32-59：41】 我认为是十毫秒或某些其他地方，它实际上检查对象存储现金以进行驱逐。
【59：41-59：46】 我认为它要小得多，心跳要小得多。
【59：47-59：54】 但我将验证那一点，十分钟秒听起来不对，它要小得多。
【59：56-00：04】 好吧，让我们看看我们是否真的在聊天中有任何内容，还有人有其他问题要问Jules吗？
【00：08-00：13】 你要问我为什么，好的，我听得很清楚。
【00：15-00：21】 这是一个前Spark VW可能会问我，好吧，为什么Spark，为什么你在这里，有什么区别？
【00：24-00：32】 好吧，好的，这里我有一个问题要问你，Jules，是的，我可以把Ray和Spark集成在一起吗？
【00：32-00：40】 很好问题，这是一个非常好的问题，但在这样做之前，让我先和你分享这张幻灯片，因为我觉得我只是喜欢这张幻灯片，只是为了你。
【00：40-00：53】 多萝西，因为我演讲时你说，你知道，我们想知道两者之间的区别，我认为它们是非常互补的，真的，Spark和Ray是非常互补的，他们他们有点，
【00：53-01：04】 你知道，Ray Spark完成了一个突然的点，Ray接过了它，但只是给你一个非常高级别的两个之间的差异，它们是非常互补的，而且它们一起工作得很好。
【01：04-01：15】 你实际上可以在Spark上运行Ray，如果你实际上想做ETL数据处理，但只是一般性的高水平，我谈到了Ray是一个非常通用的分布式计算系统。
【01：15-01：25】 它不是针对特定负载的，它为你提供了这些较低级别的原语，细粒度的原语，让你编写任何你想编写的分布式应用程序。
【01：25-01：35】所以，Ray是一个用来编写其他框架的框架，对吧？想想这个，如果你在右边，你知道，Spark非常强大，性能非常好。
【01：35-01：44】对于特定目的的分布式数据负载，它是建立在数据帧抽象之上的。
【01：44-01：53】正因为如此，你实际上有更接近的API，比如DSL，所以通过数据帧提取在DSL中，你实际上告诉Spark该做什么，对吧？
【01：53-02：03】换句话说，换句话说，它是非常粗粒度的。另一个区别是Ray是一个非常分布式的调度器，没有单一的驱动节点。
【02：03-02：13】控制一切，对吧？而如果你看图是动态计算的，你使用动态执行的原因是因为，特别是当你处理。
【02：13-02：25】机器学习算法，如训练和调整，你必须决定你实际上将如何安排你的下一个任务，这是基于你刚刚计算的东西，所以这是一个非常急切的执行，它只是执行。
【02：25-02：34】图是动态创建的，而Spark有一个静态的时间表，对吧？我要创建我的数据帧方法列表，然后我将调用。
【02：34-02：45】一个动作，当动作完成时，整个甲板就会被执行，甲板会经过这些逻辑抽象、逻辑优化、物理优化。
【02：45-02：55】它创建了一个最终的RDD抽象，然后执行那个，对吧？而在这里，一切都实际发生，不是说一个比另一个好，这只是Sparco的性质。
【02：55-03：07】建立在一个数据帧抽象之上，就像dask建立在数据帧提取上一样，用于dask和bags，ray没有这种抽象的概念，对吧？它给你构建。
【03：07-03：18】其他东西我们使用分布式对象存储，换句话说，你知道一切都是实际共享的，而且非常异步，所以我们不会同步等待，而如果你看看spark。
【03：18-03：28】你知道，当你实际创建转换时，你将在你的特定数据帧上创建一个方法列表，并且它们将按顺序执行。
【03：28-03：37】而在这里，你只是执行一些东西，然后你会得到一个返回的引用，在稍后的某个时候，你可以实际询问我是否完成了那个任务。
【03：37-03：49】Ray并不是要取代任何库，我们真的在这里集成和解释非常开放，所以你看到的所有第三方库都不是取代，对吧？
【03：49-03：58】所以，如果你实际上想做，例如，数据处理，这不一定今天RAISE的优势，你知道，RAISE不是一个数据处理框架。
【03：58-04：07】它主要是为你自己写数据过程，你可以使用dash cam，对吧？你可以使用spark和Ray，所以本地库在那里是为了。
【04：07-04：19】解决那些工作负载，它本质上是非常Pythonic的，换句话说，如果你看看Ray代码，其他库是如何集成的，是非常相似的，显然我们实际上在这里有一个pi spark，它有。
【04：19-04：28】 有几种Scala套头衫，是的，Spark通过Horowood和PyTorch Intensive Flow支持深度学习集成。
【04：28-04：40】 而在这里，你实际上可以使用这个不同的级别，另一个问题是，嗯，你怎么用spark和Ray，这很简单，有一个叫做spark on Ray的项目。
【04：40-04：50】 这里是一个非常简单的代码，spark在红色上运行，所以你所要做的就是把你的驱动器包装在这个叫做spark的类周围，然后它就会被删除。
【04：50-05：04】 然后当你做ray.init时，你实际上创建的是一个Spark上下文和一个Spark会话，然后你告诉Ray Actuary创建两个Java执行程序，对吧，所以他实际上是在启动他的执行程序，不是。
【05：04-05：15】 它没有做所有的通信，MapReduce或执行程序之间的所有通信都是由底层JVM处理的，所以Ray实际上只是在启动这个。
【05：15-05：25】 在演员身上，在演员身上，演员只是管理那个，然后现在你只需要做一个远程的驱动器部分，你可以。
【05：25-05：34】 执行一个名为远程的函数来分区，做任何你想要的转换，就是这样简单，你创建了一个spark会话。
【05：34-05：44】 你使用Spark方法继续进行你想要的转换，然后你只需调用结果，只需使用ray.get。
【05：45-05：55】 所以这是你的答案，Dorothy百万百万百万美元的答案，是的，jules这里还有一个关于Ray数据和分片的问题。
【05：56-06：08】 让我们看看好奇关于Ray数据和分片，例如，目前使用排名分片，以及它的大小是如何在恐怖字中定义的，我如何在Ray中做同样的事情，好问题。
【06：08-06：19】 因此，RAE数据集是新库，它是实验性的，已经发布，我们已经举办了几场聚会，我们实际上将在23日举行网络研讨会。
【06：19-06：29】 但Ray Ray数据背后的整体想法是提供它不是数据的替代品，请不要误解，Ray数据在那里提供最后一分钟。
【06：29-06：41】 从你的帕克文件中转换，当你将东西注入到训练函数中时，因此，当你实际想在所有工作节点之间充电你的数据时，你所做的就是检查你的真实数据集，你实际上读取了那个说来自S3桶和帕克文件，并根据数量重新分区。
【06：41-06：50】 你有多少个伟大的演员，你实际上要给那个，这就是你如何在那个上分片的方式，那就是Ray Databricks Ray D矩阵所做的。
【06：50-07：00】 在我的演示中，当我读取指标时，它读取了那个特定的帕克文件，然后它使用共享内存来放置。
【07：00-07：09】 里面的东西，然后现在我的Ray工作者正在运行训练代码，实际上可以访问它。
【07：09-07：15】 那个Horwood方面是Horwood和TensorFlow和PyTorch集成和再训练的方式。
【07：15-07：25】 我们称之为一级集成，其中一级集成是Ray不处理低级通信，Ray只。
【07：34-07：44】处理工作进程的启动和内部通信，无论内部通信是什么，无论是Horvath。
【07：44-07：53】无论是使用PyTorch的DDP还是使用TensorFlow进行实际通信，对吧，所以我们不做这个，我们只是使用后端来做这个。
【07：56-08：07】然后ray数据集成就是一种将你的parquet文件转换为，比如说，TF记录或将它转换为DASC数据帧或将它转换为Spark数据帧的方法。
【08：07-08：19】或者将其转换为pandas数据帧，如果你实际上想要使用某种转换，所以数据数据集是最后一个粘合剂，我可以这样想，我也可以这样想，
【08：19-08：29】在你的ETL和SQL结束的地方，ray处理实际上开始了。
【08：29-08：39】所以想想他们正在使用，你知道，Delta Lake和Apache Spark来完成所有的ETL，因为如果我要使用Delta，如果我要使用ETL，
【08：39-08：48】对于Spark流和SQL工作负载，我不会考虑其他任何东西，而是使用Databricks上的Spark使用Delta，对吧？
【08：48-09：00】一旦我构建了我的湖屋，我的所有数据都在那里，然后如果我只是想使用，从它中提取所有的帕克文件，然后将其输入到我的深度学习中。
【09：00-09：05】分布式训练，让我们看看，我想聊天里还有别的东西。
【09：08-09：13】非常好的答案，希望如此。
【09：18-09：23】还有人有其他问题要问乔尔吗？
【09：42-09：52】好的，如果没有人有其他问题要问朱尔斯，那么我认为我们在这里基本上完成了，非常感谢你精彩的演讲，朱尔斯。
【09：52-10：05】这是一个非常，非常酷的工具，你正在研究，你知道保持联系，好吧，非常感谢邀请，如果你有任何问题，你可以找到我。
【10：05-10：15】你听到我的联系方式，随时可以给我发一条信息，我很乐意这样做或加入我们的Slack频道，我们肯定会保持联系，我会尽力回答那个。
【10：15-10：24】同样，凯伦和多萝西，非常感谢你们的邀请，很高兴再次见到你们俩，我觉得我又回到了家。
【10：24-10：33】保持健康，希望不久的将来能再次见到你们俩，也许几个月后的Data Plus AI峰会。
【10：33-10：40】是的，是的，好的，好的，谢谢，谢谢大家，再见，干杯。