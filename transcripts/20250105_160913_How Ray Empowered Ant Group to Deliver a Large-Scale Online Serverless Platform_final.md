---
video_id: _yu0Rtuetuc
video_url: https://www.youtube.com/watch?v=_yu0Rtuetuc
video_title: How Ray Empowered Ant Group to Deliver a Large-Scale Online Serverless Platform
created_at: 2025-01-05 16:10:11
---

# How Ray Empowered Ant Group to Deliver a Large-Scale Online Serverless Platform

【00：06-09：58】大家好，我们是艺术集团的杨松和李子力，今天的主题来自我们的伙伴团队，由TG魏才带领，但他今天不能来，所以我们将是演讲者。这是我们第一次来这里，我们很高兴分享我们在比赛中的工作。今天的主题是如何为艺术集团提供一个大规模的在线服务平台。让我们开始吧。首先，我想介绍一下我的公司和我的团队。总结一下，艺术集团有五个方向：数字支付、数字连接、数字金融、数字技术和全球化。最受青睐的是艺术集团最受欢迎的应用程序支付宝，它在中国互联网支付和金融领域处于领先地位。我的团队是一个三队，在下一页我会向你展示我的团队的历史信息，但我想要分享一些高级别的信息。首先，我们是第二大贡献于雷开源软件的团队。我们与Anyscale和ResLab合作了几年，我们在开源记录部分的贡献超过了26%。除了开发速度，我们还有很多大规模的应用程序。现在在我们的生产中，我们有超过一百万个CPU调用，在中国，我们也有一个丰富的中国社区，这是由我的团队创建和运营的。在过去五年里，我们每年都会举办一次前进会议，第五次前进已经完成。在过去七月，在这次会议上，Anyscale跨AI领域没有，Baddance在这次会议上分享了他的工作。因此，我们希望雷能在中国和美国一样受欢迎。让我们进入雷在艺术中的历史部分。我的团队成立于2017年，在这一年，雷与今天有很大的不同。我们认为雷是一个通用的分布式系统，这意味着我们有很多非人工智能应用和框架。你知道，在开源中，开源的重点是Python和AI，这意味着Python优先和AI优先，但我们是不同的。你可以知道，我们已经在项目中贡献了Java和C++API。在2018年，我们的第一个用户引擎被部署到我们的生产中，引擎的名字是GFlow，它是一个流图引擎，用于支付宝的风险控制和支付。在2019年，另一个融合引擎被部署到我们的生产中，这个引擎是在线学习，我们实现了三种计算类型，包括流式ML和在线服务。你可以知道，这一次也是在线服务，它被部署到艺术中，这是主题。在2020年，鉴于业务的多样性，我们创建了一个多架构的雷，这意味着你可以在一个大的雷集群中运行很多作业，并且在这个作业中，你可以支持不同的计算范式。在2021年，整个树变得更加稳定，我们与我们的生态系统公司进行了更多的合作，如Alamaduo Academy、China Network和Wansh Bank。在2022年，我们探索了私有计算，这是一个新的场景，我们认为这是一个很好的场景，因为私有计算需要一个灵活的框架来支持常见的功能、数据和AI，它在雷中有许多优点。好的，在今年2023年，我们旨在构建一个通用的AI服务框架，在这个框架中，我们想统一传统的AI、大型语言模型和搜索引擎。这就是今天的主题。回到在线服务的历史时间线，你可以知道，在线服务是在2019年首次部署到蚂蚁的，并且它被集成到在线学习引擎中，基于此，我们构建了基本的服务能力，并将其集成到艺术集团的基础设施中。你知道，每个公司都有自己的客户基础设施，所以我们需要采用它。在2021年，我们有了另一个场景，在线资源分配。你可以想象，如果资源是用于支付和金融，那么挑战是什么？我们认为挑战是我们需要一个更灵活、高性能、可扩展和稳定的框架。幸运的是，我们基于雷实现了这一点。在2020年，我们支持了大规模的在线服务平台，这次我们服务的模型有24万次调用，我们的事件驱动服务平台达到了137万PCTPS。好的，这些都是我们的私人工作。接下来，Ch将介绍我们今年最近的成就。欢迎各位。谢谢Guam和大家，我的名字是宋立东，我很高兴在这里与大家分享我们2023年的最新成就。在艺术集团，我们已经建立了最大的推理平台，我们的推理集群共有50万个CPU和40万个GPU。我相信你们已经看到了昨天Dr. Stoker在Kenospeech上的这些数字。为了交付这个硬件，我们总共使用了超过27000个工作节点。作为影响推理平台，我们在平台上拥有非常活跃的模型部署，每周有超过3000个新模型部署和超过10万个模型更新。我们的推理集群高度自动可扩展，因此，感谢我们的产品和独立的自动缩放器，我们的推理集群现在每周可以主动自动缩放超过3000次。通过这样做，我们将平均CPU利用率提高了超过20%。所以，接下来我将谈谈我们在生产中的一些新场景和一些新功能。【09：59-10：07】在深入细节之前，我想先展示一下我们的雷服务架构概述。【10：07-10：12】它与开源服务没有太大区别。【10：12-10：17】在这个架构中，我们有服务守护进程。【10：17-10：29】它接收所有来自用户客户端的服务推理作业提交，并将这些服务作业分发到我们的服务集群。【10：29-10：40】但对于每一个，服务守护进程必须在一个特定的服务集群中创建一个对应的应用程序主控。【10：40-10：47】这个应用程序主控将在集群内创建多个代理和部署演员。【10：48-10：57】当代理准备就绪时，它们会注册到独立的服务发现组件。【10：57-11：12】通过订阅这个组件，我们的用户应用程序可以了解每个代理的位置，并仔细选择最合适的代理来发送他们的推理请求。【11：13-11：27】在每个代理中，你需要将传入的推理请求分配给本地部署演员，其中模型服务工作将在那里完成。【11：29-11：32】好的，新场景。【11：32-11：36】第一个是在GP上进行模型服务工作。【11：36-11：41】为了实现这一点，我们选择了使用视频Triton。【11：41-11：51】对于那些不熟悉Triton的人，它是一个视频单节点模型推理符号。【11：51-12：01】它支持多个推理后端，在我们的保留系统中，Triton服务器可以非常容易地分布。【12：01-12：09】我们所需要做的就是，让我们的部署演员成为Triton启动器。【12：09-12：19】因此，每当我们的应用程序主控创建一个新的部署演员时，它会立即在内部启动Triton服务器。【12：19-12：27】借助我们的演员运行环境，这个运行环境特性非常有用。【1