---
video_id: _yu0Rtuetuc
video_url: https://www.youtube.com/watch?v=_yu0Rtuetuc
video_title: How Ray Empowered Ant Group to Deliver a Large-Scale Online Serverless Platform
created_at: 2025-01-04 20:55:22
---

# How Ray Empowered Ant Group to Deliver a Large-Scale Online Serverless Platform

### 翻译稿

[00:06 - 00:08] 大家好。
[00:08 - 00:12] 我们是来自蚂蚁集团的杨松和李哲。
[00:12 - 00:17] 今天的主题由我们的团队负责人韦才提出。
[00:17 - 00:21] 但他今天无法到场，所以由我们来演讲。
[00:21 - 00:30] 这是我们第一次来到这里，非常高兴能与大家分享我们在蚂蚁集团的工作。
[00:30 - 00:40] 今天的主题是如何赋能蚂蚁集团交付一个大规模在线服务平台。
[00:40 - 00:40] 让我们开始吧。
[00:41 - 00:46] 首先，我想介绍一下我的公司和团队。
[00:46 - 01:00] 总结一下蚂蚁集团，有五个方向：数字支付、数字连接、数字金融、数字技术和全球化。
[01:00 - 01:16] 其中，蚂蚁集团最受欢迎的应用是支付宝，在中国互联网支付和金融领域处于领先地位。我的团队是Anthree团队。
[01:16 - 01:23] 在下一页我会展示更多关于我团队的历史信息。
[01:23 - 01:29] 不过在这里，我先分享一些高层次的信息。
[01:30 - 01:39] 首先，我们是Ray开源软件的第二大贡献者。
[01:39 - 01:56] 我们与AnyScale和ResLab合作多年，在开源项目中有超过26%的贡献记录。
[01:56 - 02:13] 其次，除了开发Ray之外，我们还在生产环境中拥有许多大规模的Ray应用，拥有超过100万个CPU调用。
[02:14 - 02:25] 在中国，我们还创建并运营了一个丰富的Ray社区。
[02:25 - 02:32] 过去五年，我们每年都会举办RayForward Meetup。
[02:32 - 02:52] 第五届RayForward已于去年七月结束，在这次会议上，AnyScale、CrossAI、A't和Bad Dance等公司分享了他们的工作成果。
[02:52 - 03:01] 因此，我们希望Ray在中国也能像在美国一样受欢迎。
[03:02 - 03:07] 接下来，让我们看看Ray在蚂蚁集团的历史。
[03:07 - 03:16] 我的团队成立于2017年，当时的Ray与现在有很大不同。
[03:16 - 03:31] 我们将其视为通用分布式系统，这意味着我们有许多非AI应用和框架。
[03:31 - 03:45] 你知道，在Ray开源项目中，主要关注Python和AI，即以Python和AI为主，但我们有所不同。
[03:45 - 03:55] 因此，我们可以看到，我们已经为Ray项目贡献了Python、Java和C++ API。
[03:55 - 04:05] 2018年，我们的第一个用户层引擎在生产环境中部署。
[04:05 - 04:11] 引擎名称是GFlow，它是一个流图引擎。
[04:11 - 04:17] 它用于支付宝的风险控制。
[04:17 - 04:29] 2019年，另一个融合引擎也在生产环境中部署。
[04:29 - 04:31] 这个引擎支持在线学习。
[04:31 - 04:40] 我们填补了三种计算类型：流式处理、机器学习和在线服务。
[04:40 - 04:51] 所以你可以知道，这次部署的在线服务也是蚂蚁集团首次部署的在线服务，这是一个重要的里程碑。
[04:51 - 05:08] 2020年，鉴于业务的多样性，我们构建了一个多架构系统，可以在一个大型Ray集群中运行多个Ray作业。
[05:08 - 05:16] 在这些作业中，可以支持不同的计算范式。
[05:16 - 05:22] 2021年，Ray变得更加稳定。
[05:22 - 05:35] 我们与阿里巴巴达摩院、中国联通和万向银行等生态系统公司进行了更多合作。
[05:36 - 05:45] 2022年，我们探索了隐私计算这一新场景。
[05:45 - 05:53] 我们认为这是一个适合Ray的新场景，因为在隐私计算中。
[05:53 - 06:07] 我们需要灵活的框架来支持常见的数据和AI功能，而Ray在这方面有很多优势。
[06:08 - 06:28] 今年（2023年），我们致力于构建一个通用的AI服务框架，该框架将统一传统AI、大语言模型和搜索引擎。
[06:28 - 06:32] 这就是今天的主题。
[06:32 - 06:34] 再回到在线服务。
[06:34 - 06:48] 从历史时间线可以看出，在线服务首次在蚂蚁集团部署是在2019年。
[06:48 - 06:54] 它被集成到在线学习引擎中。
[06:54 - 06:56] 基于这一点。
[06:56 - 07:05] 我们构建了基本的在线服务能力，并将其集成到蚂蚁集团的基础架构中。
[07:05 - 07:13] 每家公司都有自己的基础架构，因此我们需要进行适应。
[07:13 - 07:22] 2021年，我们有了新的场景——在线资源分配。
[07:22 - 07:34] 你可以想象，如果资源用于支付和金融，会面临哪些挑战？
[07:34 - 07:44] 我们认为，挑战在于需要一个更灵活、高性能、可扩展且稳定的框架。
[07:44 - 07:59] 幸运的是，基于Ray，我们在2022年实现了这一目标，支持了大规模在线服务平台。
[07:59 - 08:12] 此时，我们每秒处理24万个模型调用，事件驱动平台达到了137万个PC TPS。
[08:12 - 08:16] 好了，这就是我们内部工作的全部内容。
[08:16 - 08:24] 下面，李涛将介绍我们今年的最新成就。
[08:24 - 08:25] 欢迎。
[08:25 - 08:26] 好的。
[08:26 - 08:29] 谢谢杨松，大家好。
[08:29 - 08:40] 我叫李涛，很高兴在这里分享我们2023年的最新成就。
[08:40 - 08:46] 在蚂蚁集团，我们已经建立了最大的推理平台。
[08:46 - 08:52] 我们的推理集群拥有50万台CPU和4000台GPU。
[08:52 - 09:00] 相信你们昨天在Stokers Keno的演讲中已经看到了这些数字。
[09:00 - 09:07] 为了提供这些硬件，我们总共使用了27000个工作节点。
[09:07 - 09:14] 作为推理平台，我们在平台上进行了非常活跃的模型部署。
[09:14 - 09:23] 每周我们有超过3000个新的模型部署和超过10万个模型更新。
[09:23 - 09:28] 我们的推理集群高度自动扩展。
[09:28 - 09:41] 得益于我们的产品特性和独立的自动扩展器，每周我们的推理集群可以主动扩展超过3000次。
[09:41 - 09:48] 通过这种方式，我们将平均CPU利用率提高了20%以上。
[09:51 - 09:58] 接下来，我将介绍我们在生产中的新场景和一些新功能。
[09:59 - 10:07] 在详细介绍之前，我想先展示一下我们的Ray服务架构概览。
[10:07 - 10:12] 它与开源版本没有太大区别。
[10:12 - 10:17] 在这个架构中，我们有服务守护进程。
[10:17 - 10:29] 它接收所有来自用户客户端的服务推理任务提交，并将这些任务分发到各个服务集群。
[10:29 - 10:40] 对于每个任务，服务守护进程会在特定的服务集群中创建一个相应的应用程序主节点。
[10:40 - 10:47] 应用程序主节点将在集群内创建多个代理和部署执行器。
[10:48 - 10:57] 当代理准备好后，它们会注册到独立的服务发现组件。
[10:57 - 11:12] 用户应用程序可以通过订阅该组件获取每个代理的位置，并选择最适合的代理发送推理请求。
[11:13 - 11:27] 在每个代理中，你需要将传入的推理请求分配给本地的部署执行器，在那里完成模型推理工作。
[11:29 - 11:32] 新场景之一是使用GPU进行模型推理。
[11:32 - 11:36] 为此，我们选择了NVIDIA Triton。
[11:36 - 11:41] 对于不熟悉Triton的观众，它是NVIDIA的单节点模型推理服务器。
[11:41 - 11:51] 它支持多种推理后端，在我们的服务系统中，Triton服务器可以非常容易地分布。
[11:51 - 12:01] 我们只需让部署执行器成为Triton引导器。
[12:01 - 12:09] 每当应用程序主节点创建一个新的部署执行器时，它会立即启动内部的Triton服务器。
[12:09 - 12:19] 在我们的执行器运行时环境的帮助下，这个特性非常有用。
[12:19 - 12:38] 它由开源Ray提供，帮助我们处理数据依赖包或库依赖问题。我们的Ray团队几乎贡献了一半的实现代码。
[12:38 - 12:46] 当这些Triton服务器准备就绪时，它们可以选择注册到服务发现组件，后续的推理请求可以直接连接到这些Triton服务器。
[12:46 - 12:47] 是的。
[12:47 - 13:06] 由于我们的服务集群高度自动扩展，因此在我们的平台上运行的Triton服务器也具有自动扩展能力。
[13:06 - 13:21] 另一个场景是服务ARIMA应用程序。
[13:21 - 13:29] 在蚂蚁集团，我们有一个名为GPT Cache的系统。
[13:29 - 13:38] 在这个系统中，我们会尝试将历史LM响应存储在向量缓存或缓存存储中。
[13:38 - 13:48] 每当有新的推理请求进来时，我们首先进行相似性比较。
[13:48 - 13:57] 如果命中缓存，我们就直接返回预存储的响应；如果未命中，则推理请求仍需通过模型服务管道。
[13:57 - 14:11] 在我们的服务平台上，这个GPT Cache系统可以非常容易地集成。
[14:12 - 14:22] 我们只需在部署执行器中运行这个GPT Cache系统，并借助执行器运行时特性。
[14:22 - 14:33] 当然，我们还需要一个入口执行器，它会将每个传入的推理请求转发给GPT Cache执行器。
[14:33 - 14:47] 如果缓存命中，我们快速响应；如果未命中，则将请求转发给另一个运行在部署执行器中的Triton服务器。
[14:47 - 14:50] 然后我们继续做同样的事情。
[14:50 - 14:53] 如果缓存命中，我们快速响应。
[14:53 - 15:04] 如果未命中，则将请求转发给另一个运行在部署执行器中的Triton服务器。
[15:06 - 15:21] 在这张图中可以看到，GPT Cache执行器运行在CPU节点上，而Triton服务器运行在GPU节点上。
[15:21 - 15:37] 借助Ray的异构资源调度，我们现在可以更明智地分布这些不同类型的执行器，优化整体资源利用率。
[15:39 - 15:42] 接下来是一些新功能。
[15:42 - 15:46] 第一个是构建异步代理。
[15:46 - 15:58] 我们这样做是因为在大多数模型服务场景中，我们发现推理请求的执行时间差异很大。
[15:58 - 16:07] 对于长时间的请求，人们总是要付出大量的同步等待开销。
[16:07 - 16:14] 因此，我们在服务集群中部署了一个异步代理。
[16:14 - 16:20] 当然，这个异步代理也运行在一个部署执行器中。
[16:20 - 16:26] 这个异步代理可以接收并排队每个传入的推理请求。
[16:26 - 16:38] 对于长时间的请求，当结果准备好时，异步代理可以帮助我们异步返回结果。
[16:40 - 17:03] 这个异步代理的一个重要好处是，集群中的其他部署执行器可以根据自身的忙碌或空闲状态自适应地从代理中拉取推理请求。
[17:03 - 17:18] 因此，在每个运行Triton服务器的部署执行器中，我们可以看到更少的头部阻塞延迟。
[17:19 - 17:35] 在我们的评估中，相比基于轮询推送的基准方法，我们的基于拉取的方法可以提高两倍的吞吐量。
[17:38 - 17:42] 下一个功能是C++部署。
[17:42 - 17:50] 这个功能已经在我们的推荐和广告服务中广泛部署。
[17:50 - 17:55] 这些服务对延迟敏感且需要高吞吐量。
[17:55 - 18:04] 因此，出于性能要求，我们的部署执行器也必须具备高性能。
[18:04 - 18:15] 我们使用基于开源Ray的C++工作者。
[18:15 - 18:22] 顺便说一句，这个C++工作者主要是由我们的团队贡献的。
[18:22 - 18:25] 感谢杨松在这方面的努力。
[18:25 - 18:36] 借助这个C++部署执行器，我们实现了一个高性能的直接入口。
[18:36 - 18:40] 它是一个高性能的RPC服务。
[18:40 - 18:54] 凭借这个特性，我们的推理请求可以直接连接到C++部署执行器，绕过所有提到的代理。
[18:56 - 19:01] 另一个重要的特性是原生Triton推理调用。
[19:01 - 19:20] 这个特性得以实现是因为我们现在有C++部署执行器和Triton服务器，后者可以被视为一个C++库，可以更紧密高效地集成，没有任何跨语言开销。
[19:20 - 19:40] 在将C++部署执行器与其他C++组件（如推荐和广告时间线）结合时，我们实际上构建了一个更完整的分布式系统。
[19:43 - 19:48] 接下来，我将谈谈未来计划。
[19:48 - 19:54] 第一件事是共享部署。
[19:54 - 20:11] 动机在于，在推荐和搜索服务中，我们经常看到大量特征数据，这些数据太大，无法放入任何单个工作节点。
[20:11 - 20:19] 因此，我们必须将这些大数据分布在多个工作节点上。
[20:19 - 20:30] 当然，我们会创建一个代理部署执行器，并使用这个代理帮助我们进行路由策略，确保每个传入的推理请求都能转发到具有本地数据依赖的具体节点。
[20:59 - 20:59] 好的。
[20:59 - 21:04] 最后，我们有一个更大的愿景。
[21:04 - 21:13] 我们希望交付一个高性能的通用AI框架。
[21:13 - 21:33] 首先，我们将继续基于开源Ray构建和完善我们的服务平台，该平台已被证明是高度分布式、多语言支持且可扩展的。
[21:33 - 21:40] 我们相信这可以为许多模型服务场景提供良好的基础。
[21:42 - 21:52] 我们将使用直接入口构建高性能的RPC服务。
[21:52 - 22:04] 我们将广泛使用C++部署执行器，因为它可以提供高性能计算能力。
[22:05 - 22:16] 我们将开始着手共享部署，这可以为我们提供高性能的本地数据访问和检索。
[22:16 - 22:26] 我们认为，这个特性在未来处理大规模模型服务问题时非常重要。
[22:28 - 22:32] 好的，这就是我们今天的全部内容。
[22:32 - 22:39] 如我在开头所说，这项工作主要由韦才完成。
[22:39 - 22:43] 我们将尽力回答问题。
[22:43 - 22:53] 如果我们无法回答，请大家发送电子邮件至这个地址，我相信他可以给出最详细的答案。
[22:53 - 22:55] 谢谢大家。
[23:02 - 23:12] 直接入口和C++工作者有多少已经集成到开源项目中？
[23:12 - 23:19] 它们已经是开源的，但在开源社区中并不太流行。
[23:19 - 23:40] 但我们大量使用它们，特别是在推荐和广告系统中，因为我们希望整个推荐管道的所有组件都用C++实现，使整个管道更加完整。
[23:42 - 23:55] 关于推荐管道，你具体是如何使用Ray的？因为你在推理中使用Triton，也在使用直接入口，那么Ray在这个过程中扮演什么角色？
[24:04 - 24:19] 是的，我们确实使用Ray环境帮助我们在部署执行器中集成Triton依赖项。
[24:19 - 24:32] 我们还使用Ray的异构调度来调度这些执行器，无论是CPU依赖还是GPU依赖的执行器，都可以很好地调度，从而提高资源利用率。
[24:46 - 24:48] 谢谢你的演讲。
[24:48 - 24:50] 快速提问。
[24:50 - 24:58] 你说每周有超过3000个模型部署，那么对于某一类模型，比如大语言模型，通常的部署频率是多少？
[25:07 - 25:31] 我不太清楚具体的细节，建议你发送邮件给韦才，因为这是来自服务团队的一些详细信息。
[25:37 - 25:42] 共享部署与你们选择Ray有关吗？
[25:42 - 25:47] 是的，共享部署与选择Ray有关。
[25:47 - 26:05] 是的，我们只是需要将这些特征数据分布在多个工作节点上，但这些工作节点仍然基于Ray，所有的执行器都是由Ray处理的。
[26:13 - 26:19] 所有这些都是在虚拟机上运行，而不是Kubernetes或其他中间件。
[26:19 - 26:20] 只是虚拟机。
[26:20 - 26:23] 你是指所有这些都在虚拟机上运行吗？
[26:23 - 26:25] 或者是在Kubernetes上运行？
[26:25 - 26:35] 或者是其他东西？我们肯定是在云原生环境中，在Kubernetes之上运行我们的Ray集群。
[26:43 - 26:48] 如果没有更多问题，谢谢大家。
[26:48 - 26:50] 好的。
[26:48 - 26:50] 谢谢大家。

---

### 翻译说明：
1. **专业术语**：保持了专业术语的准确性，如“Ray”、“Triton”、“C++部署执行器”等。
2. **术语表**：遵循标准翻译，如“推理平台”、“模型部署”等。
3. **关键英文术语**：适当保留了关键英文术语，如“Ray”、“Triton”等。
4. **流畅自然**：确保翻译后的文本流畅自然，避免过度口语化。