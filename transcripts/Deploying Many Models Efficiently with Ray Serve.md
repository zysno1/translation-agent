# Deploying Many Models Efficiently with Ray Serve

谢谢sihuan的介绍。今天的演讲将分为三个部分，每一部分都是Ray Serve中不同的模型服务解决方案。尽管你可以根据自己的用例以任何方式组合它们。

首先，我们将深入探讨模型组合。最近我们看到的一个大趋势是，越来越多的应用程序需要多个模型来进行推理，而不仅仅是一个单一的模型。一些常见的例子包括音频转录，这需要多个模型来处理单个请求，或者计算机视觉，通常也需要多种类型的图像处理模型，如分类器或检测器来处理单个请求。但是，多模型服务带来了很多挑战。首先是高效的硬件使用，这意味着你有一定数量的CPU和GPU计算资源。如何尽可能高效地使用这些资源，尤其是在你的模型对硬件资源需求不同时。特别是随着最近LLM（大型语言模型）的兴起，现在GPU严重短缺，因此每个GPU都很重要。因此，在应用程序中的模型之间共享GPU计算资源尤为重要。此外，能够独立扩展应用程序中的模型也很重要，特别是如果不同模型有不同的延迟或流量模式。还有运营开销。在开发过程中或应用程序运行生产时，这些事情都很重要。测试你的应用程序应该简单易行。可观察性和监控在生产环境中非常重要，可以了解应用程序的状态或是否出现问题。并且能够独立升级应用程序的各个部分也应该具有灵活性。但是一旦开始向应用程序添加更多模型，所有这些都会变得更加复杂。

让我们更深入地看看这里展示的计算机视觉示例。假设你将原始图像作为输入，进行预处理，然后通过分类器和检测器模型运行预处理的数据。最后运行一些自定义业务逻辑来决定返回给客户端的最终输出。接下来几张幻灯片中，我们将以此为例进行参考。那么，如果不使用Ray Serve，你如何解决我们提到的这些挑战呢？你可能会尝试的第一种方法是单体架构，即将所有模型都塞进一个盒子里。你可以将模型放入容器中，一起扩展容器作为一个单体，并将其作为服务。但这里的一个大问题是，你无法独立扩展应用程序的一部分，因为所有内容都被打包在一起并粘合为一个整体。出于同样的原因，你无法独立进行升级。如果你升级应用程序的一部分，其他部分也会被迫更新，这使得升级过程既非常危险又成本高昂。

接下来你可能会尝试将模型分离成单独的微服务。你可以有一个API网关进行访问控制。由于每个请求都需要多个模型进行推理，微服务之间会有通信。现在你可以独立扩展每个微服务。正如我们所说，有独立扩展的能力，你可以独立升级一个微服务而不影响其他模型。但正是因为一切都分离成了各自的系统，你不能再在模型之间共享资源。你需要设置很多东西，比如服务间的通信，可能每个微服务都有数据库，监控和可观察性必须为每个微服务单独设置。端到端测试你的应用程序也更加复杂，因为你必须部署多个子系统来测试你的应用程序。因此，这种解决方案也有很多缺点。

这就是为什么在Ray Serve中，我们构建了一个结合了微服务和模型组合优势的功能，称为模型组合。一个Ray Serve应用程序由多个组合在一起的模型组成。每个模型可以有自己的硬件资源需求。例如，有些可能在GPU上运行，有些可能只在CPU上运行，有些甚至可以同时使用两者。你还可以独立自动扩展应用程序中的模型。这意味着，如果应用程序中的某个模型成为瓶颈，你可以为其分配更多资源，而不必一起扩展所有内容。它还在进程级别进行扩展，这更加灵活。微服务方法是在容器级别进行扩展。这意味着当你扩展应用程序时，不必一定要添加更多节点来扩展它。你可以在单个节点上添加更多模型副本，直到该节点的资源利用率达到最大。最后，使用Ray的独特之处在于能够使用分数资源。例如，如果这个单个模型使用半个GPU，拥有三个副本意味着你可以将所有三个副本安排在两个GPU上，并且你甚至可以使用剩余的空间来运行另一个模型。

现在让我们看看如何使用模型组合来完成这个计算机视觉示例。你获取图像，进行预处理，将其发送到特定的分类器模型和检测器模型。最后运行自定义业务逻辑。这就是你的整个多模型应用程序。请注意，当我们逐个添加组件时，我们也展示了你需要在右侧的Python文件中添加什么。重点是你只需使用一个Python文件即可定义和链接整个应用程序。这就是你需要部署应用程序到Ray集群的所有内容。我们可以看一下这个应用程序的资源分配示例。分类器可能每个占用一个CPU和三分之一的GPU。预处理通常在CPU上运行，因此我们可以将这两个部署放在具有四个CPU和一个GPU的节点上。然后检测器和业务逻辑可能也在CPU上运行，所以你可以将它们放在第二个具有两个可用CPU的节点上。因此，在应用程序中的模型之间共享资源非常容易。在一个类似的现实案例中，Samsara从我们提到的微服务架构切换到使用Ray Serve的模型组合，每年节省了50%的ML基础设施成本。

现在让我们重新审视我们一开始提到的挑战。正如我们所提到的，在应用程序中的模型之间共享资源非常容易，并且你可以独立扩展应用程序中的模型。至于运营开销，由于所有内容都在同一个集群上，测试你的应用程序变得很容易，并且为你的单个Ray集群设置监控和可观察性也非常简单。但如果我们将自己限制在一个单一的应用程序中，缺少的是独立升级。如果你更新应用程序的一部分，仍然需要更新应用程序的其他部分。这正好引出了我们演讲的第二部分，即多应用程序。

正如我们所提到的，一个使用模型组合的应用程序并不适用于所有用例。假设你有一个团队正在研究计算机视觉模型，用于自动驾驶。但也许这些模型在夜间表现不佳。因此，你有另一个团队专门研究夜间算法。理想情况下，这些模型会生活在同一个集群上，这样车辆可以在夜间切换到专门的算法。你可能还有一个团队在研究暴风雪或暴雨的算法。激光雷达和雷达对于测量你的汽车与其他道路物体之间的精确距离也非常重要。理想情况下，所有这些都将生活在同一个集群上，因为它们服务于相同的用例。但它们可能存在于不同的仓库中，并且肯定是由不同的团队管理。如果我们使用模型组合将它们全部组合在一起，它们将共享相同的升级生命周期。这意味着，如果一个团队希望对其生产中的模型进行更改，其他团队管理的模型也将被迫更新。这使得升级过程风险高、成本高且总体上非常复杂。你甚至可能需要另一个团队来管理跨ML团队的部署，确保部署过程中不会出错。这就是为什么我们增加了多应用程序功能，它建立在模型组合的基础上。你的应用程序仍然可以包含组合在一起的模型，但现在你可以在同一个Ray集群上拥有多个应用程序。每个应用程序都是自己的端点，并有自己的升级生命周期。你可以轻松地添加、删除或更新应用程序，而不必担心影响集群上的其他模型。同时，你仍然享有高效和灵活的资源分配以及在应用程序之间共享硬件资源的好处。

让我们快速浏览一个多应用程序的实际用例。AnyScale Endpoints是AnyScale托管的API端点，用于LLM推理和微调。最近，Meta AI发布了一些Llama系列模型，性能非常好。假设这些模型刚刚发布几天，我们希望将其添加到AnyScale Endpoints供用户试用和微调。如果你想将70亿参数的模型添加到AnyScale Endpoints，只需将其添加到配置文件并部署到集群中即可。这将加载到集群上，准备为用户提供服务。你还可以添加更多模型，如3B模型。然后假设你可以删除它并替换为7B模型。所有这些都可以通过serve配置文件和修改条目来完成。请注意，在整个过程中，7B模型继续提供流量，无论集群上的其他应用程序发生了什么。再次回顾一下我们一开始提到的挑战。正如我们所提到的，单个使用模型组合的应用程序已经提供了许多好处，如高效的资源分配和独立扩展模型。测试你的应用程序也非常容易，可观察性和监控只需要在整个集群上设置一次。但缺少的是独立升级。使用多应用程序，你可以将集群上的模型分组为你认为有意义的组。例如，如果它们已经存在于不同的仓库中，或者由不同的团队管理，或者其他任何要求。你可以形成这些组，然后独立更新这些组。

现在我将交给sihuan继续演讲。感谢Cindy深入探讨模型组合和多应用程序用例。在之前的场景中，我们通常一次性将所有模型加载到服务器集群中进行服务。但随着LM用例的出现，我们看到服务大量模型所需资源超过了集群所能提供的资源。为了支持这种情况，我们引入了一个新的Reserve API，称为多路复用，以支持这种用例。在进入API之前，我们来看一个简单的例子，比如未扩展的端点或Firefox AI。我们希望支持不同的开源模型，例如不同参数数量的Llama模型。更重要的是，我们希望支持不同客户的模型，以满足他们的业务用例。这带来了两个挑战：一是有限的硬件资源。你通常无法一次性将所有模型加载到集群中。二是对于不活跃的模型来说，这样做效率很低。你将这些模型加载到内存中却不用。第二个挑战是高推理延迟。如果你一直加载模型，你的在线推理服务将始终遭受冷启动时间的影响。

让我们看看没有多路复用的情况。通常我们会将模型存储在S3中。用户只需指定模型ID并通过HTTP请求发送到集群。集群代理接收到此请求后，目前没有针对此示例的处理，假设集群中有两个副本。代理将随机选择一个副本来处理此请求。假设我们选择了副本1，并从S3加载模型，进行推理并将响应返回给客户端。如果客户端继续发送模型1的请求，代理甚至不知道模型ID位于哪个副本中。代理可能会选择副本2来处理请求，这意味着你必须再次从S3加载模型1到不同的副本中。对于模型2和模型3也是如此。想象