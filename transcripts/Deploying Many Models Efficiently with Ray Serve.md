# Deploying Many Models Efficiently with Ray Serve

## 最终结果

```
[00:00:00.000 --> 1541.093]
谢谢seehan的介绍。今天的演讲将分为三个部分。每一部分都是Ray Serve中不同的模型服务解决方案。尽管你可以根据自己的用例以任何方式组合它们。首先，我们将深入探讨模型组合。最近的一个大趋势是，越来越多的应用程序需要多个模型，而不仅仅是一个单一模型来进行推理。一些常见的例子包括音频转录，这需要多个模型来处理单个请求，或者计算机视觉，通常也需要多种类型的图像处理模型（如分类器或检测器）来处理单个请求。然而，多模型服务带来了很多挑战。首先是高效的硬件使用，这意味着你有一定数量的CPU和GPU计算资源。如何尽可能高效地使用这些资源，尤其是在你的模型对硬件资源需求不同时尤其重要。特别是在最近大型语言模型（LLM）兴起的情况下，现在GPU严重短缺，因此每个GPU都很重要。因此，在应用程序中的模型之间共享GPU计算资源尤为重要。此外，能够独立扩展应用程序中的模型也很重要，特别是如果不同模型有不同的延迟或流量模式。还有运营开销。在开发过程中或应用程序运行生产时，这些事情都非常重要。测试应用程序应该简单，可观察性和监控在生产环境中非常重要，以便了解应用程序的状态或是否出现问题。并且能够独立升级应用程序的不同部分也应具有灵活性。但是一旦开始向应用程序添加更多模型，所有这些都会变得更加复杂。

让我们更深入地看看这里展示的计算机视觉示例。假设你将原始图像作为输入，进行预处理，然后通过分类器和检测器模型运行预处理数据。最后运行一些自定义业务逻辑来决定返回给客户端的最终输出。我们将在接下来的几张幻灯片中使用这个作为参考示例。那么，如果不使用Ray Serve，你可能会如何解决我们提到的这些挑战呢？你可能尝试的第一个方法是单体方法，即将所有模型塞进一个盒子里的解决方案。你可以将模型放入容器中，一起扩展容器作为一个整体。但这有一个大问题，你无法独立扩展应用程序的一部分，因为所有内容都被打包在一起并粘合为一个整体。出于同样的原因，你无法进行独立升级。如果你升级应用程序的一部分，其他所有部分也被迫更新，这使得升级过程既非常危险又非常昂贵。

接下来你可能会尝试将模型分离成单独的微服务。你可以有一个API网关进行访问控制。由于每个请求都需要多个模型进行推理，微服务之间会有通信。现在你可以独立扩展每个微服务。正如我们所说，有独立扩展，并且可以独立升级一个微服务而不影响其他模型。但正是因为一切都分成了独立的系统，你不能再在模型之间共享资源了。你必须设置许多东西，比如服务间通信，也许每个微服务都有数据库，监控和可观察性也必须为每个微服务单独设置。端到端测试你的应用程序也变得更加复杂，因为你必须部署多个子系统。所以这种方法也有很多缺点。

这就是为什么在Ray中，我们构建了一个结合了微服务和模型方法优点的功能，称为模型组合。一个Ray Serve应用程序由多个组合在一起的模型组成。每个模型可以有自己的硬件资源需求。例如，有些可能在GPU上运行，有些可能只在CPU上运行，有些甚至可以同时使用两者。你还可以独立自动扩展应用程序中的模型。这意味着如果你的应用程序中的某个模型成为瓶颈，你可以分配更多资源给该模型，而不是必须一起扩展所有内容。它还在进程级别进行扩展，更加灵活。微服务方法是在容器级别进行扩展。这意味着当你扩展应用程序时，不必一定要添加更多节点来扩展。你可以在单个节点上添加更多模型副本，直到最大化该节点的资源利用率。最后，使用Ray的独特之处在于能够使用分数资源。例如，如果单个模型使用半个GPU，那么拥有三个副本意味着你可以将所有三个副本安排在两个GPU上，并且还可以使用剩余的空间用于另一个模型。

现在让我们看看如何使用模型组合来完成这个计算机视觉示例。你获取图像，进行预处理，将其发送到特定的分类器模型和检测器模型。最后运行你的自定义业务逻辑。这就是你的整个多模型应用程序。请注意，当我们逐个添加组件时，还展示了你需要在右侧Python文件中添加的内容。关键点是你只需使用一个Python文件即可定义和链接整个应用程序。这就是你将应用程序部署到Ray集群所需的一切。我们可以看一下这个应用程序的资源分配示例。分类器可能每个占用一个CPU和三分之一的GPU。预处理通常在CPU上运行，因此可以将这两个部署放在具有四个CPU和一个GPU的节点上。然后检测器和业务逻辑可能也在CPU上运行，因此可以将它们放在具有两个可用CPU的第二个节点上。因此，在应用程序中的模型之间共享资源非常容易。在一个类似的现实案例中，Samsara从使用我们提到的微服务架构转向使用Ray Serve的模型组合，成功节省了50%的ML基础设施成本。

现在让我们重新审视我们一开始提到的挑战。正如我们所提到的，在应用程序中的模型之间共享资源非常容易，并且你可以独立扩展应用程序中的模型。至于运营开销，由于一切都在同一个集群上，测试你的应用程序非常容易，并且为你的单个Ray集群设置监控和可观察性也非常简单。但如果我们将自己限制在一个单一的应用程序中，缺少的是独立升级。如果你更新应用程序的一部分，仍然需要更新应用程序的所有其他部分。这正是我们讨论的第二部分——多应用程序的理想过渡。

正如我们所提到的，一个使用模型组合的应用程序并不适用于所有用例。假设你有一个团队正在研究计算机视觉模型，自动驾驶。但也许这些模型在夜间表现不佳。因此，你有另一个团队专门研究夜间算法。理想情况下，这些模型会存在于同一个集群上，这样车辆可以在夜间切换到专门的算法。你可能还有一个团队研究暴风雪或暴雨的算法。激光雷达和雷达对于测量你的汽车与其他道路物体之间的精确距离也非常重要。理想情况下，所有这些都将存在于同一个集群上，因为它们服务于同一个用例。但可能都存在于不同的存储库中，并且肯定是由不同的团队管理。如果我们使用模型组合将它们全部组合在一起，它们将共享相同的升级生命周期。这意味着如果一个团队想要对其生产中的模型进行更改，其他团队管理的模型也将被迫更新。这使得升级过程风险高、成本高且总体上非常复杂。你甚至可能需要另一个团队来管理跨ML团队的部署，并确保部署过程中不会出错。这就是我们添加多应用程序功能的原因，该功能建立在模型组合之上。你的应用程序仍然可以包含组合在一起的模型，但现在可以在同一Ray集群上拥有多个应用程序。每个应用程序都有自己的端点和升级生命周期。你可以轻松地添加、删除或更新应用程序，而不必担心影响集群上的其他模型。同时，你仍然享有高效和灵活的资源分配以及在应用程序之间共享硬件资源的好处。

让我们快速了解一下多应用程序的实际用例。AnyScale Endpoints是用于LLM推理和微调的托管API端点。最近，Meta AI发布了一些Llama系列模型，性能非常好。假设这些模型刚刚发布几天，我们希望将其添加到AnyScale Endpoints供用户试用和微调。如果你想将70亿参数的模型添加到AnyScale Endpoints，只需将其添加到配置文件并部署到集群即可。这将加载到集群上，准备好为用户提供服务。你还可以添加更多模型，比如3B模型。理论上，你也可以删除并替换为7B模型。所有这些都是通过serve配置文件并更改条目来实现的。请注意，整个过程中7B模型继续提供流量，无论集群上的其他应用程序发生了什么。再次回顾我们一开始提到的挑战。正如我们所提到的，单个使用模型组合的应用程序已经提供了很多好处，比如高效的资源分配和独立扩展模型，以及非常简单的应用程序测试。可观察性和监控只需要为整个集群设置一次。唯一缺少的是独立升级。使用多应用程序，你可以将集群上的模型分组为你认为合适的组。例如，如果它们已经存在于不同的存储库中，或者由不同的团队管理，或者其他任何要求。你可以形成这些组，然后独立更新这些组。现在我将交给sihuan继续剩下的演讲。

感谢cindy深入探讨了模型组合和多应用程序用例。在之前的场景中，我们通常一次性将所有模型加载到服务器集群中进行服务。但随着LM用例的出现，我们看到服务大量模型需要比集群所能提供的更多的资源。为了支持这种场景，我们引入了一种新的Reserve API，称为多路复用，以支持这种用例。在进入API之前，我们来看一个简单的例子。例如，像unscaled endpoint或Firefox AI这样的平台，我们希望支持不同的开源模型。例如，不同参数数量的Llama模型。更重要的是，我们希望支持不同客户的模型以满足他们的业务用例。这带来了两个挑战。一是有限的硬件资源。你通常无法一次性将所有模型加载到集群中。一是非常昂贵，尤其是对于GPU硬件。其次，对于非活动模型，加载到内存中而不使用是非常低效的。第二个挑战是高推理延迟。如果你一直加载模型，你的在线推理服务将始终遭受冷启动时间的影响。让我们看看没有多路复用的情况。通常我们将一个或多个模型放入S3。用户只需指定模型ID并通过HTTP请求发送到集群。集群代理接收此请求，目前在这个例子中有两个副本。代理将随机选择一个副本来处理此请求。假设我们选择了副本1，从S3加载模型，进行推理并将响应返回给客户端。如果客户端继续发送模型1，即使代理不知道模型ID位于哪个副本中，代理可能会选择副本2来处理请求。这意味着你必须再次从S3加载模型1到不同的副本中以处理请求。这对于模型2和模型3也是如此。想象一下，如果你有大量的模型，S3上有许多模型，你最终不得不供应你的模型，考虑到。
```
