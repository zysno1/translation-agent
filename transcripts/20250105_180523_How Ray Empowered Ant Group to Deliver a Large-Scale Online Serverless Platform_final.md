---
video_id: _yu0Rtuetuc
video_url: https://www.youtube.com/watch?v=_yu0Rtuetuc
video_title: How Ray Empowered Ant Group to Deliver a Large-Scale Online Serverless Platform
created_at: 2025-01-05 18:08:43
---

# How Ray Empowered Ant Group to Deliver a Large-Scale Online Serverless Platform

【00：06-00：17】大家好，我们马上开始了。今天的话题是来自我们的小伙伴团队，由蔡腾伟带领的。
【00：17-00：25】但是他今天不能来，所以我们是演讲者。这是我们第一次来这里。
【00：25-00：38】我们很高兴能与Amit分享我们的工作。今天的话题是关于我们如何赋能我们的团队，交付一个大规模的在线服务平台。
【00：38-00：46】让我们开始吧。首先，我想介绍一下我的公司和我的团队。
【00：46-00：54】总结一下我们的集团，有五个方向，数字支付、数字连接。
【00：54-01：07】数字金融、数字技术和全球化。我们集团最受欢迎的应用是支付宝。
【01：07-01：12】它在中国的互联网支付和金融领域处于领先地位。
【01：12-01：20】我的团队是动脉团队，在下一页我会展示很多。
【01：20-01：29】关于我团队的历史和信息。但是在那里，我想先分享一些高层次的信息。
【01：30-01：38】首先，我们是为Ray开源软件贡献第二大的团队。
【01：38-01：46】我们已经与AnyScale和REST实验室合作了几年。
【01：46-01：56】我们在开源中对召回部分的代码贡献超过了26%。
【01：56-02：05】其次，除了开发Ray之外，我们还有很多Ray的大规模应用。
【02：05-02：13】现在，在我们的生产环境中，我们有超过一百万个CPU调用。
【02：15-02：23】然后在中国，我们也有一个中国社区，这个社区是由我的团队创建和运营的。
【02：23-02：33】在过去五年里，我们每年都会举办Ray Forward聚会，第五次Ray Forward。
【02：33-02：45】已经在过去的七月完成了。在这个会议上，AnySkill跨越AI和舞蹈。
【02：45-02：55】我卖掉了他们的作品。所以，我们非常。
【02：55-03：01】我们是。我们希望我们能在中国像在美国一样受欢迎。
【03：02-03：16】接下来，让我们回顾一下Ray在动脉中的历史。我的团队是在2017年创建的，在这一年，Ray与今天的Ray有很大的不同。
【03：16-03：30】我们认为它是一个通用的分布式系统，这意味着我们有很多非AI的应用程序和框架。
【03：30-03：39】你知道，在Ray开源中，Ray开源专注于Python和AI。
【03：39-03：50】这意味着Python优先和AI优先。但Young，我们是不同的。所以你可以知道。你可以看到我们已经贡献了。
【03：50-03：55】Pi、Java和C++的API到项目中。
【03：55-04：03】在我们的第一层，上层。
【04：03-04：12】引擎被部署在我们的生产环境中。引擎的名字是G flow，它是一个流图引擎。
【04：12-04：23】它用于支付宝的风险控制和支付。在二十，二十，二十九在二十十九。
【04：23-04：31】另一个融合引擎被部署在我们的生产环境中。这个引擎是唯一的。
【04：31-04：40】我们感觉三种计算类型，包括流式、ML和在线服务。所以你可以知道。
【04：40-04：49】在线服务在这个时候也是在线服务。第一次被部署在动脉中，这是今天的话题。
【04：49-05：00】在二十，二十，鉴于业务的多样性，我们为Ray做了一个多租户架构。
【05：00-05：11】这意味着你可以在一个大集群中运行很多Ray作业。在这个作业中，你可以。
【05：11-05：16】支持不同的计算范式。
【05：16-05：28】在二十二十一年，动脉变得更加稳定。我们，我们做了更多的协作，与我们的生态系统进行协作。
【05：28-05：35】公司，比如阿里巴巴、达摩院、中国网络和网商银行。
【05：36-05：43】在二零二二年，我们探索了隐私计算，这是一个新的场景。
【05：43-05：53】我们认为这是Ray的一个好场景，因为在隐私计算中。
【05：53-06：01】我们需要一个灵活的框架来支持通用功能、数据和AI。
【06：01-06：07】它有很多优点，对它来说。
【06：08-06：17】好的，在今年，我们的目标是构建一个通用的AI。
【06：17-06：27】服务框架，在这个框架中，我们想要统一传统的AI、大型语言模型和搜索引擎。
【06：28-06：38】所以这就是今天的主题。回到在线调查的历史时间线。你可以知道。
【06：38-06：48】在线服务是在二零一九年首次部署在蚂蚁集团的。
【06：48-06：54】并且它被集成到在线学习引擎中。
【06：54-07：02】基于此，我们构建了保留的基本能力，并将其集成到基础设施中。
【07：02-07：12】艺术集团，你知道每个公司都有自己的客户基础设施。所以我们需要采用它。
【07：13-07：19】在我们这里，我们有另一个场景。
【07：19-07：30】在线资源分配，你可以想象，如果资源是用于支付和金融的。
【07：30-07：38】有什么挑战。我们认为挑战在于我们需要一个更灵活、高性能的。
【07：38-07：49】可扩展的、稳定的框架。幸运的是，我们在基于re调查的基础上实现了它。
【07：49-07：57】在二零二零年和二零二二年，我们支持大规模的在线无服务器平台。
【07：57-08：07】在这个时候，我们为两十四万个调用提供了模型服务，以及我们的事件驱动的无服务器平台。
【08：07-08：17】一点三七百万峰值TPS。好的，这是我们所有的私人工作。
【08：17-08：24】接下来，Chong将介绍我们今年最近的成就。
【08：25-08：31】欢迎，好的，谢谢Guya。大家好。
【08：31-08：40】我叫李冲，我很高兴在这里分享我们最近的成就。
【08：40-08：50】在任何集团，我们已经建立了最大的推理平台。我们的推理集群有五十万个CPU核心，和四千个GPU卡。
【08：50-08：59】总共，我相信你们已经在昨天Stoica博士的主题演讲中看到了这些数字。
【08：59-09：09】为了交付这些硬件，我们总共使用了超过两万七千个工作节点。作为一个推理平台。
【09：09-09：18】我们在平台上拥有非常活跃的模型部署。我们每周有超过三个新的模型部署。
【09：18-09：28】每周有一个以上的模型更新。我们的推理集群高度自动可扩展。
【09：28-09：40】多亏了我们的产品化和独立的自动缩放器，我们的推理集群现在可以每周自动缩放超过三千次。
【09：40-09：49】主动地，通过这样做，我们将平均CPU利用率提高了超过百分之二十。
【09：51-09：58】所以接下来，我将谈谈我们在生产中的新场景和一些新功能。
【09：59-10：11】在深入细节之前，我想先介绍一下我们的RedSurfing架构，它与开源的RedSurf没有太大的不同。
【10：11-10：23】在这个架构中，我们有一个服务守护进程，它接收所有来自用户客户端的服务推理作业提交。
【10：23-10：34】它必须将这些服务作业分发到我们的服务集群中。但对于每一个，服务守护程序都必须创建。
【10：34-10：46】并在特定的服务集群中创建相应的应用程序主节点。这个应用程序主节点将创建多个代理和部署参与者。
【10：46-10：56】在集群内。当代理准备就绪时，它们会向独立的服务发现组件注册自己。
【10：57-11：10】通过订阅该组件，我们的用户应用程序可以了解每个代理的位置，并仔细选择最合适的代理发送请求。
【11：10-11：20】推理请求。在每个代理中，您需要分配传入的推理请求。
【11：20-11：27】给其本地部署参与者，在那里进行模型服务工作。
【11：29-11：40】好的，新的场景，第一个是在GPU上进行模型求解工作以实现这一点，我们选择了使用NVIDIA Triton。
【11：40-11：51】对于那些不熟悉Triton的人。它是Nvidia的单节点模型推理集合。它支持多个。
【11：51-12：00】推理后端。在我们的服务系统中，Triton服务器可以非常容易地分布。
【12：00-12：08】我们所需要做的就是运行，使我们的部署参与者成为Triton引导程序。
【12：08-12：18】因此，每当我们的应用程序主节点创建一个新的部署参与者时，它会立即启动内部的Triton服务器。
【12：18-12：26】借助我们的参与者，长时间环境功能，这个长时间环境功能是。
【12：26-12：38】非常有用的，它由开源recall提供，并且在我们处理数据依赖包或库依赖程序时提供了很大的帮助。
【12：38-12：45】我相信我们的recall团队贡献了近一半的实现到开源。
【12：46-12：57】对，所以当这些Triton服务器准备好时，它们可以选择将自己注册到这个发现，服务发现组件。
【12：57-13：06】即将来的推理请求可以直接连接到这些Triton服务器。
【13：06-13：18】值得一提的是，因为我们的服务集群高度可自动扩展，所以在我们的平台上运行Triton服务器时。
【13：18-13：23】它们也变得可自动扩展。
【13：23-13：31】好的，下一个场景是为L，M应用程序提供服务。
【13：31-13：48】这个事情的背景是在我们的端组中，我们有一个名为GPT缓存的系统，在这个系统中，我们将尝试在我们的矢量或缓存存储中存储历史LLM响应。
【13：48-13：58】因此，每当有新的推理请求进来时，我们首先要做的是运行相似性比较。如果缓存命中，则我们只需。
【13：58-14：10】将预启动响应返回给用户，如果缓存未命中，则推理请求仍然必须通过模型服务管道。
【14：12-14：24】在我们的重新服务平台中，这个检查，这个G，P。T缓存系统可以非常容易地集成。我们所需要做的只是运行这个。
【14：24-14：33】Gpt缓存系统在我们的部署参与者内部。仍然，借助于参与者运行时功能。
【14：33-14：43】当然，我们需要一个集群中的入口参与者。它将帮助我们转发每个传入的推理请求。
【14：43-14：53】首先到Gbt缓存参与者。然后我们仍然做同样的事情。如果缓存命中，则我们快速响应。
【14：53-15：04】如果缓存未命中，那么我们只是将此请求转发到另一个Triton服务器，该服务器也在部署参与者中运行。
【15：06-15：17】所以你可以在这里看到的一个大事情是，在这张图片中，GPD缓存参与者在CPU节点上运行。
【15：17-15：27】而Triton服务器在GPU节点上运行。因此，多亏了Ray调用，异构结果调度。
【15：27-15：36】我们现在可以更明智地分配这些不同类型的参与者，并优化整体资源利用率。
【15：39-15：49】好的，接下来是一些新功能。第一个是内置的异步代理。我们这样做是因为在。
【15：49-15：58】我们的大多数模型服务场景中，我们发现推理请求具有非常不同的执行时间。
【15：58-16：07】对于这些长时间的请求，人们总是要支付大量的集中等待开销。
【16：07-16：18】所以我们在这里做的是在服务集群中部署一个异步代理。当然，这个异步代理在部署参与者中运行。
【16：18-16：26】以及。这个异步代理可以接收和排队每个传入的推理请求。
【16：26-16：37】对于这些长时间的请求，当结果准备好时，异步代理可以帮助我们将这些结果异步地返回给用户。
【16：40-16：52】从这个异步代理获得的一个大好处是，有了这个集群内部的代理，其他部署参与者。
【16：52-17：04】实际上可以根据自己的忙碌或空闲状态，从这个代理中自适应地拉取推理请求。
【17：04-17：12】通过这样做，在每个部署参与者中，它正在使用Triton服务器运行模型服务。
【17：12-17：18】在每个这样的部署参与者中，我们可以看到更少的队列延迟。
【17：19-17：29】在我们的评估中，与基线相比，即基于轮询的推送请求分配。
【17：29-17：35】我们的基于拉取的方法可以给我们带来两倍更好的吞吐量。
【17：38-17：48】下一个功能是C++部署。这个功能已经在我们的推荐和广告中广泛部署。
【17：48-17：58】服务，这些服务对低延迟和高吞吐量敏感。因此，由于这种性能要求。
【17：58-18：09】我们的部署参与者也必须是高性能的。所以我们所做的就是使用C++部署参与者。
【18：09-18：19】基于开源的Ray C++工作者。顺便说一下，这个C++工作者也是。
【18：19-18：31】主要由我们的Ray团队贡献的，所以感谢Guyang在这方面所做的努力，好的，有了这个C++部署参与者。
【18：31-18：40】我们所做的一件大事是实现C++直接入口。这是一个高性能的RPC服务。
【18：40-18：49】有了这个功能，我们的推理请求可以直接连接到我们的C++部署参与者。
【18：49-18：54】绕过我上面提到的所有这些代理。
【18：56-19：05】另一个重要的事情是本地Triton推理调用。这可以做到，因为我们现在有这些C++。
【19：05-19：15】部署参与者和Triton服务器，它可以被视为一个C++库，可以更紧密地和。
【19：15-19：26】高效地集成，没有任何跨语言开销。当我们制作C++部署参与者时。
【19：26-19：35】与其他C++组件一起工作在我们的推荐和广告时间线上。
【19：35-19：40】我们实际上是在做一个更全面的分布式系统。
【19：43-19：54】好的，接下来我会谈谈一些未来的计划，我们要做的第一件事是Shardit部署。
【19：54-20：03】这个事情的动机是在我们的推荐和搜索服务中，有CVR或者CTR模型。
【20：03-20：15】我们总是可以看到大量的项目，具有大量的特征数据，而这些特征数据的大小太大了，无法。
【20：15-20：27】适合任何单个工作节点。所以我们必须做的是尝试关闭这个大型特征数据。
【20：27-20：35】跨多个工作节点。当然，我们会创建代理部署。
【20：35-20：46】实际上，演员。并使用此代理帮助我们执行关闭路由策略，并确保每个传入的。
【20：46-20：56】推理请求可以转发到特定的节点，该节点具有本地可用的数据依赖关系。
【20：59-21：10】好的，最后一件事实际上是我们希望交付高性能通用AI框架的一个非常大的图景。
【21：11-21：19】为了实现这一目标，我们要做的第一件事是继续构建和。
【21：19-21：27】基于开源储备的速率服务平台，这已被证明。
【21：27-21：37】高度分布式、多语言、支持性和可扩展性。我们相信这可以成为。
【21：37-21：41】许多模型服务场景的良好基础。
【21：42-21：51】我们肯定会使用我们的直接入口，并用它来构建高性能的Rpc服务。
【21：52-22：02】我们将广泛使用C加加部署演员，因为它可以为我们提供高性能。
【22：02-22：11】计算能力。我们将开始研究这种分片部署，它为我们提供了。
【22：11-22：24】高性能、本地数据访问和数据检索。我们认为这个功能在处理大规模模型服务时非常重要。
【22：24-22：32】未来的问题。所以这就是我们今天基本上所有的内容，所以。
【22：32-22：44】正如我在开头所说的，这项工作主要由我们伙伴团队的Teng Wei完成，所以我们会尽力在这里回答问题，但如果不能，
【22：44-22：53】我鼓励你们发送电子邮件到这个地址，我相信他能给出最详细的答案。
【22：53-22：58】谢谢。
【23：02-23：10】那么直接入口和你为多少所做的C++工作已经集成到Rekor中？
【23：11-23：23】它已经开源了，但实际上在开源社区中并不流行，但我们大量在我们的生产中使用它。
【23：23-23：32】因为在推荐和广告系统中，因为我们希望使我们管道中的所有组件。
【23：32-23：40】推荐管道都是用C加加实现的，使整个管道更加完整。
【23：42-23：50】关于这一点的一个跟进问题是，在你刚才提到的那个空间，即推荐管道。
【23：50-24：00】如何，比如，你真的在用Ray做什么，因为使用Triton进行推理，对吧。而且你正在使用这个直接入口。但是什么角色。
【24：00-24：11】Ray在那里扮演什么角色，Ray是如何帮助你在那个空间的？是的，我们肯定使用Ray的运行时环境。
【24：11-24：20】帮助我们在部署演员中集成这个Triton依赖项。
【24：20-24：30】我们使用Ray的异构调度来帮助我们调度这些演员。
【24：30-24：40】CPU依赖的演员或GPU依赖的演员。它们都可以被很好地调度，并帮助我们提高资源利用率。
【24：46-24：54】感谢你的演讲。一个快速的问题。所以你提到大约有三个以上的部署。
【24：54-25：02】 每周的部署。我的问题是，对于一种模型，比如，是什么样的。
【25：02-25：12】 部署的平均或通常频率。假设我们有一个大型语言模型。然后你多久。
【25：12-25：20】 进行更新和部署。是的，我想这个范围你可以。你在这里难倒我了。
【25：20-25：28】 我真的不知道细节。是的，所以那个问题，那就是你。你应该给王腾发邮件。
【25：28-25：32】 是的，因为那是来自服务团队的一些细节。
【25：37-25：47】 它是一个与ray选择相关的分片部署。分片部署什么，是的，它是一个与ray选择相关的分片部署。
【25：47-25：58】 是的，它仍然在Ray上。我们使用，我们只需要将这些特征数据跨多个工作节点关闭，但工作节点仍然。
【25：58-26：05】 它建立在ray上，所有的actor都是处理程序，都是反应器。
【26：13-26：23】 所以所有这些都是在像Vms这样的基础上运行的。对，不是Kubernetes或其他任何东西。只是裸机Vm。我说的是，这都是在什么上运行的。
【26：23-26：33】 Vms，还是在Kubernetes上运行，还是在，是的，是的，我们。我们肯定在运行我们的ray集群。云原生环境中的ray集群。
【26：33-26：38】 在Kubernetes之上。
【26：43-26：50】 哦，好吧，如果没有更多的问题，那么非常感谢。