# Faster Model Serving with Ray and Anyscale | Ray Summit 2024

## 视频信息
- 视频ID: LKUBwGTUDZE
- 视频标题: Faster Model Serving with Ray and Anyscale | Ray Summit 2024
- 视频URL: https://www.youtube.com/watch?v=LKUBwGTUDZE

## 译文
[00:03 - 00:13] 大家好，我是Akshay，我是Racer的工程负责人，和我一起的是Ed，他是Racer的创建者，并且从一开始就领导着Racer。

[00:14 - 00:21] 快速举手示意一下，你们中有多少人已经在使用Racer或者至少尝试过它？

[00:21 - 00:31] 很棒，你们中有多少人已经将其投入生产？很好，看到越来越多的人将Racer一路用到底真是太好了。

[00:31 - 00:45] 今天我们将讨论如何使用Racer和AnyScale进行快速可靠的模型服务，由于我看到你们中的许多人对Racer仍然很陌生，我们会简要介绍一下其概述和动机，

[00:45 - 00:56] 构建Reserve以及我们在2024年所做的酷炫事情，然后我会谈谈为什么Reserve特别适合Gen AI应用。

[00:56 - 01:05] 最后，Ed将谈论我们为使Racer在任何规模上都非常出色而进行的一些扩展，以及AnyScale真正是最完整的。

[01:05 - 01:12] 模型调查性能方式，让我们马上开始吧。

[01:12 - 01:21] 这就是我们构建Racer的方式。所以如果你正在构建Ml应用程序。

[01:21 - 01:30] 你通常不仅仅是在构建一个模型，所以让我们想象一个推荐系统或计算机视觉系统的案例。

[01:30 - 01:39] 在像我们的朋友在Samsonia所做的CV应用程序的情况下，他们有很多不同的模型，也许有些用于分割，有些用于分类。

[01:39 - 01:50] 然后在上面添加一些额外的业务逻辑来完成他们的完整服务。现在，最典型的部署方式是使用微服务。

[01:50 - 01:59] 你们中的许多人都还在这个可怕的领域，每个组件都是自己的微服务，然后每个这些都独立扩展。

[01:59 - 02:11] 等等。但这有一些缺点。很难创新。有一个单一的应用程序现在被分成许多不同的配置、镜像。

[02:11 - 02:18] 你必须为每个单独的微服务设置单独的CICD管道，单独的监控。

[02:18 - 02:27] 然后当你升级它们时，你必须确保它们是向后兼容的。这真的会减慢速度并增加很多操作负担。

[02:27 - 02:35] 其次，你不能很容易地共享资源。所以让我们以我们的CV示例为例，在那里。

[02:35 - 02:46] 你可以在CPU上做一些业务逻辑，也许你的分割模型需要在某种GPU上运行，比如说A10G，而你的分类需要在不同类型的GPU上运行。

[02:46 - 02:54] 这在一个单一的微服务中很难表达，然后充分利用你的硬件。

[02:54 - 03:02] 当你进入LLMs时，这一点尤其困难，因为你在多个GPU上部署模型。

[03:03 - 03:13] 所以这就是构建Racer的动机，我们想要一个灵活、可扩展和高效的在线推理框架，让我们分解一下。

[03:13 - 03:22] 第一个方面是简单性，每个人都在使用Python进行ML开发，所以这必须是一个Python原生的框架。

[03:22 - 03:33] 使用一个简单的装饰器，你可以投入生产，所以使用Reserve，你只需在你的业务逻辑和ML模型类上添加serve.deployment装饰器。

[03:33 - 03:43] 这就是任何人，无论是数据科学家还是ML平台工程师，将这个模型投入生产所需的一切。

[03:45 - 03:57] 然后我们想为许多模型推理添加一流的支持，所以在我们的先前的CV案例中，你可以将每个这些作为单独的razor部署，每个都有自己的资源需求。

[03:57 - 04:11] 并且它们可以在同一服务内独立扩展，你还可以做诸如分数GPU之类的事情，所以假设你的一个模型只需要四分之一或一半的GPU，你可以表达这一点并且它们可以独立扩展，

[04:11 - 04:18] 在同一服务内，并且在Racer中还有一个流行的模式叫做模型多路复用，其中。

[04:18 - 04:27] 你可能有数千个微调模型，每个客户一个，你可以非常高效地将这些部署在一个副本池上。

[04:27 - 04:39] 和一个部署，最后我们希望让它非常灵活，这样你就可以使用、部署任何Gen AI模型，任何应用程序在Reserve之上。

[04:39 - 04:50] 并充分利用你的底层硬件。所有这些都需要准备好生产。而且这在Kubernetes和开源中都是可用的，以及任何规模。

[04:51 - 05:00] 这里有一些我们在过去几年看到的非常酷的成功故事，所以Samsara节省了他们年度推理成本的50%。

[05:00 - 05:11] 通过转移到Reserve，所以他们正在部署这种单体架构，当他们有了更富有表现力的框架与Reserve时，他们可以真正获得更好的硬件利用率。

[05:11 - 05:20] 而Financial真正将Razor推向了下一个规模，通过构建一个拥有4000个GPU节点的在线服务集群用于推荐系统。

[05:20 - 05:27] 然后Clary在任何规模上通过使用Rayserve的模型多路复用提高了超过百分之二十的性能。

[05:27 - 05:36] 我们看到这张图表在右手边，它显示了自从我们在二零二二年推出Racer以来的指数增长趋势。

[05:36 - 05:46] 仅仅自上次比赛峰会以来，我们已经看到了集群和用户的快速增长。所以这确实说明了社区对Racer的采用情况。

[05:48 - 06:00] 所以让我们来谈谈我们在二零二四年所做的一些事情，所以我们有三个主要的关注领域，一是继续推动Reserve中的成本和性能优化。

[06:00 - 06:09] 让它更适合Gen AI应用程序。然后第三，继续推动生产就绪性，这样你就可以非常可靠地使用Racer进行生产。这里是一些主要功能。还有很多其他事情发生，但我只列出前四个。

[06:09 - 06:19] 首先，我们在Racer中重新设计了自动缩放和负载管理，所以我们从社区中得到了一些关于自动缩放的反馈，要求它更具响应性。

[06:20 - 06:28] 并且还具有负载卸载的能力，以提高安全性和生产性。因此，这些是在Racer中添加的。

[06:28 - 06:36] 在我们推出了多个应用程序支持。所以这是你可以在同一个Racer集群中部署多个单独的ML应用程序的地方。

[06:37 - 06:46] 每个都有自己的升级生命周期。社区的一个反馈是，他们喜欢这个功能。很多人开始使用它。

[06:46 - 06:57] 因为它实现了更好的硬件利用率，但他们希望将这些ML的依赖关系分开。

[07:05 - 07:14] 这些天的软件包变得非常臃肿。这很重要，因为您正在部署不同类型的模型。因此我们增加了对单独容器镜像的支持。

[07:14 - 07:19] 在同一费率服务中。

[07:19 - 07:31] 这是我非常兴奋的一个功能，因为它反映了Racer在社区中的重要性，这是一个由社区贡献的功能，用于在任何硬件加速器上运行Racer，

[07:31 - 07:41] 因此来自Google、Amazon和Intel的人们为Ray做出了贡献，以便在Google Cloud上的TPU上运行RayServe。

[07:41 - 07:49] 在Amazon Cloud上的Inferentia和Intel上的Habana芯片上，所以你可以在任何类型的加速器上运行Racer。

[07:50 - 07:58] 最后，我们与其他公司合作，也为在线推理的最佳框架添加了支持。

[07:58 - 08:07] 我们与NVIDIA Triton合作，现在支持一种集成，你可以在Razer的基础上获得Triton的性能优势。

[08:07 - 08:12] 然后还有TensorRT LLM和VLLM支持。

[08:13 - 08:22] 好的，现在让我们看看为什么Reserve对于Gen AI应用程序来说真的很棒，而Gen AI带来了很多独特的挑战。

[08:23 - 08:28] 所以我们将通过一个案例研究来了解在Reserve上提供LLMs的情况。

[08:29 - 08:37] 首先，最大的挑战是这些模型现在绝对巨大，有七B模型，它们通常被使用。

[08:37 - 08:44] 在开始时是十四吉字节，但现在我们有七十B和四或五B模型，它们接近一太字节。

[08:44 - 08:53] 而且GPU非常昂贵，可用性似乎并没有得到很大的改善，所以你可以看到A10G，它有...

[08:53 - 09:04] 二十四吉字节的内存已经相当贵了，然后当你到达H一百时，部署这些系统的成本变得非常高，这意味着你真的想要，

[09:04 - 09:16] 尽可能地利用你的硬件，所以在任何规模下，我们今年早些时候都在考虑如何让人们能够简单地部署LLMs，

[09:16 - 09:22] 这些是人们向我们提出的一些用例。

[09:22 - 09:31] 首先，这是非常基本的，你必须做多GPU，这些模型通常太大，无法装入一个GPU内存。

[09:31 - 09:39] 现在我们也看到了多节点，所以你必须启用这些一太字节的模型在多个节点上运行。

[09:40 - 09:49] 高性能服务极其关键，正如我之前提到的，这些GPU的成本非常高，所以你必须得到。

[09:49 - 09:54] 最佳的性能。

[09:54 - 10:03] 另一个常见的模式是人们正在将LLMs适应他们自己的领域，并且他们可能会为不同的客户分别进行。

[10:03 - 10:09] 所以人们想在一个集群中提供许多不同的微调模型。

[10:10 - 10:18] 然后一个常见的模式也是提供许多不同的基础模型，所以想象一下你在构建一个聊天机器人。

[10:18 - 10:26] 其中一些查询可以用B模型处理，但有些需要更高级的推理，你需要将那个查询路由到一个B模型。

[10:28 - 10:37] 当然，LLMs就像其他任何模型一样，你不能只把一个模型放在一个端点后面并称之为一个应用程序。

[10:37 - 10:43] 你需要围绕它构建更多的组件。所以检索增强生成是非常常见的。

[10:44 - 10:51] 然后这是一个非常独特和酷的要求，你需要能够使用混合云。

[10:51 - 11:02] 很多时候人们无法在AWS或Google Cloud上获得他们需要的GPU，他们会从Lambda Labs或CoreVave等供应商或其他第三方提供商那里采购GPU。

[11:02 - 11:06] 你希望能够高效地使用所有你的GPU。

[11:08 - 11:16] 这就是我们在Racer中解决这个问题的方法，这是这个库的高级架构的样子。

[11:16 - 11:30] 首先，我们有一个Rayserve部署，可以处理所有的OpenAI API和业务逻辑，这对于与任何其他LLM工具集成至关重要，比如Lanchain、Lama Index等等。

[11:30 - 11:39] 这些可以在仅CPU上运行，然后在后台，你可以有许多不同的LLM，可以通过配置文件轻松设置。

[11:39 - 11:48] 你可以部署一个8B模型或一个70B模型以及嵌入模型，所以让我们看看它是如何解决一些我们的用例的。

[11:49 - 11:57] 模型并行非常容易设置，这是RAISE的一个核心优势，管理和编排Python进程。

[11:57 - 12:05] 在不同的GPU上，事实上，甚至管道并行现在也是可能的，因为我们刚刚推出了新的编译图API。

[12:05 - 12:17] 我们一直在内部进行实验，高性能，所以我们支持VLLM、TensorRT LLM，实际上任何你想使用的推理引擎。

[12:17 - 12:25] 但我们也在开发我们自己的专有内核，以真正加快一些执行速度。

[12:26 - 12:37] 多LoRa或多微调模型是可能的，因为Racer的多路复用支持，所以它的设置方式是我们使用一个Racer多路复用部署。

[12:37 - 12:46] 它可以根据需要有效地交换和加载LoRa权重，然后使用VLNM支持在同一批次中对许多LoRa进行推理。

[12:47 - 12:57] 多模态推理，所以在一个服务中运行8B和70B也是可能的，通过这个库很容易设置。

[12:57 - 13:02] 你可以一次性升级引擎。

[13:03 - 13:10] 最后，混合云也是可能的。这是我们内部做的一些事情，以及我们的客户正在做的。

[13:10 - 13:24] 所以，举个例子，一个70B的模型在H100上运行，其中一部分在AWS节点上运行，但它也有能力爆发到Lambda Labs的容量，或者有些人走的是相反的方向，

[13:24 - 13:31] 他们可能在第三方云上有大量的预留容量，然后当他们需要额外的容量时，他们会爆发到云端。

[13:32 - 13:43] 让我们来看看你会如何设置一个检索增强生成系统，这通常用于将更多上下文引入LLM的响应。

[13:43 - 13:52] 因此，在高层次上，它由两部分组成，一旦你得到用户查询，你将进行检索，它本身包含。

[13:52 - 14:03] 生成和嵌入，并在你的向量数据库上进行向量搜索，然后将其转换为上下文和提示，然后你可以将其发送给你的LLM。

[14:03 - 14:16] 然后将最终响应发送给用户我之前谈到了Racer的模型组合，它非常适合rag模式，你可以将所有这些设置为单独的tracer部署。

[14:16 - 14:26] 这样就可以使用不同的资源并独立扩展。现在我把它交给Ed，让他谈谈我们用Racer在任何规模上所做的出色扩展。

[14:27 - 14:37] 好的，谢谢Akshay，希望每个人都能听到我的声音，是的，所以我要谈一谈我们正在构建的一些很酷的功能，以使Rayserve在任何规模上都能很好地工作。

[14:39 - 14:47] 好吧，对于那些不太熟悉AnyScale平台的人来说，我认为可以这样想，它就像是Ray的一个扩展。

[14:47 - 14:56] Array就像AI计算引擎。它使得编写分布式程序变得非常容易，做所有在线服务应用程序的酷事，正如Akshay提到的那样。

[14:56 - 15:09] 但是还有很多其他的东西围绕着它，比如成本跟踪、管理功能、可观察性、生产准备性，这些都不太适合Ray的范围，但如果你真的要投入生产并拥有一个端到端的平台，你需要它们。

[15:09 - 15:19] 所以AnyScale是我们尝试围绕Ray构建一些真正与之集成的东西，并为你提供这个端到端的AI平台。

[15:19 - 15:30] 对于Rayserve，我们关注的几个关键领域之一是为端到端的可观察性和调试添加非常好的支持。

[15:30 - 15:42] 这对在线服务应用程序尤其重要，你知道，真正重要的是你能理解，如果你有延迟峰值，为什么会发生这种情况，如果事情出错，你知道，哪里出了问题？

[15:42 - 15:47] 并且有监控和警报，我相信很多人都对此很熟悉。

[15:47 - 15:54] 同样地，我们也专注于确保一切都是坚固的并且生产就绪。

[15:54 - 16:06] 而且由于AnyScale是一个从管理硬件到ray层再到库层再到应用程序代码的全栈，它也有很多独特的机会。

[16:06 - 16:14] 我们有一些很好的机会来进行全栈优化，提高性能并节省成本。

[16:14 - 16:25] 那么让我们更详细地谈谈每一个，首先是可观察性和调试，所以AnyScale附带了一个集成的UI，它向你展示了，

[16:25 - 16:35] 一目了然的状态，以及历史信息，并且还内置了很多协作功能，我们也有现成的日志和指标解决方案。

[16:35 - 16:41] 我在这里没有提到，但我们也有一个分布式跟踪支持的集成。

[16:41 - 16:54] 那么让我们更仔细地看看当你在AnyScale上运行一个race serve应用程序时它是什么样子的，所以这可能有点小，但这是你看到的AnyScale服务的仪表板。

[16:54 - 17:04] 所以你可以看到，左上角的这个服务目前处于推出状态，这意味着它正在逐步转向一个新版本，因为它被部署了。

[17:04 - 17:14] 对于每个版本，我们可以看到所有的关键元数据，比如使用的图像，使用的计算配置，它是哪个版本，谁部署了它，什么时候部署的。

[17:14 - 17:20] 因此，它为你提供了我的应用程序正在发生什么的一目了然的视图。

[17:20 - 17:32] 除此之外，我们还有直接内置到平台中的指标，所以这里我随机挑选了两个，一个是利用率指标，另一个是QPS指标。

[17:32 - 17:41] 但我们有一系列开箱即用的内置仪表板，你也可以自定义自己的Grafana仪表板，并根据需要添加警报。

[17:42 - 17:56] 我非常兴奋的是，今年我们在平台上推出了对日志聚合和搜索的内置支持，因此当你在任何规模上启动服务时，你现在会看到像我在屏幕中心所展示的视图。

[17:56 - 18:07] 它不仅聚合了一个集群中所有节点的日志，而且随着时间的推移，即使你在部署和升级等过程中，它也能跨多个集群进行聚合。

[18:07 - 18:17] 你可以根据版本、组件，比如应用程序是什么来过滤，并且你甚至可以搜索，例如在这里我正在搜索一个给定的请求ID，我可以看到，

[18:17 - 18:29] 在系统中，对于那个请求ID，在race serve级别日志和应用程序日志之间发生了什么，所以这只是一个绝对无价的工具，而且开箱即用真是太棒了。

[18:31 - 18:39] 然后我想提到的另一件事是，我们在Ray Summit上宣布的一件事是AnyScale现在可以在Kubernetes作为计算后端上工作。

[18:39 - 18:49] 基本上，你可能运行的任何Kubernetes集群。这样你就可以兼得两全其美，你可以在现有的计算上运行，并与你在Kubernetes上的其他应用程序集成。

[18:49 - 18:55] 但同时获得所有这些功能和其他我将在演讲中讨论的功能。

[18:56 - 19:03] 好吧，这只是关于可观察性和调试工具的一个简短概述，让我们谈谈改进的生产准备情况。

[19:03 - 19:15] 所以在AnyScale上，我们还有一个开箱即用的头节点容错功能，Ray有一个头节点的概念，默认情况下它将事物存储在内存中。

[19:15 - 19:25] 虽然一般来说没有问题，但如果你确实遇到硬件故障或任何意外情况，它可能会导致应用程序的一些停机时间。

[19:25 - 19:33] 你可以通过启用头节点容错来解决这个问题，但这会增加更多的操作负担，因为你需要外部存储。

[19:33 - 19:41] 在AnyScale上，我们默认为你配置了这一点，并处理整个恢复过程。

[19:41 - 19:48] 自动地，所以如果头节点因为任何原因宕机，你不会丢失任何请求，它将会被恢复。

[19:49 - 20:00] 我们还支持像你在UI页面上看到的那样的滚动更新，我想谈一谈的是，我们最近也扩展了这个功能，以支持增量滚动更新。

[20:00 - 20:04] 让我们来谈谈那是什么以及为什么它很重要。

[20:05 - 20:17] 这里我有一个图表，展示了当你在AnyScale上运行服务时的样子，所以有一个AnyScale控制平面，它管理一个负载均衡器以及一些运行应用程序的Ray集群。

[20:17 - 20:30] 在这种情况下，我有一个Ray Serve集群，正如Akshay提到的，这可能由多个部署、多个模型组成，它们在一个相当复杂的拓扑结构中组合在一起。

[20:30 - 20:40] 但是在这里，我把它简化为一个集群，并说它使用十个GPU，这是在稳定状态下，所以所有的请求都会被发送到这个集群，

[20:40 - 20:50] 并且它们会在集群中的所有节点之间进行负载均衡。当我们想要升级时，用户会发出一个带有新配置的部署命令。

[20:50 - 21:01] AnyScale控制平面将首先创建集群的新版本，然后更新负载均衡器以缓慢地进行分阶段推出，例如，从10%的流量开始。

[21:01 - 21:07] 然后继续，假设所有的状态和健康检查看起来都很好。

[21:07 - 21:16] 但在这个图中有一个相当大的问题，那就是现在当你升级你的应用程序时，这需要二十个GPU。

[21:16 - 21:25] 你会注意到只有百分之十的流量会去第二个集群，所以这些GPU资源中的很多可能会处于空闲状态。

[21:25 - 21:33] 这是Rayserve的一个有点独特的问题，因为你可以在一个集群上部署这些复杂的拓扑结构。

[21:33 - 21:43] 像标准的Kubernetes部署推出这样的东西并不完全符合模型，所以这是一个我们有几个客户遇到问题的领域。

[21:43 - 21:51] 我们想基本上解决它并给你最好的两个世界，所以让我们谈谈AnyScale现在支持什么。

[21:52 - 22:04] 所以关键的区别在于，我们不会启动新应用程序或新集群的完整版本，而是会创建一个基本的最小版本，使用尽可能少的资源。

[22:04 - 22:16] 在这种情况下，也许它只需要一个GPU，或者如果有两个模型，它可能只是两个GPU，然后我们将逐步缩小旧集群并扩大新集群。

[22:17 - 22:24] 同时协调流量分割，以便每个集群只获得它目前可以处理的流量。

[22:24 - 22:34] 这样，我们可以逐步扩大规模，例如，我们最终将达到50-50的分割，每个集群大约是原来的一半大小。

[22:34 - 22:49] 然后新的集群将接近完整的大小，最后我们能够完成推出，整个过程只需要n加一的GPU，所以在这种情况下，只需要十一而不是我们用简单解决方案的二十个。

[22:50 - 22:59] 所以这是一个非常简单的功能和概念，但显然这是一个巨大的改进，这也是一个需要在控制平面、负载均衡器和集群之间进行非常仔细协调的东西。

[22:59 - 23:09] 因为我们有一个控制平面，它可以同时管理所有这些组件。

[23:11 - 23:19] 好吧，所以我想谈的最后一件事，关于Rayserv在AnyScale上的最后一个主要点是我们所做的全栈优化工作。

[23:19 - 23:31] 所以今年我们添加的一个优化是安全副本迁移，所以基本上如果我们检测到集群处于某种次优配置。

[23:31 - 23:41] 我们会将副本从一个节点移动到另一个节点，以便释放实例并将其归还给云提供商。所以这是一个成本节约机制，如果你有。

[23:41 - 23:46] 多个模型可以自动上下扩展。

[23:47 - 23:55] 我们还增加了对在现货实例上提供服务的支持，而不会丢失请求，因此如果一个现货实例被抢占，任何规模都会。

[23:55 - 24:05] 通过启动一个新的副本并转移流量来处理它，然后我们真正酷的事情是，当现货实例再次可用时，

[24:05 - 24:10] 我们会从按需模式切换回现货模式以节省资金。

[24:11 - 24:19] 最后，我们看到的一个大痛点不仅适用于任何规模的客户，也适用于开源用户和一般进行模型服务的人。

[24:19 - 24:27] 是自动缩放可能会变得非常慢，特别是对于大型模型。所以我们一直在努力提高自动缩放速度。

[24:27 - 24:36] 所以让我们更详细地谈谈这一点，正如Akshay之前提到的，模型已经变得绝对巨大。

[24:36 - 24:48] 这导致了一些相当严重的问题，所以我认为很多人可能看过这个经典的XKCD漫画，人们在等待代码编译时偷懒。

[24:48 - 24:58] 但我认为我们需要为现代AI驱动的世界更新它，我们仍然在做同样的事情，但现在我们的借口是模型正在加载而不是代码编译。

[24:59 - 25:06] 所以这确实会导致一些严重的问题。所以在开发中，我敢肯定基本上在这个房间里的每个人都经历过。

[25:06 - 25:18] 只是坐在那里等待你的Kubernetes集群自动缩放，然后容器拉取，然后你的巨型Python代码启动，然后巨大的模型权重加载。

[25:18 - 25:25] 你只是坐在那里等待，实际上并没有测试或开发或做你的工作。

[25:26 - 25:35] 在生产中，这种缓慢的缩放通常会导致过度配置，如果你不能快速自动缩放以响应增加的流量。

[25:35 - 25:43] 那么你最终不得不做的基本上是拥有更多的余量和更多的空闲资源，这意味着你只是在昂贵的GPU上浪费钱。

[25:45 - 25:54] 所以我们运行了一个基准测试，看看典型的VLLM在Kubernetes上的设置会是什么样子。

[25:54 - 26:04] 哦，我的幻灯片顺序不对。好吧，我要倒回去。所以我们在这里运行了一个基准测试，看看。

[26:04 - 26:11] BLM在Kubernetes上启动一个新节点、启动一个新pod、拉取容器并加载模型权重需要多长时间。

[26:12 - 26:23] 所以这里我在右边的图上显示，对于一个7B的模型，端到端几乎需要十分钟，大约一半的时间花在启动一个节点和拉取一个容器镜像上。

[26:23 - 26:31] 另外一半的时间花在等待VLLM启动、下载模型并将模型加载到GPU内存中。

[26:31 - 26:37] 这是针对FP16的，所以模型权重总共约为140GB。

[26:38 - 26:47] 所以AnyScale在这里有几个关键优化，第一个是我们有一个自定义的容器镜像格式。

[26:47 - 26:55] 这使得拉取大型图像的速度比使用默认的Docker容器D客户端快得多。

[26:55 - 27:06] 嗯，这看起来是当你在AnyScale上启动一个集群时，你可以提供任何类型的通用容器镜像，所以这可以托管在ECR或任何容器注册表中。

[27:06 - 27:13] 它基本上只是一个带有自定义依赖项的射线图像，非常类似于您将与QBray一起使用的图像。

[27:13 - 27:23] 第一次使用此图像时，我们将从容器注册表中提取图像，然后将其自动优化为自定义图像格式。

[27:23 - 27:31] 然后将其缓存在每个区域的存储桶中，因此在那之后，任何时候您使用相同的图像，启动速度都会大大提高。

[27:32 - 27:40] 因此，我们运行了一个更简单的基准测试，只是测量拉取两个图像所需的时间，一个是六千兆字节，它就像一个射线基础图像。

[27:40 - 27:53] 一个是十三千兆字节，那个基本上是射线加VLM及其依赖项，我们比较了EKS和AnyScale，在这里你可以看到AnyScale在拉取图像和启动容器方面要快得多。

[27:53 - 28:02] 所以EKS，这是经过相当多的优化努力之后，需要超过一百秒来拉取六千兆字节的图像。

[28:02 - 28:11] 对于十三千兆字节的图像，大约是二百三十秒，而AnyScale要快得多，分别只有三秒半和十六秒。

[28:13 - 28:22] 我们还有另一个优化，即优化模型加载，以帮助处理非常大的模型权重。

[28:23 - 28:32] 所以我们看到我们的许多客户所做的常见解决方案是这样的，所以他们将他们的模型权重存储在AWS S3或其他等效的远程存储中。

[28:32 - 28:38] 他们会首先从存储下载到磁盘，然后将磁盘加载到GPU。

[28:38 - 28:47] 但在幕后，这有一些问题，因为你基本上有三个同步步骤，你首先从S3下载到本地磁盘。

[28:47 - 28:55] 然后你从本地磁盘加载到CPU，然后你从CPU加载到GPU，当模型权重真的很大时，每一个步骤都可能非常慢。

[28:56 - 29:05] 因此，AnyScale有一个安全张量兼容客户端，你可以直接指向远程存储中的模型权重，并将它们直接加载到GPU上。

[29:06 - 29:18] 在幕后，这基本上利用了管道并行性，它会逐块获取张量，并将它们直接流式传输到GPU上，避免这些同步复制步骤。

[29:19 - 29:23] 这导致模型下载时间大大加快。

[29:23 - 29:31] 因此，在一个实验中，我们只是测试七B和七十B模型的VLM启动和模型加载方面。

[29:31 - 29:39] 你可以看到，对于左边的7B模型，AnyScale客户端比使用基线快约两倍。

[29:39 - 29:44] 而对于七DB模型，它几乎，是的，大约快五倍。

[29:46 - 29:59] 因此，如果你把这些东西放在一起，端到端，AnyScale能够比在Kubernetes上运行基本相同的设置快五倍以上地扩展一个新的七DB模型副本用于服务。

[30:01 - 30:12] 是的，那么就这样，我会结束并再次强调，你知道，AnyScale提供了很多很棒的扩展，真正使Racer成为端到端AI平台的一部分。

[30:12 - 30:17] 包括可观察性、改进的生产准备度和一些非常酷的优化。

[30:17 - 30:25] 那么，我认为我们差不多到了时间，但Akshay和我将会在场回答你可能有的任何问题。

