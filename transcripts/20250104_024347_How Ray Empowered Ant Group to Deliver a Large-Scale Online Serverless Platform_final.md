---
video_id: _yu0Rtuetuc
video_url: https://www.youtube.com/watch?v=_yu0Rtuetuc
video_title: How Ray Empowered Ant Group to Deliver a Large-Scale Online Serverless Platform
created_at: 2025-01-04 02:44:51
---

# How Ray Empowered Ant Group to Deliver a Large-Scale Online Serverless Platform

[00:40 - 00:40] 今天的话题是关于如何赋能艺术团队交付一个大规模在线服务平台。 让我们开始吧。
[01:23 - 01:29] 在下一页，我将向您展示很多关于我们团队的历史和信息。 但在那之前，我想先分享一些高层次的信息。
[01:56 - 02:13] 我们与ANYSCALE和RESLAB合作多年，并且在开源部分中贡献了超过26%的记录。 其次，除了开发速度外，我们还在生产中应用了大量的RAND大规模应用，每天有超过100万次的CPU调用。
[02:52 - 03:01] 第五届REFORWARD已经在过去的七月结束。在这次大会上，ANYSCALE、跨AI团队、A'T和BAD DANCE分享了他们的工作。 所以，我们希望RAY在中国也能像在美国一样受欢迎。
[03:16 - 03:31] 我们的团队成立于2017年，这一年RAY与今天有很大的不同。 第三，我们认为RAY是一个通用的分布式系统，这意味着我们有很多非AI应用和框架。
[05:22 - 05:35] 在2021年，系统变得更加稳定。 我们与生态系统中的公司如阿里达摩院、中国网络和万商银行进行了更多的合作。
[05:53 - 06:07] 我们认为，隐私计算对于R是一个很好的应用场景。 在隐私计算中，我们需要灵活的框架来支持通用功能数据和AI，并且它有很多优势。
[06:56 - 07:05] 依赖于此。 我们构建了基本的保留能力，并将其整合到蚂蚁集团的基础架构中。
[09:41 - 09:48] 因此，得益于我们的产品团队和独立自动扩展器，我们的推理集群现在每周可以主动扩展超过3000次。 通过这种方式，我们提高了平均CPU利用率超过20%。
[09:51 - 09:58] 所以下面我将讨论我们生产中的新场景以及一些新功能。
[10:12 - 10:17] 这与开源版本并没有太大不同。 在这个架构中，我们有服务守护进程。
[10:40 - 10:47] 但是，对于每个服务守护进程，需要在某个特定的服务集群中创建相应的应用程序主控。 这个应用程序主控将在集群内创建多个代理和部署角色。
[10:57 - 11:12] 当代理准备好后，它们会向独立的服务发现组件注册自己。 并通过订阅这个组件，我们的用户应用可以了解每个代理的位置，并仔细选择最合适的代理来发送推理请求。
[11:13 - 11:27] 而在每个代理中，必须将传入的推理请求分配给本地部署的角色，在那里完成模型服务工作。
[12:47 - 13:06] 是的。 所以当这些跟踪服务器准备就绪时，可以选择向这个发现服务组件注册自己，即将到来的推理请求可以直接连接到这些Triton服务器。
[13:06 - 13:21] 另外一点是你提到的，由于我们的服务集群高度自动化可扩展，所以在我们的平台上运行trton服务器时，它们也会自动扩展。
[13:57 - 14:11] 所以每当有新的推理请求进来时，我们首先会进行相似性比较。 如果缓存命中，则我们直接将预先存储的响应返回给用户；如果缓存未命中，则推理请求仍需通过模型服务管道。
[14:53 - 15:04] 如果缓存命中，则进行快速响应。 如果缓存未命中，则我们只需将此请求转发到另一个在部署演员中运行的Triton服务器。
[15:21 - 15:37] 这里可以看到的一个大问题是，在这张图中，G P gpd 现金演员在CPU节点上运行，而trident服务器在GPU节点上运行。 多亏了资源异构结果调度，我们现在可以更明智地分配这些不同类型的演员，从而优化整体资源利用率。
[15:58 - 16:07] 我们这样做是因为在大多数模型服务场景中，我们发现推理请求具有非常不同的执行时间。 因此，对于这些长时间请求，人们总是需要支付大量的同步等待开销。
[16:26 - 16:38] 这个异步代理可以接收每个传入的推理请求队列。 对于这些长时间请求，当结果准备好时，异步代理可以帮助我们异步地将这些结果返回给用户。
[17:03 - 17:18] 我认为使用这个异步代理的一大好处是，由于它位于集群内部，其他部署角色可以根据自己的忙碌或空闲状态从这个代理中自适应地拉取推理请求。 因此，通过这样做，每个运行Triton服务器的部署角色在执行模型服务时，都可以看到更少的队首延迟。
[17:19 - 17:35] 在我们的评估中，与基于错误的推式请求分配方法相比，即基线方法，我们的轮询方法可以提供两倍的吞吐量。
[18:40 - 18:54] 它是一个高性能的RPC服务。 通过这一功能，我们的推理请求可以直接连接到我们的C++部署角色，绕过所有上述提到的代理。
[19:01 - 19:20] 另一件大事是原生Triton推理调用。 这个功能得以实现是因为我们现在有了C++部署角色和Triton服务器，后者可以被视为一个C++库，可以更紧密且高效地集成，而没有任何跨语言开销。
[19:20 - 19:40] 当我们部署C++SPAS时，与其他C++组件协同工作，在推荐和广告时间线上进行协作，实际上我们正在构建一个更全面的分布式系统。
[20:30 - 20:56] 所以我们必须尝试将这些大型特征数据分布在多个工作节点上。 当然，我们将创建代理部署演员，并使用这个代理来帮助我们实现分发路由策略，确保每个传入的推理请求能够被转发到本地可用数据依赖的具体节点。
[21:33 - 21:40] 首先，我们要继续建设和完善基于开源储备的推荐平台，该平台已被证明具有高度分布性、多语言支持和可扩展性。 我们相信这可以成为许多模型服务场景的良好基础。
[21:52 - 22:04] 我们肯定会利用直接入口并用它来构建高性能的RPC服务。 我们会广泛使用C++部署演员，因为它们能给我们带来高性能计算能力。
[22:16 - 22:26] 我们将开始着手处理共享部署，这将为我们提供高性能的本地数据访问和数据检索。 我们认为，当未来面对大规模模型服务问题时，这一特性将变得非常关键。
[22:32 - 22:39] 所以，这就是我们今天的所有内容。 如我在开头所述，这项工作主要由我们团队中的Ten G Wei完成。
[22:53 - 22:55] 如果我们无法回答，请大家发送邮件到这个地址，我相信他会给出最详细的解答。 谢谢。
[23:19 - 23:40] 这些工作已经是开源的，但实际上在开源社区中并不太受欢迎。 但在我们的生产环境中使用得很多，因为在推荐和广告系统中，我们希望让推荐管道中的所有组件都用C++实现，以使整个流程更加完整。
[24:32 - 24:40] 我们还使用Ray异构调度来帮助我们调度这些演员，无论是CPU依赖型还是GPU依赖型。 它们可以很好地被调度，并帮助我们提高资源利用率。
[25:14 - 25:31] 比如说，我们有一个大型语言模型，那么您多久更新和部署一次？ 是的，这个问题你可以给Ten G Wei发邮件，因为这是一些来自服务团队的细节。
[25:42 - 25:47] 这个问题是关于与Ray选择相关的部署。 是的，这是一个与Ray选择相关的部署问题。
[25:47 - 26:05] 是的，它仍然在运行。我们使用的是RWE，我们只需要关闭这个功能数据跨多个Walker笔记，但Walker笔记仍然基于Ray，所有处理的演员都是反应器。
[26:25 - 26:35] 还是在Kubernetes上运行？ 或者我们在云原生环境中基于Kubernetes运行我们的roclusters。
[26:48 - 26:50] 好的。 谢谢大家。