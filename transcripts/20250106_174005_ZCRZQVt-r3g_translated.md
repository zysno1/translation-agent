# Ray at Scale: Apple's Approach to Elastic GPU Management | Ray Summit 2024

## 视频信息
- 视频ID: ZCRZQVt-r3g
- 视频标题: Ray at Scale: Apple's Approach to Elastic GPU Management | Ray Summit 2024
- 视频URL: https://www.youtube.com/watch?v=ZCRZQVt-r3g

## 译文
[00:05 - 00:10] 大家好，感谢大家参加本次演讲，这是峰会的倒数第二场演讲。

[00:11 - 00:20] 今天，我和Weiwei将讨论Ray的弹性GPU管理，以实现AIML工作负载的可扩展性和效率。

[00:22 - 00:31] 首先，我来介绍一下我在Apple和LinkedIn与Ray社区合作的工作。

[00:31 - 00:41] 然后，Weiwei，你能不能简单介绍一下自己？是的，大家好，我叫Weiwei，我在Apple负责数据和Ava基础设施。

[00:41 - 00:49] 基本上帮助团队在多个云上构建运行数据处理和机器学习工作负载的基础设施。

[00:53 - 01:02] 所以今天，我们将给大家简要介绍Ray。我们会讨论Gp工作负载的扩展挑战。

[01:02 - 01:10] 我们尝试过的一些优化GPU容量管理和利用率的技术。

[01:12 - 01:22] 为什么要使用Ray？Ray为数据处理、训练、调优和服务提供了一个统一的计算API。

[01:23 - 01:36] 对于我们来说，它的一个优点是它提供了这种无缝的API，可以在用户的笔记本电脑到数据中心之间工作。

[01:36 - 01:44] 用户可以尝试一些小实验，然后迁移到集群中，而不需要做很多改变。

[01:45 - 01:50] 它对模型服务有很好的支持，包括VLLM。

[01:51 - 02:00] 用户使用我们的Ray平台有三种不同的方式，笔记本用于早期迭代。

[02:00 - 02:12] 这是他们可以尝试早期实验的地方，他们可以像在自己的笔记本电脑上一样尝试，这是一种交互模式，然后他们可以使用相同的API进行全规模，

[02:12 - 02:24] 在数据中心进行大规模工作负载，此外还有笔记本，用户总是需要成熟的SDK来进行程序化作业提交和模型部署。

[02:24 - 02:34] 还有门户网站来支持基于UX的模型部署和可观测性，这是一个非常重要的部分。

[02:34 - 02:42] 任何生产化的工作负载，虽然Ray有很多功能，但在使用Ray时也存在不少挑战。

[02:44 - 02:52] 其中一个是资源大小，当涉及到用户可以请求的资源类型时，选项太多了。

[02:52 - 03:01] 这并不是ML从业者在执行批处理推理作业或部署模型时首先考虑的问题。

[03:01 - 03:09] 自动缩放试图帮助解决这个问题，但也带来了我们看到的自己的挑战。

[03:09 - 03:17] 最后，GPU利用率是一个问题，无论是ML从业者还是...

[03:17 - 03:25] 基础设施工程师都在努力解决，因为对GPU的需求很高，而且涉及成本。

[03:27 - 03:36] 更深入地探讨这个问题，用户有不同的基础设施资源选择，所以他们必须选择。

[03:36 - 03:49] 节点或机器的数量，节点上的卡的数量，它们拥有的GPU内存，SRAM与DRAM。

[03:49 - 03:59] GPU内部和GPU之间的连接性，在他们选择的特定地理区域中的网络拥塞。

[03:59 - 04:14] 区域的整体容量和拥塞，这些选择对于人类来说太难指定了，而他们在启动一个相当大的工作负载。

[04:14 - 04:23] 正在生产化一个模型部署。我们的观察是，已经存在了，已经存在了。

[04:23 - 04:34] 减少选择并提供一些标准的计算选项是有帮助的，所以想想小、中、大。

[04:34 - 04:44] 并且提供用户可以交流的抽象。然后他们可以选择。好的，我需要这个和。

[04:44 - 04:54] 我们的平台根据用户的选择为Ray提供特定的资源配置。

[04:54 - 05:01] 虽然这并不能优化所有情况，但它覆盖了我们80%的情况。

[05:01 - 05:13] 在弄清楚如何优化GPU使用案例以及如何允许用户提供正确的节点或机器的过程中，我们采取了一个中间步骤。

[05:13 - 05:23] 他们需要的是想出特定于节点的队列和特定于节点的集群，但这并没有很好地奏效。

[05:23 - 05:31] 所以它导致了虽然用户，所以我们不能给他们这些选择，然后它最终变成了。

[05:31 - 05:39] 更多的碎片化和比以前更低的利用率。

[05:39 - 05:50] 因此，我们从那一步退了出来，然后开始研究多级调度。这样节点级别的调度可以。

[05:50 - 06:02] 处理多种类型的节点和实例，并且在主要的用户请求不可用时，可以回退到不同的选项。

[06:05 - 06:18] 另一个重要的优化GPU利用率的部分是支持自动扩展，因为大型工作负载的资源利用率，无论是批处理还是实时处理。

[06:18 - 06:27] 都不会保持静态，而是随着工作负载的生命周期而变化，我们在峰会的早期就听到了这一点。

[06:27 - 06:37] 来自Meta的Sergey谈到了Lama在训练期间如何需要增加其批处理大小，我们也观察到了类似的事情。

[06:37 - 06:45] 幸运的是，Ray对自动扩展有原生的支持，无论是对于批处理还是实时工作负载。

[06:45 - 06:52] RAISE Autoscaler的工作方式是...

[06:53 - 07:03] 等等，RAISE Autoscaler的工作方式是像它会监视工人的指标和补丁。

[07:03 - 07:08] 基于此的CRD。

[07:08 - 07:18] 我们使用Apache Unicorn，它监视RAISE autoscaler所做的修改。

[07:18 - 07:27] 然后根据autoscaler的额外需求排队Pod和工作负载。

[07:27 - 07:35] 然后它在降级方面做同样的事情，并尝试将工作负载一起协调和打包。

[07:37 - 07:45] 然后，但随着成功的自动扩展，也带来了一些挑战。

[07:45 - 08:02] 所以首先是对于非常大的工作负载，我们需要做出选择，我们是否希望受到地理区域的限制，或者我们是否希望跨越地理区域。

[08:02 - 08:14] 如果我们想要跨越区域，那么就需要考虑数据传输成本，然后出现的下一个问题是即使在单个区域，

[08:14 - 08:21] 工作负载可以接受向上扩展，但许多工作负载不接受向下扩展。

[08:21 - 08:30] 这就是我们在很多状态化工作负载中看到的，向下扩展会导致失败。

[08:30 - 08:45] 所以为了防止这种情况，我们需要确保，好吧，工作负载，我们的ray工人不能被驱逐，所以它们可以向上扩展，但不能向下扩展。

[08:45 - 08:52] 现在，这导致了较低的利用率，因为我们已经创建了这种。

[08:52 - 09:03] 无法缩小的工作负载。因此，为了解决这个问题，我们更多地关注了检查点。

[09:03 - 09:13] 嗯，但为了支持更好的检查点，我们必须从用户可以轻松使用的平台中引入更好的API来。

[09:13 - 09:22] 在所有不同的用例中实现标准的检查点。

[09:22 - 09:31] 当我们解决了那个问题后，下一个问题是快速扩展会带来自己的问题。

[09:31 - 09:42] 注意初始化开始出现不同的问题，比如表面和证书管理可能还没有准备好。节点还没有完全。

[09:42 - 09:54] IP地址可用性还没有完全到位，所以当自动缩放变得广泛可用时，所有这些问题都开始显现出来，用户可以快速自动扩展。

[09:55 - 10:04] 因此，为了解决这个问题，我们必须努力保持一些余量，以确保始终有一些。

[10:04 - 10:12] 可用的额外节点，这与利用率有关。

[10:12 - 10:23] 没有我们的调度器支持，这些步骤都不可能实现，所以现在深入研究调度器将由Weiwei提供。

[10:24 - 10:37] 谢谢你，Abin，我将谈论GPU管理和GPU资源预防，我认为我们使用了很多。

[10:37 - 10:49] 在IPo中的用例是巨大的，我们有很多组织、团队想要使用Ray，如何真正地将有限的GPU提供给所有这些团队、组织。

[10:49 - 10:58] 是我们要解决的关键问题，也是为什么我们选择独角兽的方法。所以独角兽的方法。

[10:58 - 11:06] 它是一个Kubernetes资源调度器，因为当我们运行Ray在Kubernetes上时，我们确实看到了很多挑战。

[11:06 - 11:17] 其中最重要的一件事是资源碎片化，我们看到不同的GPU节点没有被充分利用，因为这种碎片化在这里和那里发生。

[11:17 - 11:25] 同时，管理GPU配额也非常困难，想象一下你有很多不同类型的实例。

[11:25 - 11:33] 你想把这些实例分享给不同的团队是非常难管理不同类型GPU的配额。

[11:33 - 11:41] 同时，GPU利用率也是一个问题，总是成为一个问题，我们知道人们在抱怨。

[11:41 - 11:57] 没有足够的GPU，但同时实际上有时GPU空闲，我们不希望看到这种情况发生，而且缺乏可观察性，我认为这是很多地方在尝试构建这个东西时缺失的一个案例。

[11:57 - 12:05] 所以Apparatio Unicorn实际上帮助我们解决了许多问题。它具有基础设施、资源的复杂性。

[12:05 - 12:14] 来自ray。让Ray专注于应用程序方面的事情。Unicorn将处理底层的资源管理、调度。

[12:14 - 12:23] 对于我们来说，这真的带来了市场趋势准备好的平台给我们的最终用户，并且也提供了巨大的成本节省。

[12:25 - 12:34] 关于Unicorn的快速回顾，它是一个Kubernetes上的资源调度器，它提供了很多调度能力。

[12:34 - 12:46] 它提供了非常高的效率和改进的SRAs。关于Unicorn提供的功能的一些亮点，例如资源队列。

[12:46 - 12:54] 这对于多租户至关重要，同时也提供了不同队列之间的资源公平性。

[12:54 - 13:04] 并且提供了GAN调度，这对于机器学习工作负载非常重要。还有，资源权限非常复杂。

[13:04 - 13:09] 我会在稍后的会议中详细解释，并且我们经常使用它。

[13:10 - 13:20] 以及QBray与Unicode的集成，好消息是，在过去的两个月里。Unicode社区正在与QBray上游合作，贡献了。

[13:20 - 13:30] Unicode集成到AppStream，很快就会在下一个Kubernetes版本中可用，有了这个集成，启用Unicode非常简单。

[13:30 - 13:42] 对于Ray，你只需要设置一个标志，批量调度器，设置一个名为unicorn的名字，你已经安装了unicorn，然后你可以轻松地使用所有这些unicorn功能。

[13:42 - 13:52] 使用它的方法基本上是你设置标签，非常简单的标签，可能一个是应用程序ID，另一个是队列名称，以告诉Unicorn，

[13:52 - 13:58] 你想在哪个队列上运行你的类或作业，然后它就可以开箱即用。

[13:59 - 14:10] 所以，这里有一个快速的例子，你更喜欢读取作业回来，你给那个标签来定义这个应用程序的应用程序ID，你定义你想提交这个作业到哪个队列，

[14:10 - 14:20] 然后一旦你将规范提交到Kubernetes，Kubernetes操作员将基本上创建一个Kubernetes作业和一个带有头部和工作者的集群。

[14:20 - 14:29] 因为我们所做的集成，Unicorn基本上可以看到所有这些工作者和头部来自这个应用程序。

[14:29 - 14:40] 它们会将集群放入一个队列中，我们在那里管理GPU资源，并应用所有那些策略，会调用我们。

[14:40 - 14:52] 呃，在这个工作负载的地方，当你在队列中运行rejob或recluster时，这将与在命名空间中运行原生Kubernetes非常不同。

[14:52 - 15:05] 因为每个队列都提供了关于资源配额的良好概念，并且它提供了关于保证和最大配额的概念在两个维度上。

[15:05 - 15:17] 所以真正使这些队列具有弹性，当我们有不同的reclassers在队列中运行时，它支持优先级，因此也支持FIFO排序。

[15:17 - 15:26] 以及其他策略，以便在队列内对应用程序进行排序，以确保正确的应用程序首先获得GPU。

[15:26 - 15:35] 当然，还有这些队列之间，我们正在做资源公平性，这是极其重要的，因为我们有很多情况下人们都在竞争。

[15:35 - 15:46] GPU，我们不想让任何人不高兴，但我们只是想在那里保持一个公平政策，以确保每个团队仍然可以获得一些GPU来使用。

[15:46 - 15:56] 即使在我们系统的高峰时段，底层是一个大型共享GPU池。我们在这里试图得到的原则是，基本上。

[15:56 - 16:07] 我们希望确保GPU池可以尽可能多地被所有这些团队共享，在某些情况下我们仍然像隔离一样在某些极端情况下。

[16:07 - 16:16] 但在大多数情况下，我们希望使用一个大型GPU池来分享给每个人，并使用Unicorn来管理所有这些GPU配额。

[16:16 - 16:26] 这对我们来说效果非常好，这里有一张幻灯片，谈论我们在Ray中从Unicorn使用的功能。

[16:26 - 16:34] 第一个是GPU配额管理，它可以管理很多不同的资源类型，GPU只是其中之一。

[16:34 - 16:43] 第二个是超卖，我觉得这个是我们能够超卖我们的队列来利用更多的空闲GPU的关键。

[16:43 - 16:52] 所以本质上，我们可以在GPU集群下放置很多的队列，以驱动集群利用率达到非常高的水平。

[16:52 - 17:02] 而且Q层次结构是我们认为如果你有一个相当复杂的组织结构的话，这是很有意义的。

[17:02 - 17:13] 所以我们可以用它来映射到组织结构，以确保我们对一个大型业务的不同层级的组织都有适当的管理。

[17:13 - 17:23] 从业务线到小团队或个人。资源公平性，这是我们经常使用的东西。但是B，实际上，它是默认提供的。

[17:23 - 17:32] 对于所有的GPU和保证的最大容量，这是我们可以为我们的客户提供不同级别的GPU的关键。

[17:32 - 17:43] 保证是一些你总是可以使用的GPU，最大是一种最大的努力，束打包是我们真正确保我们的节点被充分利用的机制，

[17:43 - 17:51] 束打包有两个部分，我稍后会讲到，但这是关键，我们如何真正很好地利用资源。

[17:51 - 18:03] 排队是队列根本上提供的东西，当你耗尽容量时，我们不会拒绝你的应用程序，只是在队列中等待。

[18:03 - 18:16] 直到有人或一些工作负载释放了GPU，同时优先级也是支持的，所以我们确保我们在调度和打印时都考虑了优先级。

[18:16 - 18:27] 最后一个是抢占，我们大量使用它来确保我们超卖GPU，但同时，当我们需要它们回来时，我们有一种适当的方式来收回GPU。

[18:29 - 18:40] 这是一个非常简单的例子，在右边是你可以配置的示例配置，非常简单，你可以设置几个不同的队列，有保证的最大值，

[18:40 - 18:51] GPU配额，还有一些这些队列提供的额外功能，例如你可以根据需要配置动态队列或静态队列。

[18:51 - 18:56] 同时也支持Q层次结构，还有ACL。

[18:57 - 19:08] 有趣的部分是，我们正在朝着管理多类型GPU的方向前进，过去对我们来说是非常困难的，因为即使我们有混合的。

[19:08 - 19:16] 多样化的GPU实例类型，使用GPU的资源仍然是单一类型的。

[19:16 - 19:25] 对我们来说这不是很方便，因为在某些情况下，一些高端GPU，我们想限制团队的使用。

[19:25 - 19:34] 如果可能的话，我们希望他们使用更便宜的实例，所以我们没有一个好的故事来管理不同类型的GPU。

[19:34 - 19:46] 我们正在探索一种方法，使用修改过的GPU驱动程序来将不同的GPU类型作为资源暴露出来，然后你可以真正地通过独角兽来管理，然后我们有一个较低的收费。

[19:46 - 19:55] 这是从指标中得出的，让我们看看每个队列的保证GPU是什么，最大GPU队列是什么，当前的利用率是什么。

[19:55 - 20:04] 这样，我们可以很容易地跟踪不同类型GPU的分布。你想重新分配它们吗？你想改变布局吗？

[20:04 - 20:09] 完全由我们决定。所以这非常方便。这是我们正在探索的东西。

[20:10 - 20:21] 所以二进制打包，正如前面提到的，这是解决GPU碎片化问题的一个关键特性，这是一个非常严重的问题。

[20:21 - 20:32] 我们经常看到集群利用率并不高，但即将到来的GPU工作负载却无法获得GPU，因为每个节点上都没有足够的空间。

[20:32 - 20:40] 这只是因为这里和那里都有碎片，浪费了我们的GPU时间。

[20:40 - 20:48] 所以二进制打包，我们有两个方面。一个是使用调度阶段。独角兽提供的二进制打包。

[20:48 - 20:59] 这是一个你可以配置的策略，它基本上会在为每个锅分配进行调度时，尝试找出，尝试将锅打包得更多，增加密度。

[20:59 - 21:09] 在节点上，确保在移动到下一个节点之前，节点被充分利用，所以这是一个调度阶段，但这并不能解决所有问题，因为过了一段时间，

[21:09 - 21:19] 你会看到碎片化问题再次出现，然后我们需要一些后调度二进制打包机制，目前我们正在使用木匠来做这件事。

[21:19 - 21:30] 它基本上会扫描你集群上的锅分布，并尝试移动锅，使其更加紧凑。

[21:30 - 21:38] 这样我们可以释放一些节点，要么缩小规模，要么留给更大的GPU请求。

[21:38 - 21:43] 使用这个工具，我们能够大大减少碎片化。

[21:44 - 21:53] 而GAN调度是一个非常重要的特性，而且这也将作为上游QPray与独角兽集成的提供。

[21:53 - 22:08] 非常简单，你只需要在你的CR上添加一个私有标签，告诉RedoIO GAN调度已启用为真，然后它会给Unicode调度一个提示，所以我想要为这个应用程序启用GAN调度。

[22:08 - 22:19] 这是一个你可以应用的应用程序级配置，一旦你启用了它，独角兽所做的基本上是预留所有的资源。

[22:19 - 22:30] 提前，在实际调度之前，这样我们就可以真正避免某些重新集群只得到部分资源的情况，现在工人们正在取得进展，这非常有帮助。

[22:30 - 22:35] 然后我会花一些时间来谈谈GPU预热，这是我们最近用来提高利用率的一个相当新的特性。

[22:36 - 22:47] 因为我们支持如此多的不同用例，有延迟敏感或非敏感的工作负载，有交互式会话，实时会话。

[22:47 - 22:57] 还有一些快速迭代实验，用户只想尽快得到结果，所以在所有这些不同的用例中，用户只想，很简单，只想，

[22:57 - 23:09] 在X、Y、Z小时内完成他们的工作，这很有道理，但另一方面，管理员真的想让集群利用率很高，GPU非常昂贵，我们不想，

[23:09 - 23:23] 在那里花很多钱。所以有一个冲突。如何解决这个问题是我们利用独角兽队列，保证Mx容量。

[23:23 - 23:32] 它的意思是基本上我们可以为每个队列定义保证的容量，这是调度器总是会满足的容量。

[23:32 - 23:42] 在任何情况下，即使每个队列都在争夺G.P us。那些保证的容量仍然可用。

[23:50 - 24:03] 最大值是你可以扩展队列的方式，如果其他团队没有使用GPU，集群上有一些可用的GPU，这些GPU将最多达到你的最大值。

[24:03 - 24:11] 这样我们就可以在同一个集群中放置很多队列，我们根据客户的需求设置保证的最大容量。

[24:11 - 24:20] 然后我们能够使它们真正具有弹性。在空闲时间，我们确保利用率，因为某些队列仍然有。

[24:20 - 24:27] 杰出的请求。但在繁忙时段，我们进行权限管理以确保保证仍然可以满足。

[24:28 - 24:38] 我将快速演示一下它是如何工作的，想象我们有两个队列，非常简单，两个队列，现在集群是空的，队列是空的。

[24:38 - 24:46] 利用率都是零百分比，所以现在让我们看看，我们有一些工作负载，第一个队列中的读取集群正在运行。

[24:46 - 24:58] 它使用了30%，所以两个队列都是，我们将最大值设置为集群最大值，但这个第一个队列我们给了60%的保证GPU。

[24:58 - 25:09] 第二个队列只得到了40%，然后一旦我们提交了读取集群到第一个队列，它就开始消耗资源。

[25:09 - 25:17] 现在，因为第二个队列中没有工作负载，它消耗了50%的GPU，这很好。

[25:17 - 25:27] 我们甚至可以更高，因为这个队列真的很忙，我们让它扩展到百分之百。那很好。我们很高兴，因为现在所有的GPS都被利用了。

[25:27 - 25:38] 但如果另一个读取集群被提交到第二个队列，现在集群中没有可用的GPU，pod将会在那里等待。

[25:38 - 25:47] 在这个时候，Unicorn基本上会开始触发预热过程，因为现在发生的是。

[25:47 - 25:55] 第一个队列使用的超过保证容量，但第二个队列需要一些更多的资源来达到其保证容量。

[25:55 - 26:04] 所以Unicron基本上会说，哦，我需要从过度利用的队列移动一些GPU到未充分利用的队列。

[26:04 - 26:18] 它基本上会尊重保证容量，而且当我们进行抢占时，你将考虑应用程序优先级和任务优先级以及你的任务运行了多长时间，想象如果任务优先级相同。

[26:18 - 26:28] 那么你基本上会选择两部分来打印，这是一个非常简单的例子，然后你会收回GPU并重新分配GPU到第二个队列。

[26:28 - 26:39] 然后第二个队列将获得资源来运行，但想象如果你，在这个时候，如果你向第二个队列提交更多工作负载，因为我们仍然处于百分之百的利用率之下，

[26:39 - 26:49] 所以第二个队列将无法获得GPU，这是我们给用户的合同，我发现这对用户来说很容易理解。

[26:49 - 27:00] 它是如何工作的，他们能够提出一些关于他们的设计，他们想要如何操作他们的保证或最大容量，对我们来说效果很好。

[27:01 - 27:13] 这里是模拟，只是为了模仿它是如何工作的，因为我们有这些关于跟踪不同队列上的利用率的指标。

[27:13 - 27:21] 我们能够虚拟化，就像，基本上说它是如何工作的。这是一个非常常见的生产用例。

[27:21 - 27:30] 例如，当我们启动一个集群时，我们启动五个队列，其中一个队列的工作负载比其他队列多得多，这是很常见的。

[27:30 - 27:40] 我们让它使用更多的资源。正如你所见，大多数的GPU都被第一个队列所利用。

[27:40 - 27:49] 但如果是其他队列开始请求GPU呢？这次因为所有的GPU都在使用中。

[27:49 - 27:57] 那么我们将考虑确保每个Q仍然可以获得保证的容量。所以如果你看看。

[27:57 - 28:06] 在那一刻，主机器启动，将开始从第一个队列重新分配GPU到其他队列。

[28:06 - 28:19] 如果你比较这两个图表，第一个队列使用了四百多个GPU，但在经过一段时间后，所有队列都准确地获得了之前定义的保证容量。

[28:19 - 28:29] 所以这就是我们如何在生产环境中实现它，只是为了给每个队列提供保证的容量和最大容量。

[28:29 - 28:37] 团队，我们设法在生产集群中使用类似的东西，并且效果非常好。

[28:39 - 28:49] 所以关于这个会议的快速总结，我们在苹果公司扩展Ray时遇到了很多问题。

[28:49 - 28:57] 我认为我们希望你们从这个会议中带走的是，首先，非正式的集群。

[28:57 - 29:07] 是多租户环境所需要的，也是为了使可靠的自动缩放成为效率的关键。有很多不同的。

[29:07 - 29:17] 挑战被提到，以使自动缩放真正可靠，也尝试使用多样化的循环池和。

[29:17 - 29:30] 我们正在尽可能多地使用多样化的笔记本电脑，只是为了确保我们的所有队列都可以访问不同类型的GPU。

[29:30 - 29:39] 非常容易，同时也使用束打包来减少调度阶段和调度后阶段的碎片化。

[29:39 - 29:47] 同时也要利用，在我们的案例中，我们利用市场趋势的独角兽队列，如何为每个团队规划资源。

[29:47 - 29:58] 以及如何确保他们可以共享有限的GPU资源，同时平衡GPU利用率的提高和良好的用户体验。

[29:58 - 30:08] 实际上这非常困难，有时会有权衡，所以这真的取决于你如何定义你的系统SRA以及你对GPU利用率的关注程度。

[30:08 - 30:17] 所以你需要在那里找到正确的平衡，然后设计基础设施以确保它可以发生。

[30:18 - 30:33] 是的，谢谢，这是我们会议的内容，我们可以回答一些问题。不幸的是，我认为我们已经没有时间了，但是感谢Weiwei和Aben，我想他们会留在后面，所以如果你有问题，请随时去找他们。

[30:33 - 30:37] 但感谢你们的到来。

