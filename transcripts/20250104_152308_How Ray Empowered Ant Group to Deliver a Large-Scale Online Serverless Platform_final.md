---
video_id: _yu0Rtuetuc
video_url: https://www.youtube.com/watch?v=_yu0Rtuetuc
video_title: How Ray Empowered Ant Group to Deliver a Large-Scale Online Serverless Platform
created_at: 2025-01-04 15:24:54
---

# How Ray Empowered Ant Group to Deliver a Large-Scale Online Serverless Platform

[00:06 - 00:08] 大家好。
[00:08 - 00:12] 我们是来自艺术小组的顾阳松和zli。
[00:12 - 00:17] 今天的话题是由我们的伙伴团队带领，由魏才带领的。
[00:17 - 00:21] 但他今天不能来，所以我们将会作为发言人。
[00:21 - 00:30] 这是我们第一次来到这里，我们很高兴能够在这里分享我们的工作。
[00:30 - 00:40] 今天的话题是关于我们如何赋能艺术小组交付一个大规模在线服务平台。
[00:40 - 00:40] 让我们开始吧。
[00:41 - 00:46] 首先，我想介绍一下我的公司和我的团队。
[00:46 - 01:00] 总结一下，我们小组有五个方向：数字支付、数字连接、数字金融、数字技术和全球化。
[01:00 - 01:16] 而我们最受欢迎的应用程序是阿里，它在中国互联网、支付和金融领域处于领先地位。我的团队是Anthree团队。
[01:16 - 01:23] 在下一页，我将向您展示很多关于我的团队的历史信息。
[01:23 - 01:29] 但在这里我想先分享一些高层次的信息。
[01:30 - 01:39] 首先，我们是第二大贡献开源软件的团队。
[01:39 - 01:56] 我们与AnyScale和ResLab合作多年，并且在开源中贡献了超过26%的部分记录。
[01:56 - 02:13] 其次，除了开发Rate之外，我们在生产中还有大量Rand的大规模应用，每天有超过100万次CPU调用。
[02:14 - 02:25] 而在中国，我们还有一个丰富的中国社区，这个社区是由我的团队创建和运营的。
[02:25 - 02:32] 并且在过去五年中，我们每年都会举办ReForward Meetup。
[02:32 - 02:52] 第五届ReForward已经在上个月结束，在这次会议上，AnyScale、Cross AI、A&T和Bad Dance分享了他们的作品。
[02:52 - 03:01] 所以，我们希望Ray在中国能像在美国一样流行。
[03:02 - 03:07] 让我们进入Rain在Art中的历史部分。
[03:07 - 03:16] 我们的团队成立于2017年，这一年Ray与现在相比有所不同。
[03:16 - 03:31] 我们认为Ray是一个通用的分布式系统，这意味着我们有很多非AI应用程序和框架。

[03:31 - 03:45] 你们知道，在重新开源中，重新开源专注于Python和人工智能，这意味着首先关注Python和AI，但我们仍然有所不同。
[03:45 - 03:55] 所以你可以看到，我们已经为reproject贡献了pi、Java和C++的API。
[03:55 - 04:05] 在2018年，我们的第一个用户界面引擎在生产中部署。
[04:05 - 04:11] 这个引擎名为gflow，它是一个流式图引擎。
[04:11 - 04:17] 它被用于支付宝的支付风控。
[04:17 - 04:29] 在2019年，另一个融合引擎也在生产中部署。
[04:29 - 04:31] 这个引擎涉及学习。
[04:31 - 04:40] 我们完成了三种计算类型，包括流式处理、机器学习和在线服务。
[04:40 - 04:51] 因此，你可以知道这次在线服务也是首次部署在蚂蚁集团，并且是重点。
[04:51 - 05:08] 在2020年，鉴于业务的多样性，我们实现了ruh的多架构，这意味着你可以在一个大的rec集群中运行多个rjob。
[05:08 - 05:16] 在这个作业中，你可以支持不同的计算范式。
[05:16 - 05:22] 在2021年，系统更加稳定。
[05:22 - 05:35] 我们与生态系统公司如阿里达摩院、中国网络和万杉银行等进行了更多的合作。
[05:36 - 05:45] 在2022年，我们探索了隐私计算这一新场景。
[05:45 - 05:53] 我们认为在隐私计算中，这是一个很好的应用场景。
[05:53 - 06:07] 因为我们需要灵活的框架来支持常见的功能数据和AI，并且它有很多优势。
[06:08 - 06:28] 好的，今年即2023年，我们的目标是构建一个通用的AI服务框架，在这个框架中，我们希望统一传统的大型语言模型和搜索引擎。
[06:28 - 06:32] 这就是今天的主题。
[06:32 - 06:34] 回到在线服务。
[06:34 - 06:48] 从历史时间线来看，你可以知道在线服务首次在2019年部署在蚂蚁集团。
[06:48 - 06:54] 它整合到了在线学习引擎中。
[06:54 - 06:56] 依赖于此。
[06:56 - 07:05] 我们建立了基本的预留能力，并将其集成到蚂蚁集团的基础设施中。

[07:05 - 07:13] 你知道每家公司都有自己的客户基础设施，所以我们需要采用它。
[07:13 - 07:22] 在2021年，我们还有另一个场景，即在线资源分配。
[07:22 - 07:34] 你可以想象，如果资源用于支付和金融领域，会面临哪些挑战？
[07:34 - 07:44] 我们认为，挑战在于我们需要一个更加灵活、高性能且可扩展、稳定的框架。
[07:44 - 07:59] 幸运的是，我们在2020年基于RE实现了这一点，并在2022年支持了大规模的在线调查平台。
[07:59 - 08:12] 在此期间，我们的服务模型调用量达到了24万次，而事件驱动的在线调查平台达到了137万PC TPS。
[08:12 - 08:16] 好的，这就是我们所有的私有工作。
[08:16 - 08:24] 下一位将介绍我们今年的最新成就。
[08:24 - 08:25] 欢迎。
[08:25 - 08:26] 好的。
[08:26 - 08:29] 谢谢，Guan，大家好。
[08:29 - 08:40] 我叫李通，非常高兴能在这里与大家分享我们在2023年的最新成就。
[08:40 - 08:46] 在End Group，我们已经建立了最大的推理平台。
[08:46 - 08:52] 我们的推理集群总共有50万个CPU核心和4000个GPU核心。
[08:52 - 09:00] 我相信你们昨天在Dr. Stoker的肯演讲中已经看到了这些数字。
[09:00 - 09:07] 为了交付这些硬件，我们总共使用了超过27000个工作站。
[09:07 - 09:14] 作为一个推理平台，我们在平台上进行了非常高频的模型部署。
[09:14 - 09:23] 我们每周有超过3000个新的模型部署，每周有超过100000次的模型更新。
[09:23 - 09:28] 我们的推理集群高度自动扩展。
[09:28 - 09:41] 因此，得益于我们的产品团队和独立的自动扩展器，我们的推理集群现在可以每周主动扩展超过3000次。
[09:41 - 09:48] 通过这种方式，我们将平均CPU利用率提高了20%以上。
[09:51 - 09:58] 所以下面我将讨论我们生产中的新场景和一些新功能。
[09:59 - 10:07] 在进入细节之前，我想首先展示一下我们的Reay服务架构概述。
[10:07 - 10:12] 它与开源的Rease服务架构没有太大区别。
[10:12 - 10:17] 在这个架构中，我们有一个服务守护进程（serving keeper）。

[10:17 - 10:29] 它接收所有来自用户客户端的服务推理作业提交，并且需要将这些服务作业分配到我们的服务集群中。
[10:29 - 10:40] 但对于每个作业，服务守护进程需要在一个特定的服务集群中创建相应的应用程序主控。
[10:40 - 10:47] 这个应用程序主控将在集群内创建多个代理和部署角色。
[10:48 - 10:57] 当这些代理准备就绪时，它们会向独立的服务发现组件注册自己。
[10:57 - 11:12] 通过订阅这个组件，我们的用户应用可以了解每个代理的位置，并仔细选择最合适的代理来发送推理请求。
[11:13 - 11:27] 在每个代理中，必须为传入的推理请求分配使用本地部署角色，在那里完成模型服务工作。
[11:29 - 11:32] 好的，新的场景。
[11:32 - 11:36] 第一个场景是在GPU上进行模型推理。
[11:36 - 11:41] 为了实现这一点，我们选择了使用Triton。
[11:41 - 11:51] 对于不熟悉Triton的人来说，它是一种视频单节点模型推理引擎。
[11:51 - 12:01] 它支持多种推理后端，在我们的保留系统中，Triton服务器可以很容易地被分布。
[12:01 - 12:09] 我们需要做的就是让部署角色成为Triton引导程序。
[12:09 - 12:19] 因此，每当应用程序主控创建一个新的部署角色时，它会立即在内部启动Triton服务器。
[12:19 - 12:27] 在我们运行时环境的帮助下，这个运行时环境特性非常有用。
[12:27 - 12:38] 它是由开源Recall提供的，并在处理数据依赖包或库依赖程序时提供了很大帮助。
[12:38 - 12:46] 我相信我们的Recall团队对开源贡献了近一半的实现。
[12:46 - 12:47] 是的。
[12:47 - 13:06] 所以当这些跟踪服务器准备就绪时，可以选择向这个发现服务组件注册自己，即将来的推理请求可以直接连接到这些Triton服务器。

[13:06 - 13:21] 另外一点是你提到的，由于我们的服务集群高度自动扩展，因此当你在我们的平台上运行trton服务器时，它们也会变得自动可扩展。
[13:23 - 13:29] 好的，下一个场景是处理区域m应用程序。
[13:29 - 13:38] 这个背景是我们团队中有一个系统叫做GPT t 现金积累。
[13:38 - 13:48] 在这个系统中，我们会尝试将历史LM响应存储在我们的向量或现金存储中。
[13:48 - 13:57] 所以每当有新的推理请求进来时，我们首先进行的是相似性比较。
[13:57 - 14:11] 所以如果缓存命中，则我们直接将预先存储的响应返回给用户；如果缓存未命中，则推理请求仍需通过模型服务管道。
[14:12 - 14:22] 在我们的保留平台中，这个聊天GPT t 现金系统可以很容易地集成。
[14:22 - 14:33] 我们需要做的就是在一个部署角色内部运行这个GPT现金系统，并仍然借助角色运行时功能的帮助。
[14:33 - 14:47] 当然，我们需要在集群中有一个入口角色，它会帮助我们将每个进入的推理请求转发到GBT现金角色。
[14:47 - 14:50] 然后我们仍然做同样的事情。
[14:50 - 14:53] 如果缓存命中，我们就快速响应。
[14:53 - 15:04] 如果缓存未命中，我们就将此请求转发到另一个正在部署角色中的Triton服务器。
[15:06 - 15:21] 在这张图中你可以看到一个重要的点是，G P GPD现金角色在CPU节点上运行，而Triton服务器在GPU节点上运行。
[15:21 - 15:37] 因此，得益于资源异构结果调度，我们现在可以更明智地分配这些不同类型的角色，并优化整体资源利用率。
[15:39 - 15:42] 好的，接下来是一些新功能。
[15:42 - 15:46] 第一个是构建异步代理。
[15:46 - 15:58] 我们这样做是因为在大多数模型服务场景中，我们发现推理请求具有非常不同的执行时间。
[15:58 - 16:07] 因此，对于这些长时间的请求，人们总是需要支付大量的同步等待开销。

[16:07 - 16:14] 在这里我们所做的就是，在服务集群中部署了一个异步代理。
[16:14 - 16:20] 当然，这个异步代理也是在部署Actor中运行的。
[16:20 - 16:26] 这个异步代理可以接收每个传入的推理请求队列。
[16:26 - 16:38] 对于这些长时间请求，当结果准备好时，异步代理可以帮助我们异步地将这些结果返回给用户。
[16:40 - 17:03] 我认为从这个异步代理中获得的一个重要好处是，由于它位于集群内部，其他部署Actor可以根据它们自身的忙碌或空闲状态，自适应地从该代理拉取推理请求。
[17:03 - 17:18] 因此，通过这样做，每个运行Triton服务器进行模型服务的部署Actor都能看到更少的队头延迟。
[17:19 - 17:35] 在我们的评估中，与基于错误的轮询推送分配方法相比，我们的轮询方法能提供两倍的吞吐量。
[17:38 - 17:42] 下一个特性是C++部署。
[17:42 - 17:50] 这个特性已经在我们的推荐和广告服务中广泛部署。
[17:50 - 17:55] 这些服务对延迟敏感且需要高吞吐量。
[17:55 - 18:04] 因此，由于这些性能需求，我们的部署Actor也必须具有高性能。
[18:04 - 18:15] 所以我们所做的就是使用基于开源项目的C++工作器来创建C++部署Actor。
[18:15 - 18:22] 另外，这个C++工作器主要由我们的团队贡献。
[18:22 - 18:25] 所以感谢Gyang在这个项目上的努力。
[18:25 - 18:36] 好了，有了这个C++部署Actor之后，我们做了一件大事，实现了C++直接入口。
[18:36 - 18:40] 它是一个高性能的RPC服务。
[18:40 - 18:54] 利用这一功能，我们的推理请求可以直接连接到我们的C++部署Actor，绕过我之前提到的所有这些代理。
[18:56 - 19:01] 另一件大事是原生Triton推理调用。
[19:01 - 19:20] 因为我们现在有了C++部署Actor和Triton服务器（可以被视为C++库），所以可以更紧密和高效地集成，而没有任何跨语言开销。

[19:20 - 19:40] 在我们进行C++部署演员制作时，与其他C++组件在推荐和广告时间线中协同工作，实际上我们在构建一个更全面的分布式系统。
[19:43 - 19:48] 好的，接下来我将讨论一些未来的计划。
[19:48 - 19:54] 我们首先要做的是共享部署。
[19:54 - 20:11] 这个举措的动机在于，在我们的推荐和搜索服务中使用CVR或CTR模型时，我们可以看到大量具有大型特征数据的项目。
[20:11 - 20:19] 而这些特征数据量太大，无法全部放入任何一个工作节点。
[20:19 - 20:30] 因此我们需要尝试将这些大型特征数据分布在多个工作节点上。
[20:30 - 20:56] 当然，我们会创建代理部署演员，并使用这个代理来帮助我们执行分发路由策略，确保每个传入的推理请求都能被转发到特定的数据依赖节点。
[20:59 - 20:59] 好的。
[20:59 - 21:04] 所以最后一件事其实是从大局来看。
[21:04 - 21:13] 我们希望提供高性能的通用AI框架来实现这一目标。
[21:13 - 21:33] 首先，我们将继续建设和完善基于开源储备的推荐平台，该平台已被证明是高度分布式的、多语言支持的和可扩展的。
[21:33 - 21:40] 我们相信这可以成为许多模型服务场景的良好基础。
[21:42 - 21:52] 我们肯定会使用直接接入，用它来构建高性能的RPC服务。
[21:52 - 22:04] 我们会广泛使用C++部署演员，因为它们能为我们提供高性能计算能力。
[22:05 - 22:16] 我们将开始着手进行共享部署，这将为我们提供高性能的本地数据访问和数据检索。
[22:16 - 22:26] 我们认为，当未来处理大规模模型服务问题时，这一功能将会非常重要。
[22:28 - 22:32] 所以，今天就讲到这里。
[22:32 - 22:39] 如我在一开始所说，这项工作主要由我们团队的Ten G Wei完成。

[22:39 - 22:43] 好的，我们会尽力在这里回答问题。
[22:43 - 22:53] 但如果不能回答，我鼓励大家给这个地址发邮件，我相信他可以给出最详细的解答。
[22:53 - 22:55] 谢谢。
[23:02 - 23:12] 那么，你们所做的直接入口和C++工作有多少已经集成到开源中了？
[23:12 - 23:19] 其实它已经是开源的，但在开源社区里并不太受欢迎。
[23:19 - 23:40] 但我们大量在生产中使用它，因为我们在推荐和广告系统中需要这样做。我们希望我们的整个流水线中的所有组件，包括推荐流水线，都能用C++实现，从而使整个流水线更加整体化。
[23:42 - 23:55] 关于这一点的一个跟进问题是，在您刚才提到的推荐流水线中，您实际上在用Ray做什么？
[23:55 - 23:58] 因为您提到在推理时使用了Triton，对吧？
[23:58 - 24:04] 而且您使用了这个直接入口，但Ray在这个过程中扮演什么角色呢？
[24:04 - 24:07] Ray是如何帮助您的？
[24:07 - 24:19] 是的，我们确实使用Ray的异构环境来帮助我们整合部署中的Triton依赖。
[24:19 - 24:32] 我们还使用Ray的异构调度来帮助我们调度这些演员，无论是CPU依赖型还是GPU依赖型。
[24:32 - 24:40] 这些都可以很好地调度，从而帮助我们提高资源利用率。
[24:46 - 24:48] 谢谢您的分享。
[24:48 - 24:50] 快速提问。
[24:50 - 24:58] 您提到每周大约有3000多次模型部署。
[24:58 - 25:07] 我的问题是，对于一种类型的模型，比如大型语言模型，部署的平均频率是多少？
[25:07 - 25:14] 比如说，我们有一个大型语言模型，那么更新和部署的频率如何？
[25:14 - 25:31] 是的，这个问题你可能得去问Ten G Wei，因为他了解服务团队的细节。
[25:37 - 25:42] 它涉及到Ray的选择，这与部署有关。
[25:42 - 25:47] 是的，这与选择Ray有关。

[25:47 - 26:05] 是的，它仍然在运行。我们只需要关闭这个功能数据跨越多个沃克笔记，但沃克笔记仍然基于ray，并且所有处理的演员都是反应器。
[26:13 - 26:19] 所以所有这些都在类似于虚拟机上运行，而不是Kubernetes或其他中间的东西。
[26:19 - 26:20] 只是虚拟机。
[26:20 - 26:23] 我们的所有内容都只是在虚拟机上运行吗？
[26:23 - 26:25] 还是在Kubernetes上运行？
[26:25 - 26:35] 或者说，我们肯定是在云原生环境下基于Kubernetes运行我们的roclusters。
[26:43 - 26:48] 哦，好吧，如果没有更多问题，那么非常感谢大家。
[26:48 - 26:48] 好的。
[26:48 - 26:50] 谢谢大家。