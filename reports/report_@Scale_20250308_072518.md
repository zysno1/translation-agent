# 视频翻译处理报告

## 视频信息
- 视频标题：Ray, a Unified Distributed Framework for the Modern AI Stack | Ion Stoica
- 视频来源：YouTube
- 视频长度：00:21:40
- 视频ID/链接：oH9pJavu-PU
- 原始语言：en
- 目标语言：中文


## 处理统计
- 处理时间：2025-03-08 07:25:18 至 2025-03-08 07:25:18
- 音频提取耗时：0.00秒
- 转录耗时：0.00秒
- 翻译耗时：0.00秒
- 内容章节数：1
- 总字数（原文）：2809
- 总字数（译文）：273


## 导出文件

| 文件类型 | 文件路径 |
| ------- | ------- |
| 报告文件 (Markdown) | `./reports/report_@Scale_20250308_072518.md` |



## 内容概览
### 视频主题摘要
```plaintext
jostoica 是加州大学伯克利分校的教职员工，Sky Computing 俱乐部主任，同时也是 AnyScale 和数据库的联合创始人兼执行主席。他在职业生涯中参与了多个开源项目，如 Apache Spark、Alluxio 和 Ray。在本次演讲中，他重点介绍了 Ray 项目及其在现代机器学习中的应用。

随着 AI 技术的发展，尤其是大型语言模型（如 ChatGPT）的兴起，AI 计算需求急剧增长，远超单节点或处理器的能力。摩尔定律的放缓和专用硬件加速器的性能提升仍无法满足这一需求。因此，分布式计算成为必然选择，Ray 应运而生。

Ray 是一个通用的分布式计算框架，旨在简化机器学习应用程序的构建与扩展。它通过统一的 API 支持数据预处理、训练、超参数调优、批量预测和模型部署等任务。Ray 的核心优势在于其异步执行、任务并行化和共享内存对象存储，使开发者能够高效地扩展整个机器学习流水线。Ray 已被广泛应用于支持大型语言模型的训练与部署，包括 OpenAI 和 Cohere 等公司。

总之，Ray 极大地简化了现代机器学习基础设施的构建，特别是在处理大规模 AI 模型时，提供了强大的支持与灵活性。
```

### 关键词
Ray, 最后, AI, 首先, API, 18, ray, remote

### 内容结构
未能识别出明确的章节结构


## 完整对照翻译

### 按句分段对照翻译

| 时间 | 原文 | 译文 |
|------|------|------|
| 00:00:00 | Hi, my name is jostoica. I'm a faculty at uc Berkeley, and I am also the director of the sky comp... | ```plaintext
大家好，我的名字是 jostoica。我是加州大学伯克利分校的教职员工，同时也是 Sky Computing 俱乐部的主任。此外，我还是 AnyScale 和数据库的联... |

### 完整文本

#### 原文

```
Hi, my name is jostoica. I'm a faculty at uc Berkeley, and I am also the director of the sky computing club. I am also the confounder and executive chairman of both any scale and database. During my career, I was involved in several open source projects, including Apache meaas, Apache park, allushow and ray. As these projects became popular, they were backed by companies to ensure their long term viability and growth. S. In this talk, I will focus on the last and the most recent of these projects. This talk is organized in four sections. I will start with strength and challenges in AI that have motivated our orchonre. I will then provide a short overview of ray, and I will conclude with several array use cases. So let me start with a trends. It is more and more clear, and not only because a huge impact ChatGPT had over the last couple of months, that AI will disrupt every industry, finance, healthcare, transportation, manufacturing and more. At the same time, the computer amounts of AI applications continue to grow much faster. There's is a capability base of a single node or a single processor. Here is a plot from the last year showing that over the past twelve years, the computer manrequiring to train state of the art machine learning models have doubled every 5.7 miles. This is equivalent to the computer te MaaS growing about ten times every 18 miles. So why is this? Well, one reason is that large model have been shown to achieve much better accuracy. This a plot from the GPT -3 seminal paper published in 2020, which shows that the accuracy of a model increases significantly with its size. And this is for a variety of training and fine tuning tasks, including zero, one shot and a few shots learning. Another reason is the emergence of the foundation models that have been shown to be effective in learning multiple tasks as well as improving individual tasks, which is a great animation illustrating these points that we borrowed from the last year's gospoon palm, their pasthway language model. In contrast, according to the well known Marcle, which powers the compute industry, until the beginning of this century, the performance of a single core only doubled every 18 months. Worse yet, since then, the performance growth started to slow down. Today, per core performance improved just a few percent every 18 months. To put things into perspective, here I am overlaying the more slow performance increase as well as a real per core performance increase on the previous plot. The gap between the compute demands of training, the state of the art models and the performance of a single core is huge and is growing rapidly. In fact, note that even if a morlaw wer't ending, you have been of little help to breach the gap. But what about specialized heart or hummask? After all, over the past decade, we have seen a plethora of new hardware accelerators being released to better support these AI workloads. Well, it turns out that our specialized hardware accelerators, certainly hathey are not enough. Here I am, overlathe performance for both nvideo gps and gotpus on the same plot. Note that despite the fact that the more slow has slowed down, this processor are still able to provide about a two times performance improvement every 18 months. They do so not necessary by increasing only the number of transistors, but also by space aggressively specializing their hardware architecture for deep learning workloads. However, despite the impressive increase in performance of these specialized professessors, they are still falling way short of satisfying the computer manof training state of the art machine learning models. The gap still continues to grow and continues to grow exponentially. Now, over the past several months, you might have read some headlines. That is, the age of huge AI models is ending. Indeed, even some Alman, the OpenAI founders and co, was quoted saying this in appriothis year. However, in a more recent interview, some Alman clarified his position. In short, OpenAI internal data suggethat, the scaling law still holds, that is, model performance continue to improve with a model size. However, what is different is that due to the huge cost and GPU shortage, it is no longer feasible to increase a model size by ten times a year. Instead, models are expected to just double in size or maybe triple every year. But even if the machine learning models were to stop increasing in size, it will take decades for specialized processors to catch up. For example, consider a recently released pale model from gothis is not the largest model, but it still requires over six thousands of the later generation tips to train. Assuming the performance of these processors continue to double every 80MaaS, which is a big gift, it will take almost 90 years to be able to train the same model roughly the same time on a single processor. As such, there is no way to run this AI workloss, but to distribute them on many processors, on many machines. This is already today, and we believe it will remain for the foreseeable future. Next, let me talk about the challenges of building a machine learning applications or pipeline. So far, I only talk about training, but of course, a machine learning application or a pipeline is more complex. Most machine learning pipelines consist of three stages. First, a data pre processing stage, where you ingest the data, featze the data or computer documents and beddings. Then you take these data and you train it. And finally, you take the train models and you serve it. In addition, some machine learning pipelines might include higher parameter tuning to tunes, a model and batch prediction to evaluate the model before deploying it in production. Now imagine that we need to scale each of these stages. While we have disability systems to scale each of these stages in isolation, we would need them to stitch all these systems together to build our end to end pipeline. Unfortunately, such solution is far from optimal. First, it is hard to develop because each system own api. Then it is hard to deploy and it's hard to manage. And each system has its own semantics when it comes to falteras and data consistency, and these semantics are hard to compose. And finally, the enter and pipeline can be slow as to move the data between the stages, we typically need to read the data to to write the data and read the data from persistent stores like block stores, which is slow. We develop a way to address these very challenges. Rates is a unified compute framework for distributed application that is general enough to scale all these machine learning workloads and more. But instead of using different systems to scale a state of on machine learning piyeline, which ryou can use different libraries running on top of ray to scale each of these stages. So now there is no need to stitch together a bunch of disparsystems. Instead, ray enables you to implement your entire machine learning application using a single system. Ray started as a class project at Berkeley in 2016. His initial goal was to scale deep neural network training and reenforcement learning. In 2017, we released the first two libraries on top of ray ra leap for enforcement learning and ray tune for high parameter tuning. In 2019, any scale was founded. Any scale is a company behind raay. In 2020, we release ray version 1.0 and which includes a new library's raay serve to serve machine learning models. Then in 2022, we release version 20 zero, which includes AI run time or air for sure. We seamlessly integrate all the machine learning libraries. On top of right, a has a very minimalist sity of flexible api. The core api has just six costs. I will not go over each of these apis individually. Instead, I will illustrate the most important ones via a few simple examples. But first, why is there so general? To understand this, let's discuss the four key concepts in ray that together we spell as fast. First, ray takes the two compute abstraction provided by most procedural language ages today, function and classes, and provides the ability to execute them transparently and remotely as tasks and actors. Second, ray execute task and actors methods asynchronously. That is, when we spawn a task or an actor method, we immediately get back a reference to its return value. We call such reference a distributed future or a future for short future. Enables a parallel execution of task and actors methods. Finally, ray employs a share in memory object store to improve the performance and efficiency of passing large amounts of data between tasks and actors. Race score is written in C++, and it provides bindings for several languages. The most mature and popular binding is for Python. Here is a trivient example to illustrate how ray works. Considers the following Python code, which consists of a function f which takes 1s, and then we invoke this function and therefore the entire program, it will take 2s, because the agof each invocation takes 1s. Now notice there is no dependence between the two functions during theory. We can execute this function in rallows you to do that. So let's see how it's doing that. Now to execute f remotely and in parallel as a task, we need to decorate f of x declaration with the Radot remote. Also, when we invoke the function, we need to ask a suffix dot remote to the function called. So let's see what happens now is we invoke the function the first time. Well, in this case, we spawn a task, and before the task is scheduled or even executed, we return a future. In this case, except said, remember the future, it's a reference to the task return value. Because this call is nonblocking and returns immediately, we can now call the second invocation. We can involve the second time function f and assume this function f again is going to be run on a different, now a different worker, and is going to return again immediately. A future. Finally, we call radar, get to obtain the results returned by both f of a and f of b. The important point to note here is that in this case, f of a and f of b are going to be executed in parallel, because they are called basically at the same time, and they are executed on different workers. Eventually, each of these function will finish executions and is going to return the results to the rate dot get function, which will eventually display these results. Now, because f of a and f of b are executed in parallel, now it takes just 1s instead of two to finish the entire program. Similarly, ray allows a programmer to instantiate remote via class as an actor. To do so again, we decorate a class with Radot remote and use a doremore suffix when instantiating the class as an actor and invoking its methods. Finally, ray also allows developers to specify the amount of resources to allocate when a task or actors are created. So far, we illustrated task, actor, and futures. The final key component of ray is a shared in memory. Stoto illustrate how this component is working, consider again a simple example. In this example, we consider a function g, which takes as an argument the result returned by a function f. Assume that function f is going to be executed on now two, and which is going to return, you are going to return greater again, a future which is going to be past the g, and function g is going to be remotely executed as a task or no straight now, because g misthe results from f, it needs to wait for f to execute. When the results is available, call it X X will be transferred to nose three, and g now can take x and start executing. To understand the advantage of the shared memory of your store, assume this store did not exist. Now in this case, the f result would have to be sent back to node one in order to be passed to node g and then send to node three, where g is going to get executed. So without a shared in memory object store, instead of transferring x only once from between now 23, we have to transfer it twice, once between node two and node one, and a second time between node one and node three. As part of rto zero last year, we also release the alpha versions of ray air where ray R, where R stands for air. Antime air integrates seamless emmenlearning libraries for different workloads. Ray air has integration with many popular third party tools and systems. For example, ray data is from many external data story systems like data breaks and snowlight. Ray train supports common machine learning libraries like PyTorch, tensor flow and more. Pretunes works with existing hyperparametal tuning systems like optuna and hyperopas, well, with experimental tracking systems like ml flow and rbiases and raersupports flask fast api grano, and provides also monitoring. And now to illustrate the advantage of ray air, let me take another simple example. So let's say you have an end to end pipeline. You want to scale with ray, which consists of data preprocessing, training, high parameter tuning and bash prediction. The takeaway of this example will be the tray air allows you to implement the entire pipeline using a single script with an intuitive api. So let's start first. Let's take a look at data preprocessing here. We load a csv file from some custom location, like a file storage. We then generate a training and validation data set, and we also generate a test data set by removing the column we want to predict. Finally, we create a preprocessor, which will normalize a given column of the data set, in this case, mean radius. Now let's move to training. For model training, I'll use a transformer model available from huggiphain. This example we are passing, boss, the training and validation data sets. In addition, with a preprocessor we defined earlier by calling trainer, fit will process the training data set with a given preprocessor and train our transformer model on the resulting data set. The validation data set will be used to generate validation statistics and avoid overfitting. We can also PaaS this trainer to a tuner object. We can specify the high parameter space to sample as a metric we want to optimize for, in this case, minimizing the loss rate. Notice here that both a trainer as a tuner can be distributed by calling tunhundred or fit. Here we can get a checkpoint with the best model. Next, we take the best train model checkpoint from the tuner and PaaS it into the batch predictor module. There are two key points to notice in this example. First, you can implement your entire pipeline with a single script. Second, you can scale it from your laptop to a cluster with just a few line changes. Finally, reserve lets you deploy the model and expose an endpoint for your application, again with a very simple api. Finally, a few use cases of ray. Ray is emerging as a compute framework for modern machine learning infrastructure. In one example, Shopify has used three to implement this next generation machine learning platform. The generality of ray enables them to support the entire machine learning life cycle using a single system in their own ords. Our bet on ray to power our machine learning platform is proving instrumental in our ability to accelerate and scale our entire machine learning life cycle. Maybe not surprising that it's emerging as a compute frame or powering large language model or laws and applications as shown in this diagram, there's is the application in context learning and fine tuning layers in know as a use cases. AI has been used by OpenAI to train its largest models, including chargas, well as by cohera to train their large language models. And in the context of large language models, it's been used not only to train this model but also to serve these models as well as to scale other components of a large language model application, such as document embedding generation, as shown in this slide. To conclude, raare distributed compuframework that dramatically simplifies building machine learning infrastructure and platforms as well as scaling and productizing large language models applications. Thank you. 
```

#### 译文

```
```plaintext
大家好，我的名字是 jostoica。我是加州大学伯克利分校的教职员工，同时也是 Sky Computing 俱乐部的主任。此外，我还是 AnyScale 和数据库的联合创始人兼执行主席。在我的职业生涯中，我参与了多个开源项目，包括 Apache Meaas、Apache Spark、Alluxio 和 Ray。随着这些项目的流行，它们得到了公司的支持，以确保其长期的可行性和增长。

在这次演讲中，我将重点介绍这些项目中最近期的一个。这次演讲分为四个部分。首先，我会讨论人工智能（AI）的优势和挑战，这激发了我们的研究动机。然后，我会简要介绍 Ray，并以几个 Ray 的使用案例来结束本次演讲。让我们从趋势开始。

越来越明显的是，不仅仅因为 ChatGPT 在过去几个月的巨大影响，AI 将颠覆每一个行业，如金融、医疗保健、运输、制造业等。同时，AI 应用的计算需求继续以远超单节点或单处理器能力的速度增长。以下是过去一年的数据图表，显示在过去十二年中，训练最先进的机器学习模型所需的计算量每 5.7 个月翻一番。这相当于计算能力每 18 个月增长约十倍。

为什么会这样？原因之一是大型模型已被证明能够实现更高的准确性。这是来自 2020 年发布的 GPT-3 开创性论文中的一个图表，显示模型的准确性随着其规模的增加而显著提高。这对于多种训练和微调任务，包括零样本、一样本和少样本学习都适用。另一个原因是基础模型的出现，这些模型已被证明在学习多个任务以及改进单个任务方面非常有效。我们去年借用了 Google 的 Pathway 语言模型动画来说明这一点。

相比之下，根据著名的摩尔定律，直到本世纪初，单核性能每 18 个月才翻一番。更糟糕的是，自那时以来，性能增长开始放缓。如今，每个核心的性能每 18 个月仅提高了几个百分点。为了更好地理解这一情况，我在之前的图表上叠加了更慢的性能增长以及实际的单核性能增长。训练最先进模型的计算需求与单核性能之间的差距巨大且迅速扩大。即使摩尔定律没有结束，它也无助于弥补这一差距。

那么，专用硬件或加速器呢？毕竟，在过去十年中，我们看到了大量新的硬件加速器发布，以更好地支持这些 AI 工作负载。事实证明，即使是专用硬件加速器也不足以满足需求。这里我展示了 NVIDIA GPU 和其他 GPU 的性能在同一图表上的对比。尽管摩尔定律已经放缓，但这些处理器仍然能够在每 18 个月内提供大约两倍的性能提升。它们通过不仅增加晶体管数量，还积极优化硬件架构以适应深度学习工作负载来实现这一点。然而，尽管这些专用处理器的性能大幅提升，它们仍远远无法满足训练最先进机器学习模型的计算需求。这一差距仍在迅速扩大。

在过去几个月里，你可能读到一些关于大规模 AI 模型时代即将结束的头条新闻。确实，OpenAI 的创始人之一在今年早些时候也曾提到这一点。然而，在最近的一次采访中，他澄清了自己的立场。简而言之，OpenAI 内部数据显示，扩展定律仍然成立，即模型性能继续随着模型规模的增加而提高。不同的是，由于巨大的成本和 GPU 短缺，每年将模型规模增加十倍已不再可行。相反，模型预计每年只会翻倍或三倍增长。即使机器学习模型停止增长，专用处理器也需要几十年才能赶上。例如，考虑谷歌最近发布的 PaLM 模型，这不是最大的模型，但它仍需要超过六千个新一代 TPU 来训练。假设这些处理器的性能每 80 个月翻一番，这需要近 90 年才能在单个处理器上大致相同的时间内训练同一模型。因此，运行这些 AI 工作负载的唯一方法是将其分布在许多处理器和机器上。这已经是今天的情况，我们相信这种情况将在可预见的未来继续存在。

接下来，让我谈谈构建机器学习应用程序或流水线的挑战。到目前为止，我只讨论了训练，但当然，机器学习应用程序或流水线更为复杂。大多数机器学习流水线由三个阶段组成：首先是数据预处理阶段，您在此摄取数据、提取特征或计算文档和嵌入。然后您使用这些数据进行训练。最后，您将训练好的模型部署为服务。此外，一些机器学习流水线可能包括超参数调优以优化模型，以及批量预测以在生产前评估模型。现在想象我们需要扩展每个阶段。虽然我们有分布式系统可以单独扩展每个阶段，但我们需要将所有这些系统整合在一起以构建端到端流水线。不幸的是，这种解决方案远非最优。首先，开发困难，因为每个系统的 API 不同。其次，部署和管理也很困难。每个系统在容错和数据一致性方面都有自己的语义，这些语义难以组合。最后，整个流水线可能会很慢，因为在阶段之间移动数据时，通常需要从持久存储（如块存储）中读写数据，这很慢。

我们开发了一种方法来解决这些挑战。Ray 是一个统一的分布式计算框架，通用性足够强，可以扩展所有这些机器学习工作负载及其他更多。与其使用不同的系统来扩展机器学习流水线的各个阶段，您可以使用在 Ray 上运行的不同库来扩展每个阶段。因此，无需拼凑多个分散的系统。相反，Ray 使您能够使用单个系统实现整个机器学习应用程序。Ray 起源于 2016 年在伯克利的一个课堂项目，最初的目标是扩展深度神经网络训练和强化学习。2017 年，我们发布了基于 Ray 的第一个两个库：用于强化学习的 Ray RLlib 和用于超参数调优的 Ray Tune。2019 年，AnyScale 成立，它是 Ray 背后的公司。2020 年，我们发布了 Ray 1.0 版本，其中包括用于服务机器学习模型的新库 Ray Serve。然后在 2022 年，我们发布了 2.0 版本，其中包含用于 AI 运行时的 Ray AIR。我们无缝集成了所有机器学习库。Ray 具有非常简洁灵活的 API，核心 API 只有六个函数。我不会逐一介绍这些 API，而是通过几个简单示例来说明最重要的部分。首先，为什么它如此通用？要理解这一点，让我们讨论 Ray 中的四个关键概念，它们合起来就是 FAST。首先，Ray 提供了两种计算抽象，即大多数过程语言提供的函数和类，并提供了将它们透明地远程执行为任务和演员的能力。其次，Ray 异步执行任务和演员的方法。也就是说，当我们启动一个任务或演员方法时，我们会立即获得其返回值的引用。我们称这种引用为分布式未来或简称未来。未来使任务和演员方法并行执行成为可能。最后，Ray 使用共享内存对象存储来提高在任务和演员之间传递大量数据的性能和效率。Ray 核心是用 C++ 编写的，并为几种语言提供了绑定。最成熟和流行的绑定是 Python。以下是一个简单的例子，说明 Ray 如何工作。考虑以下 Python 代码，它由一个函数 f 组成，该函数每次调用需要 1 秒，因此整个程序需要 2 秒。注意这两个函数之间理论上没有依赖关系。我们可以并行执行这些函数，Ray 允许您这样做。让我们看看它是如何实现的。要将 f 远程并行执行为任务，我们需要用 @ray.remote 装饰 f(x) 声明，并在调用函数时添加 .remote 后缀。当第一次调用函数时，我们启动一个任务，并在任务调度甚至执行之前返回一个未来。这个调用是非阻塞的并且立即返回，因此我们现在可以调用第二次。我们可以再次调用函数 f，并假设它将在不同的工作者上运行并立即返回一个未来。最后，我们调用 ray.get 来获取 f(a) 和 f(b) 返回的结果。重要的是要注意在这种情况下，f(a) 和 f(b) 将并行执行，因为它们几乎同时被调用并在不同的工作者上执行。最终，每个函数都会完成执行并将结果返回给 ray.get 函数，该函数会显示这些结果。因为 f(a) 和 f(b) 是并行执行的，所以整个程序只需要 1 秒而不是 2 秒。同样，Ray 允许程序员通过类实例化远程演员。为此，我们用 @ray.remote 装饰类，并在实例化类作为演员和调用其方法时使用 .remote 后缀。最后，Ray 还允许开发人员指定创建任务或演员时分配的资源量。到目前为止，我们介绍了任务、演员和未来。Ray 的最后一个关键组件是共享内存。为了说明这个组件的工作原理，考虑一个简单的例子。在这个例子中，我们考虑一个函数 g，它以函数 f 返回的结果作为参数。假设函数 f 将在节点二上执行，并返回一个未来，该未来将传递给 g，而 g 将作为任务远程执行。因为 g 需要 f 的结果，它必须等待 f 执行完毕。当结果可用时，X 将被传输到节点三，g 现在可以接收 X 并开始执行。为了理解共享内存对象存储的优势，假设这个存储不存在。在这种情况下，f 的结果必须先发送回节点一，然后再传递给节点 g，最后发送到节点三，g 将在那里执行。因此，如果没有共享内存对象存储，我们将不得不在节点二和节点一之间传输一次 X，再在节点一和节点三之间传输一次 X。作为 Ray 2.0 的一部分，我们在去年发布了 Ray AIR 的 alpha 版本，其中 R 代表 AIR。Ray AIR 无缝集成了针对不同工作负载的机器学习库。Ray AIR 与许多流行的第三方工具和系统集成。例如，Ray Data 支持从许多外部数据存储系统（如 Databricks 和 Snowflake）加载数据。Ray Train 支持常见的机器学习库，如 PyTorch 和 TensorFlow。Ray Tune 与现有的超参数调优系统（如 Optuna 和 Hyperopt）集成，以及实验跟踪系统（如 MLflow）。Ray Serve 支持 Flask、FastAPI 和 Gradio，并提供监控功能。为了说明 Ray AIR 的优势，让我们看一个简单的例子。假设您有一个端到端流水线，想要使用 Ray 扩展，该流水线包括数据预处理、训练、超参数调优和批量预测。这个例子的关键在于 Ray AIR 允许您使用单个脚本和直观的 API 实现整个流水线。让我们从数据预处理开始。我们从某个自定义位置（如文件存储）加载 CSV 文件，生成训练和验证数据集，并通过移除要预测的列生成测试数据集。最后，我们创建一个预处理器，用于规范化数据集中的特定列，在此例中为平均半径。接下来是训练。对于模型训练，我将使用来自 Hugging Face 的转换器模型。在这个例子中，我们传递训练和验证数据集，以及之前定义的预处理器。通过调用 trainer.fit，将使用给定的预处理器处理训练数据集，并在处理后的数据集上训练转换器模型。验证数据集将用于生成验证统计信息，避免过拟合。我们还可以将这个 trainer 传递给 tuner 对象，并指定要采样的超参数空间和优化指标，例如最小化损失率。请注意，trainer 和 tuner 都可以通过调用 tune.run 或 fit 分布式执行。在这里，我们可以获取最佳模型的检查点。接下来，我们从 tuner 获取最佳训练模型检查点，并传递给批量预测模块。在这个例子中有两个关键点需要注意。首先，您可以使用单个脚本实现整个流水线。其次，只需几行代码更改即可从笔记本电脑扩展到集群。最后，Ray Serve 使您可以轻松部署模型并为应用程序暴露一个端点。最后，Ray 的一些使用案例。Ray 正成为一个现代机器学习基础设施的计算框架。例如，Shopify 使用 Ray 实现了下一代机器学习平台。Ray 的通用性使他们能够使用单个系统支持整个机器学习生命周期。我们对 Ray 的押注正在帮助我们加速和扩展整个机器学习生命周期。也许不出所料，Ray 正成为支持大型语言模型及其应用的计算框架。如图所示，这是上下文学习和微调层的应用场景。OpenAI 使用 Ray 训练其最大模型，包括 ChatGPT，Cohere 也使用 Ray 训练其大型语言模型。在大型语言模型的背景下，它不仅用于训练这些模型，还用于部署这些模型以及扩展其他组件，如文档嵌入生成。总之，Ray 是一个分布式计算框架，极大地简化了构建机器学习基础设施和平台，以及扩展和产品化大型语言模型应用的过程。谢谢。
```
```



## 处理日志
处理过程中未记录特殊情况。

## 建议操作
- 检查翻译质量，特别是专业术语和复杂表达
- 校对时间戳是否与视频内容同步
- 确认章节划分是否准确反映内容结构
- 考虑为重要概念添加注释或解释
